[{"content":"","date":null,"permalink":"/posts/","section":"","summary":"","title":""},{"content":"","date":null,"permalink":"/tags/agent/","section":"Tags","summary":"","title":"Agent"},{"content":"","date":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"Ai"},{"content":"","date":null,"permalink":"/tags/cot/","section":"Tags","summary":"","title":"Cot"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"Llm"},{"content":"","date":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"Rag"},{"content":"","date":null,"permalink":"/tags/react/","section":"Tags","summary":"","title":"React"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"在大型语言模型（LLM）的浪潮中，我们一直在探索如何让AI的回答更精准、更可靠。检索增强生成（RAG） 应运而生，通过引入外部知识库，成功地为LLM的回答提供了事实依据，显著减少了“一本正经地胡说八道”（即“幻觉”）。然而，技术的演进从未停歇。当我们还在赞叹RAG的巧妙时，一个更强大的范式——Agentic RAG——已经悄然兴起，它预示着AI正从一个被动的“问答机器”向一个主动的“任务执行者”转变。\n这不仅仅是一次简单的升级，而是一场范式革命。让我们深入探讨这两种技术，并剖析Agentic RAG为何代表着AI自主性的未来。\n第一幕：RAG——让AI“开卷考试”的智慧 #在Agentic RAG出现之前，标准的RAG架构已经解决了LLM应用中的一个核心痛点：知识的局限性。LLM的知识被其训练数据所“冻结”，无法获取最新信息，也无法访问私有数据。RAG通过一个简单而优雅的流程解决了这个问题。\nRAG的工作流：先检索，再生成 #正如上图所示，标准的RAG流程可以分解为以下几个步骤：\n用户提问：用户发起一个请求。 查询检索：系统将请求转化为一个查询，并在指定的知识源（如数据库、PDF文档、API接口）中进行搜索。 上下文注入：将检索到的最相关信息片段，作为“上下文（Context）”提供给LLM。 增强生成：LLM基于用户的原始问题和注入的上下文，生成一个有据可依的答案。 正如英伟达（NVIDIA）在其技术博客中提到的，RAG能够“将LLM与最新、最权威的信息连接起来”，这使得它在企业知识库问答、智能客服等场景中大放异彩。它就像给了LLM一本参考书，让它从“闭卷考试”变成了“开卷考试”。\nRAG的局限性 #尽管RAG非常有效，但它的工作模式是线性的、固定的。它善于回答那些“答案就在某处”的问题，但面对需要多步骤推理、多源信息整合、甚至需要与外部世界进行动态交互的复杂任务时，标准RAG就显得力不从心了。它像一个出色的图书管理员，能帮你找到资料，但无法帮你撰写一篇综合性的研究报告。\n第二幕：Agentic RAG——赋予AI思考和行动的能力 #Agentic RAG的出现，正是为了突破标准RAG的局限。它不再满足于简单地“查找和回答”，而是引入了“AI智能体（AI Agent）”的核心概念，让系统具备了自主规划、动态决策和工具调用的能力。\n“AI智能体”一词由著名AI学者、前特斯拉AI总监Andrej Karpathy等行业领袖大力推广，他认为这是继LLM之后，构建未来AI系统的关键。Agentic RAG正是这一理念的完美实践。\nAgentic RAG的核心架构：规划、协作与记忆 #观察上图的下半部分，我们可以发现Agentic RAG的架构远比标准RAG复杂，其核心组件包括：\n聚合器智能体（Aggregator Agent）：这是系统的“大脑”或“项目经理”。它接收用户的复杂任务后，并不急于执行，而是率先进行思考和规划。\n规划与推理（Planning \u0026amp; Reasoning）：这是Agentic RAG与RAG最根本的区别。它会利用先进的提示工程技术来分解任务：\n思维链（Chain of Thought, CoT）: 引导模型一步步地思考，将复杂问题分解为一系列有序的子问题。 ReAct (Reason + Act): 这是由Google Brain团队提出的强大框架，它让智能体在“推理”和“行动”之间交替进行。智能体首先思考“我应该做什么？”，然后选择一个工具去“行动”（如进行一次网络搜索），接着根据行动结果进行新一轮的“推理”，如此循环往复，直到任务完成。 专职智能体与多工具使用（Specialized Agents \u0026amp; Multi-Tool Use）：聚合器智能体将拆解后的子任务分配给不同的“专家”智能体。每个专职智能体都掌握着特定的工具：\n数据分析智能体：可能负责查询本地或云端数据库。 网络研究智能体：负责使用搜索引擎获取实时信息。 云服务智能体：负责调用AWS、Azure等云平台的API来执行计算或获取服务状态。 这种“分而治之”的策略，让系统能够协同处理来自不同渠道的信息，能力边界极大扩展。 Agentic RAG与MCP（企业核心系统）的关系\n在这里，图中特意标出的“MCP Servers”值得我们特别关注。 “MCP”通常代表“Master Control Program”或泛指企业内部的**“任务关键型平台（Mission-Critical Platform）”**。它象征着那些存储着公司核心、私有、高价值数据的后端系统，比如ERP（企业资源规划系统）、CRM（客户关系管理系统）、核心数据库或是私有云基础设施。\nAgentic RAG与MCP的关系，揭示了这项技术在企业应用中的巨大潜力：\n打破数据孤岛：标准RAG通常连接的是文档、网页等非结构化或半结构化数据。而Agentic RAG通过一个专门的智能体（如图中的Agent 1），被赋予了访问和理解MCP这类复杂内部系统的能力。这个智能体拥有必要的安全凭证、API接口知识和数据解析能力。\n实现深度业务自动化：通过连接MCP，Agentic RAG不再只是一个“知识问答”工具，而是成为了一个能够执行实际业务操作的“数字员工”。例如，它可以根据自然语言指令，去CRM中查询特定客户的所有订单记录，并与ERP中的库存数据进行比对，最终给销售人员生成一个备货建议。\n确保数据安全与合规：与让LLM直接连接核心数据库不同，通过一个受控的、有特定权限的智能体作为“中间人”，企业可以更好地管理数据访问、审计操作记录，并确保所有交互都在企业的安全防火墙内进行，满足合规要求。\n因此，Agentic RAG与MCP的结合，意味着AI的能力从处理公开信息，正式深入到企业运营的核心腹地，这是实现真正意义上“AI驱动的业务自动化”的关键一步。\n记忆（Memory）：Agentic RAG引入了记忆模块，包括短期记忆（用于跟踪当前任务的上下文）和长期记忆（用于从过去的经验中学习）。这使得智能体在执行长序列任务时不会“忘记”之前的步骤，并且能够持续优化其行为策略。\n一个生动的例子 #假设用户提出的任务是：“分析一下我们公司上一季度在AWS上的云服务开销，并与市场同类产品的定价进行对比，生成一份成本优化建议报告。”\n标准RAG：可能会因为无法同时访问公司内部数据库和实时网络信息而束手无策。 Agentic RAG： 聚合器智能体接收任务并规划： 推理(Reason): “首先，我需要获取上一季度的AWS账单数据。” 行动(Act): 委派云服务智能体调用AWS Billing API。 推理(Reason): “拿到数据后，我需要了解市场上同类产品的最新定价。” 行动(Act): 委派网络研究智能体搜索Google或Bing。 推理(Reason): “现在我有了内部成本和外部市场价，需要进行对比分析并撰写报告。” 行动(Act): 将所有信息汇总，交给生成模型（如Gemini）完成报告撰写。 这个过程中，智能体自主地规划、行动、反思，完美地完成了这个复杂的、跨领域的任务。\n范式之争：RAG vs. Agentic RAG # 特征 标准 RAG Agentic RAG 工作流 线性、固定的“检索-生成” 动态、迭代的“思考-行动”循环 核心角色 检索器 + 生成器 规划者(智能体) + 执行者(工具) 自主性 被动响应 主动规划与决策 工具使用 通常是单一或同类知识源 可协同使用多种异构工具（API、数据库、搜索引擎） 任务复杂度 适用于事实性问答 可处理需要多步骤、跨领域推理的复杂任务 记忆能力 无状态或仅有短期上下文 具备短期和长期记忆，能够进行持续学习 标准RAG是否过时？远非如此——简单即是优势 #看到Agentic RAG的强大能力，我们可能会问：标准RAG是否即将被淘汰？答案是：远非如此。在许多场景下，标准RAG不仅足够胜任，甚至是更优的选择。复杂性是把双刃剑，Agentic RAG的强大能力伴随着更高的实现难度、更长的响应时间和更多的计算成本。\n“对于目标明确、范围可控的任务，标准RAG的简洁性和高效性是无与伦比的。” 这是许多资深AI架构师的共识。以下是标准RAG依然闪耀的几个典型场景：\n企业内部知识库问答（Q\u0026amp;A on a Defined Corpus）\n场景：当一家公司的目标是让员工能快速查询内部的规章制度、产品手册、IT支持文档或HR政策时。 优势：知识源是封闭且确定的。标准RAG的直接“查询-检索-回答”流程响应速度快，结果可预测性高。引入复杂的智能体不仅没有必要，反而会增加系统的延迟和维护成本。 客户支持与智能客服（Customer Support \u0026amp; Chatbots）\n场景：一个电商网站的聊天机器人需要根据用户的订单号查询物流状态，或者根据产品FAQ回答常见问题。 优势：这些任务通常是重复性的、模式化的。标准RAG可以快速、准确地从知识库中提取信息，提供标准答案，确保服务质量的一致性。其架构简单，更容易规模化部署和管理。 内容摘要与事实核查（Summarization \u0026amp; Fact-Checking）\n场景：需要快速将一篇长篇报告、法律文件或新闻文章进行摘要，并确保摘要内容完全基于原文。 优势：任务目标单一明确。标准RAG能够确保生成的内容严格依据提供的文档，有效防止信息捏造。其可控性强的特点在此类场景中至关重要。 对速度和成本敏感的应用（Speed and Cost-Sensitive Applications）\n场景：需要进行大规模、高并发的实时问答服务。 优势：Agentic RAG的多步推理和工具调用链条，天然地比标准RAG的单轮检索要慢，并且会消耗更多的Token（意味着更高的API调用成本）。在性能和成本是首要考虑因素时，标准RAG的轻量级架构是明显的赢家。 简而言之，选择RAG还是Agentic RAG，并非一个“谁更好”的问题，而是一个“什么场景下用谁”的权衡。 如果你的任务边界清晰、目标单一，那么标准RAG的简单、高效和可靠性将是你的最佳选择。\n结论：未来已来，拥抱自主智能 #从RAG到Agentic RAG的演进，不仅仅是技术组件的增加，更是AI哲学思想的深刻变革。\nRAG 让AI成为了一个更博学的学者，在特定领域提供精准的知识服务。 Agentic RAG 则让AI进化成了一个能干的自主工作者，能够应对开放世界的复杂挑战。 在构建AI应用时，我们需要成为一个明智的“建筑师”，根据任务的地基和蓝图，选择最合适的工具。对于结构清晰的“信息查询亭”，标准RAG是坚固可靠的选择；而对于需要探索和建造的“智能摩天楼”，Agentic RAG则为我们提供了无限的可能性。\n随着AI智能体框架（如LangChain、LlamaIndex）的成熟和多模态能力的融合，Agentic RAG将在个性化助理、自动化科学研究、企业自动化流程（RPA）、复杂的软件开发等领域展现出惊人的潜力。它不再仅仅是回答我们问题的工具，而是将成为我们身边可靠的、能独立完成复杂任务的数字化同事。\n未来已来，是时候开始思考如何在我们自己的应用中，构建和利用这种强大的自主智能了。\n","date":"2025年6月26日","permalink":"/posts/%E4%BB%8Erag%E5%88%B0agentic-ragai%E6%AD%A3%E5%9C%A8%E5%AD%A6%E4%BC%9A%E7%8B%AC%E7%AB%8B%E6%80%9D%E8%80%83%E5%92%8C%E8%A1%8C%E5%8A%A8/","section":"","summary":"\u003cp\u003e在大型语言模型（LLM）的浪潮中，我们一直在探索如何让AI的回答更精准、更可靠。\u003cstrong\u003e检索增强生成（RAG）\u003c/strong\u003e 应运而生，通过引入外部知识库，成功地为LLM的回答提供了事实依据，显著减少了“一本正经地胡说八道”（即“幻觉”）。然而，技术的演进从未停歇。当我们还在赞叹RAG的巧妙时，一个更强大的范式——\u003cstrong\u003eAgentic RAG\u003c/strong\u003e——已经悄然兴起，它预示着AI正从一个被动的“问答机器”向一个主动的“任务执行者”转变。\u003c/p\u003e","title":"从RAG到Agentic RAG：AI正在学会独立思考和行动"},{"content":"","date":null,"permalink":"/tags/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/","section":"Tags","summary":"","title":"大语言模型"},{"content":"","date":null,"permalink":"/","section":"风间小筑","summary":"","title":"风间小筑"},{"content":"","date":null,"permalink":"/tags/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA/","section":"Tags","summary":"","title":"检索增强"},{"content":"","date":null,"permalink":"/tags/%E4%BC%81%E4%B8%9A%E5%BA%94%E7%94%A8/","section":"Tags","summary":"","title":"企业应用"},{"content":"","date":null,"permalink":"/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","section":"Tags","summary":"","title":"人工智能"},{"content":"","date":null,"permalink":"/tags/%E7%9F%A5%E8%AF%86%E5%BA%93/","section":"Tags","summary":"","title":"知识库"},{"content":"","date":null,"permalink":"/tags/%E6%99%BA%E8%83%BD%E4%BD%93/","section":"Tags","summary":"","title":"智能体"},{"content":"","date":null,"permalink":"/tags/%E8%87%AA%E5%8A%A8%E5%8C%96/","section":"Tags","summary":"","title":"自动化"},{"content":"","date":null,"permalink":"/tags/devops/","section":"Tags","summary":"","title":"DevOps"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"在 Kubernetes 的世界里，为外部应用（如 Prometheus、Grafana Agent 或 CI/CD 流水线）创建一个安全的访问凭证是每个运维和 DevOps 工程师的必备技能。这个凭证，通常是一个 Bearer Token，就像一把钥匙，允许应用与 Kubernetes API Server 对话。\n但这把钥匙能打开哪些门，取决于我们授予它的权限。直接给出 cluster-admin 的\u0026quot;万能钥匙\u0026quot;虽然简单，却极不安全。遵循最小权限原则 (Principle of Least Privilege, PoLP) 才是专业的做法。\n这篇博客将带你走过完整的流程：\n创建一个专用的服务账户 (ServiceAccount)。 定义精确的只读监控权限 (ClusterRole)。 授权并生成一个安全的 Bearer Token。 深入排查并解决最常见的 403 Forbidden 错误。 提供一个一键部署的自动化脚本。 准备好了吗？让我们开始吧！\n核心理念：最小权限原则 #我们的目标是创建一个用于监控的 Token，它只需要读取集群资源状态，而绝不需要修改或删除任何东西。因此，我们将创建一个专用的 ServiceAccount，并只授予它必要的 get, list, watch 权限。\n步骤一：奠定基础 (Namespace 和 ServiceAccount) #将监控相关的组件放在一个独立的命名空间中是一种良好的实践，便于管理和隔离。\n# 1. 创建一个名为 \u0026#34;monitoring\u0026#34; 的命名空间 kubectl create namespace monitoring # 2. 在新命名空间中创建一个服务账户 kubectl create serviceaccount monitoring-sa -n monitoring 步骤二：定义权限 (ClusterRole) #监控系统需要查看集群范围的资源（如 Nodes）和所有命名空间中的 Pods，所以我们使用 ClusterRole 来定义权限。\n一个常见的错误起点是只包含基础资源的只读权限。但别担心，我们将从这里开始，并逐步完善它。\n步骤三：授权与生成 Token #现在，我们通过 ClusterRoleBinding 将 ClusterRole 的权限授予我们的 monitoring-sa。然后，使用现代 Kubernetes (v1.24+) 推荐的方式生成一个有时效性的 Token。\n遭遇战：直面 403 Forbidden 错误 #假设我们已经创建了包含基础只读权限的 ClusterRole 和 ClusterRoleBinding，并生成了 Token。当我们将这个 Token 配置到监控工具（例如 kube-state-metrics 或我们手动测试）中去获取节点资源利用率时，很可能会遇到第一个拦路虎：\n# 使用 curl 测试访问 Kubelet 的 Summary API $ curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $APISERVER/stats/summary # 返回的错误 { \u0026#34;error\u0026#34;: \u0026#34;https://127.0.0.1:6443/stats/summary returned HTTP status 403 Forbidden\u0026#34; } 这是一个经典的权限问题。API Server 告诉我们：\u0026ldquo;我知道你是谁，但你没权限访问这个资源。\u0026rdquo;\n破案线索一：/stats/summary 是什么？ #这个端点并非普通的 API 资源，它由每个节点上的 Kubelet 提供，用于返回节点和 Pod 的详细资源使用统计（CPU、内存等）。kubectl top 命令就依赖它。要访问它，ClusterRole 中必须包含对 nodes/stats 资源的权限。\n破案线索二：为什么又是 403？ #修复了第一个问题后，我们再次尝试，却遇到了一个新的 403 错误，但这次的信息更具体了：\n{ \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;nodes \\\u0026#34;vbox\\\u0026#34; is forbidden: User \\\u0026#34;system:serviceaccount:monitoring:monitoring-sa\\\u0026#34; cannot get resource \\\u0026#34;nodes/proxy\\\u0026#34; ...\u0026#34;, \u0026#34;code\u0026#34;: 403 } 错误信息直指 nodes/proxy。这是为什么？\n因为我们是通过 API Server 访问 Kubelet 的，这个过程其实是 API Server 在为我们充当代理。Kubernetes 对这个代理功能本身也设置了权限。\nnodes/stats 是访问数据的权限。 nodes/proxy 则是使用 API Server 代理功能的\u0026quot;门票\u0026quot;。 两者缺一不可！\n最终解决方案：完整的 ClusterRole #现在，我们知道了所有必需的权限。下面是最终正确的 ClusterRole YAML 定义：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-clusterrole rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/metrics - nodes/stats # 允许访问 Kubelet 统计数据 - nodes/proxy # 允许使用 API Server 作为代理访问节点 - services - endpoints - pods - replicationcontrollers - persistentvolumes - persistentvolumeclaims verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: - daemonsets - deployments - replicasets - statefulsets verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;, \u0026#34;networking.k8s.io\u0026#34;] resources: - ingresses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;, \u0026#34;/metrics/cadvisor\u0026#34;] verbs: [\u0026#34;get\u0026#34;] 一键部署：自动化脚本 #为了让整个过程变得简单可重复，这里提供一个完整的 Bash 脚本。它会创建所有必要的资源，并生成最终的 Token。这个脚本是幂等的，可以安全地重复运行。\n将以下内容保存为 create-monitoring-token.sh：\n#!/bin/bash # ============================================================================= # # 脚本名称: create-monitoring-token.sh # 功能描述: 为 Kubernetes 监控创建一个拥有正确只读权限的 ServiceAccount 和 Bearer Token。 # # ============================================================================= set -e # --- 配置变量 --- NAMESPACE=\u0026#34;monitoring\u0026#34; SERVICE_ACCOUNT_NAME=\u0026#34;monitoring-sa\u0026#34; CLUSTER_ROLE_NAME=\u0026#34;monitoring-clusterrole\u0026#34; CLUSTER_ROLE_BINDING_NAME=\u0026#34;monitoring-clusterrole-binding\u0026#34; # --- 美化输出 --- GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; echo -e \u0026#34;${YELLOW}Starting script to create monitoring token...${NC}\u0026#34; # --- 步骤 1: 创建 Namespace (如果不存在) --- echo \u0026#34;--\u0026gt; Checking/Creating Namespace: ${NAMESPACE}\u0026#34; kubectl get namespace ${NAMESPACE} \u0026amp;\u0026gt; /dev/null || kubectl create namespace ${NAMESPACE} # --- 步骤 2: 创建 ServiceAccount (如果不存在) --- echo \u0026#34;--\u0026gt; Checking/Creating ServiceAccount: ${SERVICE_ACCOUNT_NAME}\u0026#34; kubectl get serviceaccount ${SERVICE_ACCOUNT_NAME} -n ${NAMESPACE} \u0026amp;\u0026gt; /dev/null || kubectl create serviceaccount ${SERVICE_ACCOUNT_NAME} -n ${NAMESPACE} # --- 步骤 3: 创建/更新 ClusterRole 和 ClusterRoleBinding --- echo \u0026#34;--\u0026gt; Applying ClusterRole and ClusterRoleBinding...\u0026#34; cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ${CLUSTER_ROLE_NAME} rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/metrics - nodes/stats - nodes/proxy - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: - deployments - replicasets - statefulsets - daemonsets verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;, \u0026#34;networking.k8s.io\u0026#34;] resources: - ingresses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ${CLUSTER_ROLE_BINDING_NAME} subjects: - kind: ServiceAccount name: ${SERVICE_ACCOUNT_NAME} namespace: ${NAMESPACE} roleRef: kind: ClusterRole name: ${CLUSTER_ROLE_NAME} apiGroup: rbac.authorization.k8s.io EOF # --- 步骤 4: 生成 Bearer Token --- echo \u0026#34;--\u0026gt; Generating Bearer Token for ServiceAccount: ${SERVICE_ACCOUNT_NAME}\u0026#34; TOKEN=$(kubectl create token ${SERVICE_ACCOUNT_NAME} -n ${NAMESPACE}) # --- 步骤 5: 打印 Token --- echo -e \u0026#34;\\n${YELLOW}======================[ MONITORING BEARER TOKEN ]======================${NC}\u0026#34; echo -e \u0026#34;${GREEN}${TOKEN}${NC}\u0026#34; echo -e \u0026#34;${YELLOW}=======================================================================${NC}\u0026#34; echo -e \u0026#34;\\n${YELLOW}Script finished successfully. Store this token securely!${NC}\\n\u0026#34; 如何使用脚本：\n保存为 create-monitoring-token.sh。 赋予执行权限: chmod +x create-monitoring-token.sh。 运行: ./create-monitoring-token.sh。 脚本执行完毕后，您将得到一个可以直接用于监控配置的安全 Token。\n深入理解 ClusterRole 中的资源权限 #我们上面定义的 ClusterRole 看起来很长，但每一行都有其明确的目的。理解每个资源条目控制的内容，是掌握 Kubernetes RBAC 的关键。让我们逐一解析。\n核心 API 组 (apiGroups: [\u0026quot;\u0026quot;]) #这是 Kubernetes 最基础、最核心的资源所在的分组。\nnodes: 控制对集群节点的访问。\nget/list/watch: 允许监控系统发现集群中有哪些节点、它们的状态（如 Ready, NotReady）和基本信息（IP 地址、操作系统、Kubelet 版本等）。这是服务发现的基础。 nodes/proxy: 这是关键权限，控制是否允许通过 API Server 代理到节点上的 Kubelet。\nget: 授予使用代理的\u0026quot;门票\u0026quot;。没有它，即便有后续权限，也无法通过 API Server 访问 Kubelet 的任何端点。 nodes/stats: 控制访问 Kubelet 提供的资源统计端点 (/stats/)。\nget: 允许获取节点及其上所有 Pod 的详细资源使用情况（CPU、内存、文件系统、网络）。这是 kubectl top node/pod 和 Metrics Server 获取数据的核心。 nodes/metrics: 控制访问 Kubelet 提供的度量指标端点 (/metrics/)。\nget: 允许获取 Kubelet 自身以及 cAdvisor、Prober 等组件以 Prometheus 格式暴露的详细指标。这对于深度监控节点健康至关重要。 services: 控制对 Service 资源的访问。\nget/list/watch: 允许发现应用是如何通过 Service 对外暴露的，获取其 ClusterIP、端口等信息。Prometheus 可以用它来发现需要抓取指标的目标。 endpoints: 控制对 Endpoints 资源的访问。Endpoints 对象存储了 Service 背后真实 Pod 的 IP 地址和端口。\nget/list/watch: 这是服务发现的核心。监控系统通过它找到具体提供服务的 Pod，然后去抓取这些 Pod 上的 /metrics 端点。 pods: 控制对 Pod 资源的访问。\nget/list/watch: 允许发现集群中运行的所有 Pod，获取它们的元数据（标签、注解）、状态（Running, Pending）、IP 地址等。这是自动发现监控目标（例如带有特定注解的 Pod）的基础。 Apps API 组 (apiGroups: [\u0026quot;apps\u0026quot;]) #这个分组包含了管理应用部署的核心工作负载资源。\ndeployments, replicasets, statefulsets, daemonsets: get/list/watch: 允许监控系统了解应用的部署结构。例如，一个 Deployment 有多少个预期的副本（replicas），当前有多少个可用，一个 StatefulSet 的所有实例是否都正常运行。这对于告警\u0026quot;应用副本数不足\u0026quot;或\u0026quot;部署失败\u0026quot;至关重要。 Networking API 组 (apiGroups: [\u0026quot;extensions\u0026quot;, \u0026quot;networking.k8s.io\u0026quot;]) #这个分组控制集群的网络入口资源。(注：extensions 是早期版本的 API 组，为了向后兼容而保留)\ningresses: get/list/watch: 允许监控系统发现应用是如何通过 Ingress 暴露到集群外部的，可以监控 Ingress 的配置、后端服务和 TLS 设置等。 非资源 URL (nonResourceURLs) #这类权限不针对某个具体的 Kubernetes 对象，而是直接针对 API Server 上的某个 URL 路径。\n/metrics: get: 允许直接访问 API Server 自身的 /metrics 端点，以监控 API Server 的健康状况、请求延迟、错误率等核心指标。 通过这样精细化的权限配置，我们确保了监控 Token 只能做它该做的事：观察。它能看到集群的全貌，但无法进行任何修改，就像一个尽职尽责的哨兵，忠实地报告情况，但绝不干涉战场的运作。这种安全边界的建立，是生产级 Kubernetes 环境的基石。\n验证 Token 是否生效 #创建完 Token 后，我们需要验证它是否真的能正常工作。这里提供几个实用的测试命令：\n# 1. 测试基本访问权限 curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $APISERVER/api/v1/nodes # 2. 测试 Pod 列表访问 curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $APISERVER/api/v1/pods # 3. 测试节点指标访问 curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; $APISERVER/api/v1/nodes/$(hostname)/proxy/metrics 如果这些命令都返回了正确的 JSON 响应，说明我们的 Token 配置成功了。\n如何清理（删除创建的资源） #如果您想删除此脚本创建的所有资源，可以运行以下命令：\n# !! 警告: 这将删除所有相关资源，请谨慎操作 !! # 定义变量 (与脚本中保持一致) NAMESPACE=\u0026#34;monitoring\u0026#34; SERVICE_ACCOUNT_NAME=\u0026#34;monitoring-sa\u0026#34; CLUSTER_ROLE_NAME=\u0026#34;monitoring-clusterrole\u0026#34; CLUSTER_ROLE_BINDING_NAME=\u0026#34;monitoring-clusterrole-binding\u0026#34; echo \u0026#34;Deleting monitoring RBAC resources and Namespace...\u0026#34; # 删除 ClusterRoleBinding kubectl delete clusterrolebinding ${CLUSTER_ROLE_BINDING_NAME} --ignore-not-found=true # 删除 ClusterRole kubectl delete clusterrole ${CLUSTER_ROLE_NAME} --ignore-not-found=true # 删除整个 Namespace (这将同时删除其中的 ServiceAccount) kubectl delete namespace ${NAMESPACE} --ignore-not-found=true echo \u0026#34;Cleanup complete.\u0026#34; 实际应用场景 #场景一：Prometheus 监控 #在 Prometheus 的配置中，我们需要使用这个 Token 来访问 Kubernetes API：\napiVersion: v1 kind: Secret metadata: name: prometheus-k8s-token namespace: monitoring type: Opaque stringData: token: ${TOKEN} --- apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: k8s namespace: monitoring spec: endpoints: - port: https scheme: https tlsConfig: insecureSkipVerify: true bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token 场景二：Grafana Agent #Grafana Agent 也需要类似的配置：\napiVersion: v1 kind: Secret metadata: name: grafana-agent-token namespace: monitoring type: Opaque stringData: token: ${TOKEN} --- apiVersion: monitoring.grafana.com/v1alpha1 kind: GrafanaAgent metadata: name: k8s-agent namespace: monitoring spec: metrics: instances: - name: k8s scrapeConfigs: - jobName: kubernetes-apiservers kubernetes_sd_configs: - role: endpoints bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 常见问题排查 #1. Token 过期问题 #如果遇到 Token 过期，错误信息通常如下：\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: {}, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Unauthorized\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Expired\u0026#34;, \u0026#34;code\u0026#34;: 401 } 解决方案：\n重新生成 Token 考虑使用 TokenRequest API 创建长期有效的 Token 2. 权限不足问题 #如果遇到权限不足，错误信息会明确指出缺少什么权限：\n{ \u0026#34;kind\u0026#34;: \u0026#34;Status\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;metadata\u0026#34;: {}, \u0026#34;status\u0026#34;: \u0026#34;Failure\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;nodes \\\u0026#34;node-1\\\u0026#34; is forbidden: User \\\u0026#34;system:serviceaccount:monitoring:monitoring-sa\\\u0026#34; cannot get resource \\\u0026#34;nodes/proxy\\\u0026#34; in API group \\\u0026#34;\\\u0026#34; at the cluster scope\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Forbidden\u0026#34;, \u0026#34;code\u0026#34;: 403 } 解决方案：\n检查 ClusterRole 中是否包含了所需的资源权限 确认 ClusterRoleBinding 是否正确绑定 使用 kubectl auth can-i 命令检查权限 最佳实践建议 # 定期轮换 Token\n建议每 90 天轮换一次 Token 使用自动化脚本进行轮换 确保轮换过程不影响监控系统运行 最小权限原则\n只授予必要的权限 定期审查权限配置 使用 kubectl auth can-i 验证权限 安全存储\n使用 Kubernetes Secrets 存储 Token 避免在代码或配置文件中硬编码 使用 RBAC 控制对 Secret 的访问 监控 Token 使用情况\n记录 Token 的创建和轮换时间 监控异常访问模式 设置告警机制 总结 #通过本文，我们学习了：\n如何创建一个安全的监控 Token 如何正确配置 RBAC 权限 如何验证 Token 的有效性 如何在实际场景中应用 如何排查常见问题 如何遵循最佳实践 记住，安全不是一蹴而就的，而是需要持续维护和改进的过程。定期审查和更新你的安全配置，确保你的 Kubernetes 集群始终处于最佳的安全状态。\n","date":"2025年6月10日","permalink":"/posts/kubernetes-%E5%AE%9E%E6%88%98%E4%BB%8E%E9%9B%B6%E5%88%B0%E4%B8%80%E5%88%9B%E5%BB%BA%E7%9B%91%E6%8E%A7-token/","section":"","summary":"\u003cp\u003e在 Kubernetes 的世界里，为外部应用（如 Prometheus、Grafana Agent 或 CI/CD 流水线）创建一个安全的访问凭证是每个运维和 DevOps 工程师的必备技能。这个凭证，通常是一个 Bearer Token，就像一把钥匙，允许应用与 Kubernetes API Server 对话。\u003c/p\u003e","title":"Kubernetes 实战：从零到一创建监控 Token"},{"content":"","date":null,"permalink":"/tags/rbac/","section":"Tags","summary":"","title":"RBAC"},{"content":"","date":null,"permalink":"/tags/%E5%AE%89%E5%85%A8/","section":"Tags","summary":"","title":"安全"},{"content":"","date":null,"permalink":"/tags/%E7%9B%91%E6%8E%A7/","section":"Tags","summary":"","title":"监控"},{"content":"","date":null,"permalink":"/tags/microsd/","section":"Tags","summary":"","title":"MicroSD"},{"content":"","date":null,"permalink":"/tags/%E5%AD%98%E5%82%A8%E5%8F%AF%E9%9D%A0%E6%80%A7/","section":"Tags","summary":"","title":"存储可靠性"},{"content":"","date":null,"permalink":"/tags/%E5%8D%95%E6%9D%BF%E8%AE%A1%E7%AE%97%E6%9C%BA/","section":"Tags","summary":"","title":"单板计算机"},{"content":"","date":null,"permalink":"/tags/%E9%AB%98%E8%80%90%E4%B9%85%E5%BA%A6/","section":"Tags","summary":"","title":"高耐久度"},{"content":"","date":null,"permalink":"/tags/%E9%97%AA%E5%AD%98/","section":"Tags","summary":"","title":"闪存"},{"content":"作为树莓派（Raspberry Pi）等单板计算机（SBC）的首选存储介质，microSD 卡因其小巧、廉价且易于使用而备受青睐。然而，其可靠性问题也一直是社区热议的焦点，毕竟谁也不想自己的项目因为一张小小的卡片而功亏一篑。最近，一篇在 Reddit 上引发广泛讨论的帖子，通过对 256 张 microSD 卡进行长达近两年的严苛测试，为我们揭示了 microSD 卡可靠性的真实面貌。\n测试背景：一场严苛的耐力赛 #一位名叫 a-coder 的用户在 Reddit 的 r/raspberry_pi 板块分享了他的测试项目。他购买了 256 张来自不同品牌、产品线和容量的 microSD 卡，并对其中 223 张进行了连续写入和数据校验的压力测试。在长达近两年的时间里，他向这些卡写入了超过 47 PB 的随机数据，直到它们出现故障。\n残酷的现实：并非所有 microSD 卡生而平等 #测试结果显示，microSD 卡的可靠性存在巨大的差异。一些卡在经历不到 10 次完整的读写循环后就出现了第一个错误，其中不乏一些知名品牌的产品。而另一部分卡则表现出惊人的耐用性，甚至有卡在接近 10 万次读写循环后依然“零失误”。\n更令人警醒的是，偶发性错误似乎是 microSD 卡的常态。在所有参与测试的卡中，高达 82% 的卡至少出现过一次错误。 这意味着，即使您使用的是信誉良好的品牌，也无法完全避免数据损坏的风险。\n如何为你的树莓派选择可靠的 microSD 卡？ #既然 microSD 卡的可靠性如此参差不齐，我们该如何为自己的树莓派选择一块“靠谱”的存储卡呢？结合 Reddit 帖子的讨论和相关技术文章，我们总结出以下几点建议：\n选择高耐久度（High Endurance）或工业级（Industrial）卡: 相比普通消费级卡，这些专为持续写入场景（如监控摄像头）设计的卡拥有更高的耐用性。 容量越大越好: 更大的容量意味着更多的闪存单元，这使得磨损均衡（wear leveling）算法可以将写入操作更均匀地分布，从而有效延长卡的使用寿命。 选择信誉良好的品牌和卖家: 知名品牌通常拥有更可靠的产品质量。 同时，务必从正规渠道购买，以防买到假冒伪劣产品。 关注应用性能等级（A1/A2）: A1 或 A2 等级的卡针对应用运行进行了优化，能提供更好的随机读写性能，这对于运行操作系统的树莓派至关重要。 关注写入负载: 如果您的应用需要频繁写入数据（例如日志记录、数据库操作），那么投资一块高质量的高耐久度卡至关重要。 久经考验的优胜者：值得信赖的品牌与型号 #基于 Reddit 的测试和社区的广泛反馈，以下几个品牌和系列的 microSD 卡在可靠性和性能方面表现突出，值得优先考虑：\n三星 (Samsung): 三星的卡在耐久性测试中表现出色。\nPRO Endurance 系列: 这是专为高强度写入设计的典范，在本次 Reddit 测试和社区中都备受推崇，非常适合用作树莓派的系统盘。 EVO Plus / EVO Select 系列: 作为消费级产品，它们在性能和价格之间取得了很好的平衡，对于普通应用场景来说是性价比很高的选择。 闪迪 (SanDisk): 作为闪存领域的巨头，闪迪同样提供了可靠的选择。\nMax Endurance / High Endurance 系列: 与三星的 Endurance 系列类似，这两个系列也是高耐久度的代名词，能够承受更长的录制和写入时间。 Extreme / Extreme Pro 系列: 这两个系列在读写速度上表现优异，尤其是在随机读写性能方面，非常适合需要快速响应的应用。 金士顿 (Kingston): 金士顿的工业级卡在测试中表现抢眼。\nIndustrial 系列: 该系列在测试中展现了超越普通消费卡的强悍耐用性，是追求极致稳定性的用户的可靠之选。 Canvas Go! Plus / Canvas React Plus 系列: 这两个系列在性能测试中也取得了不错的成绩，是值得考虑的选项。 其他值得关注的品牌:\nKioxia (铠侠): Kioxia 的 Exceria High Endurance 和 Exceria G2 系列在测试中表现良好，可靠性高于平均水平。 Lexar (雷克沙): Lexar 的卡在测试中也表现出了不错的耐久度。 Silicon Power (广颖电通): 该品牌的部分型号在一些性能测试中表现令人惊喜，甚至超过了一些知名大厂。 延长 microSD 卡寿命的实用技巧 #除了选择合适的卡，我们还可以通过一些方法来减少不必要的写入，从而延长 microSD 卡的使用寿命：\n减少日志写入: 将系统日志和应用日志的写入频率降至最低，或者将日志输出到外部存储设备或远程服务器。 禁用交换分区（Swap）: 禁用或减少交换分区的使用可以显著降低对 microSD 卡的写入压力。 使用 tmpfs: 将临时文件目录（如 /tmp）挂载到内存中，可以避免频繁的临时文件读写操作。 采用只读文件系统: 对于一些应用场景固定的项目，可以考虑将文件系统设置为只读模式，从根本上杜绝写入操作。 结论：没有绝对的可靠，只有更优的选择和策略 #Reddit 上的这次大规模测试为我们敲响了警钟：microSD 卡的可靠性远非我们想象的那么简单。它受到品牌、技术、使用方式和工作负载等多重因素的影响。\n对于树莓派爱好者和开发者而言，与其寄希望于找到一张“永不损坏”的卡，不如采取更务实的策略：根据你的应用场景，选择专为高强度写入设计的卡，并尽可能地优化系统以减少不必要的写入操作。 同时，定期备份重要数据 永远是避免灾难性后果的最有效手段。\n最终，虽然 microSD 卡在树莓派生态系统中仍然是不可或缺的一环，但我们必须清醒地认识到它的局限性，并采取相应的措施来保障我们项目的稳定运行。\n","date":"2025年6月5日","permalink":"/posts/%E6%B7%B1%E5%BA%A6%E6%8F%AD%E7%A7%98microsd-%E5%8D%A1%E5%9C%A8%E6%A0%91%E8%8E%93%E6%B4%BE%E4%B8%8A%E7%9A%84%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%88%B0%E5%BA%95%E5%A6%82%E4%BD%95/","section":"","summary":"\u003cp\u003e作为树莓派（Raspberry Pi）等单板计算机（SBC）的首选存储介质，microSD 卡因其小巧、廉价且易于使用而备受青睐。然而，其可靠性问题也一直是社区热议的焦点，毕竟谁也不想自己的项目因为一张小小的卡片而功亏一篑。最近，一篇在 Reddit 上引发广泛讨论的\u003ca href=\"https://old.reddit.com/r/raspberry_pi/comments/1l0v25s/how_reliable_are_microsd_cards_well_as_it_turns/\" target=\"_blank\" rel=\"noreferrer\"\u003e帖子\u003c/a\u003e，通过对 256 张 microSD 卡进行长达近两年的严苛测试，为我们揭示了 microSD 卡可靠性的真实面貌。\u003c/p\u003e","title":"深度揭秘：MicroSD 卡在树莓派上的可靠性到底如何？"},{"content":"","date":null,"permalink":"/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/","section":"Tags","summary":"","title":"树莓派"},{"content":"","date":null,"permalink":"/tags/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/","section":"Tags","summary":"","title":"数据安全"},{"content":"","date":null,"permalink":"/tags/%E7%A1%AC%E4%BB%B6%E9%80%89%E8%B4%AD/","section":"Tags","summary":"","title":"硬件选购"},{"content":"","date":null,"permalink":"/tags/faiss/","section":"Tags","summary":"","title":"Faiss"},{"content":"","date":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github"},{"content":"","date":null,"permalink":"/tags/library/","section":"Tags","summary":"","title":"Library"},{"content":"","date":null,"permalink":"/tags/memvid/","section":"Tags","summary":"","title":"Memvid"},{"content":"在数据爆炸的时代，我们不断寻求更高效、更经济的数据存储方案。Memvid 项目 (https://github.com/Olow304/memvid) 另辟蹊径，提出了一种颇具创意的思路：将文本数据编码为二维码，并将其嵌入视频帧中，从而实现一种新颖的数据存储和检索机制。这种方法巧妙地结合了视频编码的高压缩率和二维码的便捷信息承载能力，为特定场景下的数据存储提供了新的可能性。\n核心实现原理：化字为码，藏数于影 #Memvid 的核心思想是将数据（目前主要针对文本）转化为一系列二维码图像，再将这些图像作为视频的连续帧进行编码和存储。检索时，则通过语义索引定位到具体的帧，解码二维码即可还原原始数据。\n数据编码流程：从文本到视频帧的精妙转换 #Memvid 的数据编码流程设计精密，充分考虑了数据处理的效率和完整性。\ngraph TD A[输入文本数据] --\u003e B{文本分块处理}; B -- 智能分块, 句子边界检测 --\u003e C{文本块大小 \u003e 100 字符?}; C -- 是 --\u003e D[gzip 压缩]; C -- 否 --\u003e E[直接进入下一步]; D --\u003e F[Base64 编码]; E --\u003e F; F --\u003e G[生成 QR 码]; G --\u003e H[QR 码图像转换为视频帧\n帧号 = 数据块 ID]; H --\u003e I[编码为 MP4 视频文件]; I --\u003e J[生成压缩的 MP4 视频文件]; 具体步骤如下：\n文本分块处理与语义完整性： 系统首先会对输入的文本数据进行分块处理。与简单的按固定字符数切分不同，Memvid 支持智能分块，能够检测句子边界。这意味着在分割长文本时，系统会尽可能地保持每个文本块的语义完整性，避免将一个完整的句子拆分到不同的二维码中，这对于后续的数据理解和检索至关重要。用户可以根据需求配置分块的大小，以平衡单个二维码承载的信息量和容错率。\nQR 码生成与压缩优化： 每个经过分块处理的文本块都会被转换成一个独立的二维码图像。为了提升存储效率，当文本块的字符数超过预设阈值（例如 100 个字符）时，系统会自动启用 gzip 压缩算法对文本数据进行压缩，然后再生成二维码。这一步能显著减少二维码所包含的数据量，从而降低最终视频文件的大小。编码过程中还包含了 base64 编码，这有助于确保数据在不同系统和环境中的兼容性和完整性。整个二维码生成过程还内置了压缩检测机制，确保只有在压缩能带来实际收益时才执行。\n视频帧转换与数据块映射： 生成的二维码图像随后被转换为标准的视频帧格式（例如，PNG 或 JPEG 图像序列），并被编码进视频流中。Memvid 的一个巧妙之处在于，每个视频帧直接对应一个数据块 ID。例如，视频的第一帧存储的是数据块 1 的二维码，第二帧存储数据块 2 的二维码，以此类推。这种直接映射关系简化了后续的数据检索过程，使得可以根据数据块 ID（即帧号）快速定位到包含特定信息的视频帧。\n检索机制：语义理解与精准定位 #Memvid 不仅仅是将数据存入视频，它还提供了一套高效的检索机制，尤其是针对文本数据的语义搜索。\ngraph TD AA[用户输入查询文本] --\u003e BB[使用 sentence-transformers 生成查询嵌入向量]; BB --\u003e CC[在 FAISS 索引中进行相似性搜索\n（利用 JSON 索引文件中的嵌入向量）]; CC --\u003e DD[获取匹配的文本块 ID （即帧号）]; DD --\u003e EE[从 MP4 视频文件中提取对应帧 （OpenCV）]; EE --\u003e FF[使用 OpenCV QR 检测器解码帧中的 QR 码]; FF --\u003e GG{数据是否曾被压缩?}; GG -- 是 --\u003e HH[自动解压缩]; GG -- 否 --\u003e II[直接获得数据]; HH --\u003e JJ[返回原始文本数据]; II --\u003e JJ; 具体步骤如下：\n语义搜索索引构建： 为了实现快速且智能的文本检索，Memvid 引入了先进的自然语言处理技术。系统使用 sentence-transformers 库来为每个原始文本块生成对应的嵌入向量 (embedding vector)。这些向量是文本在高维空间中的数学表示，能够捕捉文本的语义信息。随后，所有文本块的嵌入向量被用于构建一个 FAISS (Facebook AI Similarity Search) 索引。FAISS 是一个专门为高效相似性搜索和海量向量聚类而设计的库，它能够极大地加速在高维空间中查找与查询向量最相似的文本向量的过程。\n帧提取与解码还原： 当用户输入查询请求时，系统首先使用相同的 sentence-transformers 模型将查询文本转换为查询嵌入向量。然后，利用预先构建的 FAISS 索引，快速找到与查询向量在语义上最相似的文本块的嵌入向量。由于每个文本块的嵌入向量都与其在视频中的帧号（即数据块 ID）相关联，系统因此能够确定包含相关信息的具体视频帧。获取到帧号后，Memvid 使用 OpenCV 库从视频文件中精确提取出对应的帧图像。OpenCV 内置的二维码检测器 (QR detector) 随后被用来识别并解码帧中的二维码。如果数据在编码时被压缩过，解码模块还会自动执行解压缩操作，最终将原始文本数据还原并呈现给用户。\n存储优化：压缩视频与轻量索引的平衡 #Memvid 在存储设计上也充分考虑了效率和实用性：\n系统最终会生成两个核心文件：\n压缩的 MP4 视频文件：这是实际存储数据的载体。通过利用现代视频编解码器（如 H.264, H.265 等）强大的帧间和帧内压缩能力，可以将大量的二维码图像高效地压缩成一个相对较小的 MP4 文件。视频编码本身就是为处理图像序列而设计的，其压缩算法能够有效去除冗余信息。 轻量级 JSON 索引文件：这个文件不存储原始数据，而是存储用于快速检索的元数据。它包含了每个文本块的嵌入向量以及其对应到视频帧号（数据块 ID）的映射关系。JSON 格式轻量且易于解析，使得索引文件本身不会占用过多存储空间，并且能够被快速加载和查询。 核心优势与技术亮点 #Memvid 的设计展现了几个显著的优势和技术创新点：\n高存储压缩率：根据项目说明，这种设计据称可以实现高达 10 倍的存储压缩率。这主要归功于两个方面：一是文本数据本身的 gzip 压缩（针对较大文本块）；二是视频编解码器对大量相似二维码图像序列的强大压缩能力。 亚秒级检索速度：通过 sentence-transformers 生成高质量的语义向量，并结合 FAISS 高效的相似性搜索能力，使得即使在大量数据中也能实现亚秒级的快速检索。这对于需要快速从海量归档数据中查找信息的用户来说至关重要。 巧妙融合不同技术优势：该方案的创新之处在于，它成功地将传统数据库所追求的随机访问能力（通过帧号直接定位）与视频流媒体的存储效率（利用视频压缩）结合起来。这是一种跨领域的技术融合。 二维码的容错与便捷性：二维码本身具有一定的容错能力，即使图像略有损坏或失真，仍有可能成功解码。同时，二维码作为一种成熟的技术，其编解码库也相对完善。 可靠性验证：项目提供的测试代码验证了二维码编解码流程的可靠性，确保了数据存取的准确性。 潜在应用场景与展望 #虽然 Memvid 可能不适用于所有类型的数据存储需求，但它在某些特定场景下展现出独特的潜力：\n大规模文本归档：对于那些需要长期保存但不频繁访问的大量文本数据（如日志、历史文档、代码库快照等），Memvid 提供了一种低成本、高压缩率的存储方案。 离线数据分发：可以将大量结构化文本数据打包成一个视频文件，方便离线分发和查阅。 数据隐私的物理隔绝：在某些极端情况下，将数据存储在视频中，并配合物理存储介质（如硬盘、U 盘），可以实现一种物理上的数据隔绝。 当然，这种技术也存在一些潜在的挑战和需要进一步探讨的问题，例如：\n写入性能：将大量数据编码为二维码并生成视频的过程可能相对耗时。 数据更新的复杂性：视频文件一旦生成，修改其中部分数据的成本较高，可能需要重新编码整个视频或部分片段。 对非文本数据的适应性：目前主要针对文本数据，对于二进制数据或其他复杂数据结构的存储和检索，可能需要额外的处理和优化。 视频质量与解码成功率的平衡：视频压缩参数的选择会影响最终视频的清晰度，进而影响二维码的解码成功率。需要在压缩率和解码可靠性之间找到最佳平衡点。 总而言之，Memvid 项目展示了一种富有想象力的数据存储方法。它通过巧妙地结合二维码技术、视频编码压缩以及现代语义搜索工具，为特定应用场景提供了一个兼具高压缩率和快速检索能力的新颖解决方案。随着技术的不断演进，我们有理由期待未来会出现更多这样跨界融合的创新，以应对日益增长的数据存储和管理挑战。\n","date":"2025年6月4日","permalink":"/posts/memvid-%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%9E%90%E5%BD%93%E8%A7%86%E9%A2%91%E7%BC%96%E7%A0%81%E9%81%87%E4%B8%8A%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E7%9A%84%E9%9D%A9%E6%96%B0/","section":"","summary":"\u003cp\u003e在数据爆炸的时代，我们不断寻求更高效、更经济的数据存储方案。Memvid 项目 (\u003ca href=\"https://github.com/Olow304/memvid\" target=\"_blank\" rel=\"noreferrer\"\u003ehttps://github.com/Olow304/memvid\u003c/a\u003e) 另辟蹊径，提出了一种颇具创意的思路：将文本数据编码为二维码，并将其嵌入视频帧中，从而实现一种新颖的数据存储和检索机制。这种方法巧妙地结合了视频编码的高压缩率和二维码的便捷信息承载能力，为特定场景下的数据存储提供了新的可能性。\u003c/p\u003e","title":"Memvid 技术解析：当视频编码遇上数据存储的革新"},{"content":"Prometheus 和 VictoriaMetrics 是两款流行的开源监控和时间序列数据库解决方案。虽然它们都服务于相似的目的，但在架构、性能、可扩展性和功能集方面存在显著差异。本文将深入探讨这两者之间的关键区别，帮助您根据具体需求做出明智的选择。\n1. 简介 # Prometheus: 最初由 SoundCloud 开发，现为云原生计算基金会 (CNCF) 的一部分，是一个强大的监控和告警工具包，专门用于处理多维环境中的时间序列数据。 它因其原生的多维数据收集、强大的查询语言 PromQL 以及在 DevOps 和 SRE 社区中的广泛采用而闻名。然而，随着监控规模的扩大，Prometheus 在存储、高基数处理、高可用性和性能方面可能会遇到挑战。 VictoriaMetrics: 作为一个高性能、经济高效且可扩展的时间序列数据库，VictoriaMetrics 可以作为 Prometheus 的长期远程存储解决方案，也可以完全替代 Prometheus。 它以卓越的数据压缩能力、高速数据摄取以及在处理大规模监控任务方面的吸引力而著称。 VictoriaMetrics 的设计目标之一就是解决 Prometheus 在大规模场景下的一些固有局限性。 2. 核心架构差异 #理解两者架构上的根本差异至关重要。\nPrometheus 架构:\nPrometheus 通常以单体应用的形式运行，负责数据抓取、存储（本地时序数据库 TSDB）、通过 PromQL 进行查询处理以及通过 Alertmanager 进行告警。 其本地存储通常不是为无限期的长期存储而设计的。 为了实现高可用性和水平扩展，Prometheus 通常需要依赖 Thanos 或 Cortex 这样的外部组件，这会增加架构的复杂性。 例如，Thanos 通过 Sidecar、Query、Store、Compact 等组件来实现全局查询视图、无限存储和数据压缩。 VictoriaMetrics 架构:\nVictoriaMetrics 采用了更为模块化的架构，即使是其集群版本也相对简洁。 其核心组件包括： vminsert: 负责数据摄取，并将数据分发到 vmstorage 节点。 vmselect: 负责数据查询，从 vmstorage 节点检索数据并执行计算。 vmstorage: 负责数据的实际存储和压缩。 这种设计天然支持水平扩展和高可用性，因为每个组件都可以独立扩展。 它避免了 Prometheus 在扩展时可能遇到的复杂性。 3. 核心差异对比 # 特性 Prometheus VictoriaMetrics 基本架构 单体应用；扩展和高可用依赖 Thanos/Cortex 等。 模块化 (vminsert, vmselect, vmstorage)；天然支持集群和水平扩展。 数据模型 拉取模型 (Pull-based) 为主，通过 PushGateway 支持推送。 支持拉取和推送两种模型 (Pull and Push-based)，接受多种流行协议的数据，如 Prometheus, InfluxDB, OpenTSDB, Graphite, Datadog。 数据摄取速率 每秒可达约 240,000 个样本（具体取决于硬件和配置）。 通常显著高于 Prometheus，在摄取率、查询性能、CPU 和内存使用方面表现更优。 每秒可达数百万甚至数千万数据点。 数据查询速率 每秒可达约 80,000 次查询（具体取决于硬件和配置）。 通常比 Prometheus 更快，特别是在处理高基数数据和复杂查询时。 内存使用 相对较高，处理高基数时间序列时内存消耗较大。 显著低于 Prometheus，在处理高基数时间序列时内存优化更好，比 Prometheus 少约 7 倍。 CPU 使用 相对较高。 通常低于 Prometheus，CPU 效率更高。 数据压缩 使用 LZF 压缩算法。 使用更高效的压缩算法（如 zstd），数据压缩率更高，与 Prometheus 相比可减少约 7 倍的存储空间。 这显著降低了存储成本。 磁盘空间使用 需要更多的磁盘空间。 由于高效的数据压缩，需要的磁盘空间更少，可以节省高达 70% 的磁盘空间。 磁盘 I/O 相对较高。 磁盘 I/O 操作更少，效率更高。 磁盘写入频率 相对更频繁地将数据写入磁盘。 减少将数据写入磁盘的频率。 查询语言 PromQL。 MetricsQL，向后兼容 PromQL，并提供额外功能和改进，例如更强大的聚合函数和窗口函数。 可扩展性 单实例不可扩展，需要借助 Thanos 或 Cortex 等第三方组件实现集群和长期存储，这会带来运营开销。 设计上支持水平扩展，集群版本可以将 vminsert、vmselect 和 vmstorage 组件分别扩展。 单节点版本也能处理大量数据。 高可用性与可靠性 本身不支持集群和原生高可用性，需通过运行重复实例或整合 Thanos、VictoriaMetrics 等方案实现。 设计时就考虑了高可用性，支持复制和集群，确保数据在实例故障时不会丢失，更为可靠。 集群版本提供开箱即用的高可用性。 架构复杂度 单机部署相对简单，但实现高可用和大规模扩展时架构会变得复杂（例如引入 Thanos）。 单节点版本部署简单，集群版本虽然组件更多，但概念清晰，相对 Thanos 架构更轻量，运营更简单。 长期存储 本地存储有保留期限制，长期存储依赖远程存储后端。 可以作为 Prometheus 的长期远程存储解决方案，并自身具备高效的长期存储能力。 数据回填 (Backfilling) 不直接支持历史数据导入。 支持通过其 API 或工具轻松导入历史数据。 多租户 (Multi-tenancy) 支持有限，通常通过标签或部署多个 Prometheus 实例实现。 提供更简单直接的多租户支持，通过头部或 URL 路径进行租户隔离。 与 Grafana 集成 与 Grafana 良好集成。 与 Grafana 良好集成，由于兼容 Prometheus API，可直接作为 Prometheus 数据源在 Grafana 中使用。 4. 各自的优势和劣势 #Prometheus:\n优势: 强大的查询语言 PromQL 和灵活的数据模型。 庞大且活跃的社区，拥有丰富的文档和第三方集成。 作为 CNCF 的一部分，与云原生生态系统紧密集成。 广泛的 Exporter 生态系统，方便采集各种系统的指标。 劣势: 单机性能和存储容量有限，不适合大规模或长期存储。 原生不支持高可用性和水平扩展，需要依赖其他组件，增加了复杂性和运营成本。 在高基数场景下，资源消耗（尤其是内存和 CPU）较高，可能导致性能问题。 缺乏简单的数据回填机制。 VictoriaMetrics:\n优势: 卓越的性能: 更高的数据摄取和查询速率，更低的资源消耗（CPU、内存、磁盘 I/O）。 出色的数据压缩能力: 显著降低存储成本，通常比 Prometheus 节省更多存储空间。 原生支持高可用性和水平扩展: 集群版本易于部署和管理，运营开销较低。 操作简单: 单节点版本易于部署和维护，启动速度快。 兼容 Prometheus API 和 PromQL: 易于从 Prometheus 迁移和集成现有工具链（如 Grafana）。 针对高基数时间序列进行了优化: 在高基数场景下表现优异。 支持数据回填和多租户: 提供了 Prometheus 缺失或实现复杂的功能。 劣势: 相对 Prometheus 而言，社区规模和生态系统可能稍小，但正在快速发展。 其自带的 UI (vmui) 功能相对 Grafana 较为简单。 告警功能需要单独配置 vmalert 组件（类似于 Prometheus 的 Alertmanager）。 一些高级功能（如下采样 Downsampling 的某些精细控制）在开源版本中可能与商业版本有所不同。 没有类似 Prometheus 的 WAL (Write-Ahead Logging) 日志，在突然故障的情况下，最近几秒内未刷盘的数据可能丢失（尽管这种情况很少见，且其快速的数据刷盘机制在一定程度上缓解了此问题）。 5. 如何选择？ #选择 Prometheus 还是 VictoriaMetrics 取决于您的具体需求和场景：\n选择 Prometheus 如果: 您的监控规模相对较小，短期数据存储已足够。 您高度依赖 PromQL 的特定高级功能和庞大且成熟的社区生态及众多开箱即用的 Exporter。 您已经深度集成在 CNCF 生态系统中，并希望利用其原生集成，且对引入 Thanos 等组件扩展不敏感。 对数据写入的极端原子性有极高要求（依赖 WAL）。 选择 VictoriaMetrics 如果: 您需要处理大规模的时间序列数据和高基数场景。 对性能、资源效率（低内存、低 CPU、低磁盘占用）和成本效益有较高要求。 需要简单易用的高可用和水平扩展方案，并希望降低运营复杂度。 正在寻找 Prometheus 的长期存储解决方案或更高性能的替代品。 希望简化监控架构，减少对复杂第三方组件的依赖。 需要数据回填或更灵活的多租户功能。 6. 总结 #Prometheus 仍然是一个强大且广泛使用的监控解决方案，特别适用于中小型环境和重视其成熟生态系统的用户。然而，随着数据量的增长和对性能、可扩展性及成本效益要求的提高，VictoriaMetrics 凭借其卓越的性能、高效的资源利用和内置的扩展能力，正成为一个极具吸引力的替代方案或增强组件。 对于许多寻求现代化、高性能且易于管理的时间序列数据库和监控解决方案的组织而言，VictoriaMetrics 提供了令人信服的价值主张，能够有效解决 Prometheus 在规模化场景下的诸多痛点。 实际的基准测试和 PoC 对于根据特定工作负载做出最终决策至关重要。\n","date":"2025年6月4日","permalink":"/posts/prometheus-vs-victoriametrics/","section":"","summary":"\u003cp\u003ePrometheus 和 VictoriaMetrics 是两款流行的开源监控和时间序列数据库解决方案。虽然它们都服务于相似的目的，但在架构、性能、可扩展性和功能集方面存在显著差异。本文将深入探讨这两者之间的关键区别，帮助您根据具体需求做出明智的选择。\u003c/p\u003e","title":"Prometheus vs VictoriaMetrics"},{"content":"","date":null,"permalink":"/tags/steam/","section":"Tags","summary":"","title":"Steam"},{"content":"","date":null,"permalink":"/tags/valve/","section":"Tags","summary":"","title":"Valve"},{"content":"","date":null,"permalink":"/tags/%E5%8D%8A%E6%9D%A1%E5%91%BD/","section":"Tags","summary":"","title":"半条命"},{"content":"","date":null,"permalink":"/tags/%E5%88%9B%E6%96%B0/","section":"Tags","summary":"","title":"创新"},{"content":"","date":null,"permalink":"/tags/%E7%A7%91%E6%8A%80%E4%BA%BA%E7%89%A9/","section":"Tags","summary":"","title":"科技人物"},{"content":"","date":null,"permalink":"/tags/%E6%95%B0%E5%AD%97%E5%8F%91%E8%A1%8C/","section":"Tags","summary":"","title":"数字发行"},{"content":"","date":null,"permalink":"/tags/%E6%B8%B8%E6%88%8F%E4%BA%A7%E4%B8%9A/","section":"Tags","summary":"","title":"游戏产业"},{"content":"","date":null,"permalink":"/tags/%E6%B8%B8%E6%88%8F%E5%BC%80%E5%8F%91/","section":"Tags","summary":"","title":"游戏开发"},{"content":"Gabe Newell，这个在数字娱乐领域掷地有声的名字，被全球数百万玩家亲切地称为\u0026quot;G胖\u0026quot;或\u0026quot;Gaben\u0026quot;。作为 Valve Corporation 的联合创始人兼总裁，以及革命性数字发行平台 Steam 的主要推动者，Newell 不仅塑造了 PC 游戏的现代格局，更以其独特的领导风格和对创新的不懈追求，成为了业界传奇。让我们更深入地探索这位行业巨擘的非凡历程，包括他所克服的挑战和取得的辉煌成就。\nGabe Newell (摄于2019年国际邀请赛)\n从哈佛辍学生到微软\u0026quot;毕业生\u0026quot; #Gabe Logan Newell 于1962年11月3日出生于美国科罗拉多州，并在加利福尼亚州的戴维斯市度过了他的成长岁月。 他曾是哈佛大学的一名学生，但在1983年，受到时任微软销售主管史蒂夫·鲍尔默的鼓动，他毅然决定辍学，投身于这家正处于高速发展期的科技公司。 Newell 后来风趣地表示，他在微软最初三个月学到的东西，远超在哈佛三年所学，这坚定了他留在微软的决心。\n在微软的十三年黄金岁月里，Newell 深度参与了早期 Windows 操作系统的开发，包括 Windows 1.01、1.02 和 1.03 版本。他不仅是重要的程序开发者，还曾担任过前两个 Windows 版本的项目经理，并领导了公司多媒体部门的创建。这段经历不仅为他积累了宝贵的技术和管理经验，也让他获得了\u0026quot;微软百万富翁\u0026quot;的称号。\nValve 的诞生：源于对游戏的激情与《半条命》的艰辛创世 #90年代中期，id Software 推出的《毁灭战士》(Doom) 和《雷神之锤》(Quake) 等游戏以前所未有的沉浸感和技术实力震撼了 Newell。他敏锐地意识到，视频游戏将成为未来娱乐产业的核心。受到同样从微软辞职并投身游戏开发的 Michael Abrash (后加入 id Software 开发《雷神之锤》) 的影响，Newell 与同事 Mike Harrington 决定共同追逐他们的游戏梦想。\n1996年8月24日，一个对 Newell 而言双喜临门的日子——他与 Lisa Mennet 结婚的同一天——Valve, L.L.C. （后更名为 Valve Corporation）正式宣告成立。他们将自己在微软时期积累的财富投入到公司的启动和首款游戏的开发中。这款雄心勃勃的作品，就是后来名垂青史的《半条命》(Half-Life)。\n《半条命》的开发过程并非一帆风顺。团队最初购买了《雷神之锤》引擎的授权，并对其进行了大量修改。然而，初版《半条命》在1997年E3展上反响平平，Newell 和团队意识到游戏需要彻底的革新，而不是简单的修补。这是一个巨大的挑战，意味着可能要推翻已有的许多工作并面临发行延期的风险。 他们毅然决然地对游戏的核心设计、敌人AI、关卡叙事进行了大刀阔斧的重制。 Newell 强调故事叙述的重要性，坚持通过环境和角色行为来讲述故事，而不是依赖过场动画。这种对质量的极致追求导致游戏多次延期，给初创的 Valve 带来了不小的压力。\n最终，坚持得到了回报。1998年11月，《半条命》正式发售，凭借其革命性的叙事方式、流畅的第一人称射击体验、智能的敌人AI以及引人入胜的科幻背景，迅速征服了全球玩家和媒体。游戏获得了超过50个年度游戏大奖，销量也节节攀升，为 Valve 和 Newell 在业界奠定了坚实的基础。2000年，Mike Harrington 因希望追求其他事业而离开 Valve，将其股份出售给 Newell，自此 Newell 成为公司的主要掌控者。\n《半条命》初代封面艺术图\nSteam 的崛起：一场数字发行的豪赌与持久战 #在开发备受期待的《半条命2》期间，Newell 敏锐地洞察到互联网在游戏分发领域的巨大潜力。他带领团队投入了大量精力，秘密开发一个名为 Steam 的数字发行和服务平台。Steam 于2003年正式上线，最初的目的是为 Valve 自家的游戏（尤其是即将到来的《半条命2》）提供便捷的自动更新、反盗版保护和在线服务。\nSteam 的早期推广充满了挑战。\n玩家的抵制与不适应： 当时玩家普遍习惯于购买实体光盘，对于必须联网验证、下载更新的 Steam 感到不便甚至反感。许多人认为 Steam 是强制安装的累赘，并且对其稳定性和游戏所有权感到担忧。 技术难题： 早期的 Steam 客户端体验不佳，下载速度慢，服务器不稳定，这些都极大地影响了用户体验。 《半条命2》源代码泄露危机： 2003年9月，《半条命2》的源代码被黑客窃取并泄露到互联网上，这对 Valve 造成了毁灭性的打击，不仅可能导致游戏提前\u0026quot;剧透\u0026quot;，更让团队士气受挫，并让 Newell 本人深感愤怒和沮丧。面对危机，Newell 没有退缩。他公开承认了泄露事件，并积极与 FBI 合作追查黑客。同时，团队顶住压力，重新评估了开发进度，并最终在一年多后成功推出了高质量的《半条命2》。 这款游戏与 Steam 进行了强制绑定，尽管初期引发了巨大争议，但也极大地推动了 Steam 的装机量。 为了克服这些挑战，Valve 和 Newell 采取了一系列措施：\n持续迭代和优化平台： Valve 不断投入资源改进 Steam 客户端的性能、下载速度、用户界面和社交功能。 打造核心吸引力： 除了 Valve 自家的优秀游戏外，Steam 逐渐引入了成就系统、好友列表、社区中心、创意工坊等功能，增强了用户粘性。 革命性的促销策略： \u0026ldquo;Steam 夏季/冬季特卖\u0026quot;等大规模促销活动以极具吸引力的折扣吸引了海量用户，并逐渐成为 PC 玩家的年度盛事。 开放平台，吸引第三方开发者： Valve 逐步降低门槛，鼓励独立开发者和大型发行商将游戏带到 Steam 平台，极大地丰富了游戏库。便捷的开发者工具和庞大的用户基础使 Steam 成为发行PC游戏的首选平台之一。 到2011年，Steam 已经占据了 PC 数字游戏下载市场50%至70%的份额，彻底改变了 PC 游戏的购买、分发、更新和游玩方式。它不仅为 Valve 带来了丰厚的利润，也为整个 PC 游戏生态系统的繁荣做出了巨大贡献。\nValve 的持续创新：经典频出与硬件探索 #在 Newell 的领导下，Valve 并未止步于《半条命》和 Steam 的成功。公司以其独特的\u0026quot;Valve Time\u0026rdquo;（指项目开发周期往往超出预期，但最终成品质量较高）而闻名，持续产出了一系列广受好评的经典游戏，包括：\n《反恐精英》(Counter-Strike) 系列： 从《半条命》的模组发展成为全球最受欢迎的多人竞技射击游戏之一。 《传送门》(Portal) 系列： 以其创新的空间解谜玩法和黑色幽默的剧情赢得了无数赞誉。 《军团要塞2》(Team Fortress 2)： 凭借其独特的艺术风格、职业设定和持续更新，保持了长久的生命力。 《求生之路》(Left 4 Dead) 系列： 开创了合作打僵尸的新潮流。 《Dota 2》： 从《魔兽争霸3》的流行模组发展而来，Valve 聘请了核心开发者 IceFrog，将其打造成全球顶级的 MOBA 电竞项目，拥有庞大的玩家社区和高额的赛事奖金。 Valve的经典游戏之一《传送门2》\n除了软件开发，Valve 也积极探索硬件领域。\nSteam Machine 的尝试与教训： 2010年代中期，Valve 曾试图通过 Steam Machine 进军客厅游戏市场，这是一种基于 Linux 的游戏主机。然而，由于操作系统兼容性问题、价格定位、以及缺乏统一的硬件标准，Steam Machine 并未取得预期的成功。这是一个重要的挑战和学习过程，让 Valve 认识到硬件生态的复杂性。 VR 领域的深耕 (Valve Index)： Valve 是虚拟现实技术的早期推动者之一，推出了高端 VR 设备 Valve Index。尽管 VR 市场发展未达预期般迅猛，但 Valve 仍在持续投入，并开发了如《半条命：爱莉克斯》(Half-Life: Alyx) 这样的标杆级 VR 游戏，展示了其技术实力和对新交互方式的探索。 Steam Deck 的异军突起： 吸取了 Steam Machine 的经验教训，Valve 于2022年推出了掌上游戏电脑 Steam Deck。这款设备面临的挑战是如何在便携设备上流畅运行为PC设计的游戏，并提供良好的用户体验。 Valve 通过定制的 APU、优化的 SteamOS (基于 Linux) 以及 Proton 兼容层，成功地让大量 Steam 游戏库中的作品在 Steam Deck 上运行。Steam Deck 凭借其强大的性能、开放的生态和相对合理的价格，获得了市场的热烈反响，成为近年来最成功的硬件产品之一。 Valve的掌上游戏电脑 Steam Deck\n独特的领导哲学与扁平化公司文化 #Newell 和 Valve 以其非传统的公司文化而著称，尤其是\u0026quot;扁平化管理结构\u0026quot;。理论上，Valve 没有严格的上下级汇报关系，员工可以自由选择参与自己感兴趣的项目，办公桌也装有轮子方便移动和协作。 Newell 相信雇用最优秀的人才，并给予他们充分的自主权和信任，是催生创新的最佳方式。他认为管理应该服务于创造过程，而非成为一种层级。\n然而，这种扁平化结构也并非没有挑战。\n决策效率： 在缺乏明确领导的情况下，一些项目的决策过程可能会变得缓慢或难以达成共识。 项目管理与责任归属： 大型项目的协调和责任分配可能会变得复杂。 \u0026ldquo;Valve Time\u0026quot;的成因之一： 自由探索的文化有时也可能导致项目周期拉长。 尽管如此，这种文化也确实吸引了大量顶尖人才，并孕育了许多极具创意的产品。Valve 通过内部的同行评议和项目驱动的模式来部分弥补传统管理结构的缺失。\n个人生活、健康挑战与多元拓展 #Newell 曾长期遭受福克斯角膜内皮营养不良症的困扰，这是一种可能导致失明的眼部疾病。 幸运的是，他在2006年和2007年成功接受了两次角膜移植手术，视力得以恢复。 这段经历也让他对生物技术和视觉科学产生了兴趣。他与 Lisa Mennet Newell 于1996年结婚，育有两个儿子，但两人后来离婚。\nNewell 的兴趣并不仅限于游戏。\n慈善事业： 他与 Valve 员工 Yahn Bernier 共同创立了名为\u0026quot;The Heart of Racing\u0026quot;的赛车队，参与赛车运动并为西雅图儿童医院和新西兰的 Starship 儿童慈善机构筹集资金。 新西兰情缘： 新冠疫情期间，他因在新西兰度假而意外滞留，为了感谢当地人民的友善款待，他与合作伙伴共同策划了一场名为\u0026quot;We Love Aotearoa\u0026quot;的免费音乐和娱乐活动。 海洋探索： Newell 拥有海洋研究组织 Inkfish，并于2022年收购了深海探索平台 Hadal Exploration System，展现了他对未知领域探索的热情。 神经科学的探索 (Starfish Neuroscience)： 近年来，Newell 将大量精力投入到神经接口技术的研究中。他与 Philip Sabes 共同创立了 Starfish Neuroscience 公司，致力于开发下一代神经接口，旨在改善与世界的交互方式，并计划在2025年底发布其首款芯片。这无疑是一个极具前瞻性和挑战性的领域，Newell 希望能借此解决更广泛的认知和交互问题。 财富、现状与未来的展望 #据《福布斯》估计，截至2024年，Gabe Newell 的净资产约为95亿美元，这主要得益于他在 Valve 的大量股份（据信他拥有公司至少四分之一，甚至可能超过一半的股权）。 尽管他近年来似乎更多地专注于 Starfish Neuroscience 等个人项目，但他仍然是 Valve 的精神领袖和战略方向的把控者。\nValve 自身也面临着新的挑战，例如来自 Epic Games Store 等平台的竞争，云游戏的兴起，以及如何持续保持创新活力等。但凭借其在 PC 游戏领域的深厚积累、Steam 平台的强大生态以及不断探索新技术的精神，Valve 依然是游戏行业中不可忽视的力量。\n传奇仍在书写 #从一位对计算机充满热情的哈佛辍学生，到微软的明星程序员，再到 Valve 的缔造者和游戏行业的颠覆者，Gabe Newell 的人生旅程充满了远见、勇气和对卓越的不懈追求。他不仅通过《半条命》等作品定义了游戏体验，更凭借 Steam 平台彻底改变了游戏的发行和消费模式。 尽管 Newell 的重心似乎有所转移，但他对技术革新和改善人类体验的探索从未停止。Gabe Newell 的传奇故事远未结束，他对数字世界的影响力必将深远而持久。\nGabe Newell，一位持续影响着游戏与科技领域的传奇人物 (摄于2013年)\n","date":"2025年5月24日","permalink":"/posts/gabe-newell-%E4%B8%80%E4%BD%8D%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E4%BA%86%E6%B8%B8%E6%88%8F%E7%95%8C%E7%9A%84%E8%BF%9C%E8%A7%81%E8%80%85%E4%B8%8E%E9%9D%A9%E6%96%B0%E8%80%85/","section":"","summary":"\u003cp\u003eGabe Newell，这个在数字娱乐领域掷地有声的名字，被全球数百万玩家亲切地称为\u0026quot;G胖\u0026quot;或\u0026quot;Gaben\u0026quot;。作为 Valve Corporation 的联合创始人兼总裁，以及革命性数字发行平台 Steam 的主要推动者，Newell 不仅塑造了 PC 游戏的现代格局，更以其独特的领导风格和对创新的不懈追求，成为了业界传奇。让我们更深入地探索这位行业巨擘的非凡历程，包括他所克服的挑战和取得的辉煌成就。\u003c/p\u003e","title":"Gabe Newell：一位重新定义了游戏界的远见者与革新者"},{"content":"","date":null,"permalink":"/tags/%E4%BA%BA%E7%89%A9/","section":"Tags","summary":"","title":"人物"},{"content":"","date":null,"permalink":"/tags/ai-tool/","section":"Tags","summary":"","title":"Ai Tool"},{"content":"","date":null,"permalink":"/tags/best-practices/","section":"Tags","summary":"","title":"Best-Practices"},{"content":"","date":null,"permalink":"/tags/coding/","section":"Tags","summary":"","title":"Coding"},{"content":"","date":null,"permalink":"/tags/cursor/","section":"Tags","summary":"","title":"Cursor"},{"content":"本文档基于英文版“CURSOR BEST PRACTICES”指南翻译并优化，旨在为开发者提供在软件开发中高效运用 AI 编程工具（特别是 Cursor）的八项关键实践。这些建议经过重新组织，以简体中文呈现，并采用清晰的视觉结构和图示，方便阅读和理解。\n1. 为 AI 设定清晰规则 #目标：为 AI 制定清晰的指导规则，确保其生成的代码符合项目的技术选型、文件结构和编码规范。\n技术栈：明确使用的编程语言、框架和关键依赖库。 文件结构：定义项目目录的组织方式和约定。 编码规范：设定统一的代码风格、命名约定和最佳实践。 提示 通过预设规则，可以有效约束 AI 的输出，避免生成不符合项目标准的代码。\n2. 提供充分的上下文信息 #目标：以 Markdown 文档形式提供详尽的项目背景信息，以减少 AI 生成不准确内容（即“幻觉”）的概率。\n推荐包含的文档内容： 产品需求文档 (PRD) 技术选型说明 (tech-stack-doc) 项目文件结构概览 (file-structure) 前端开发规范 (frontend-guidelines) 后端架构设计 (backend-structure) 推荐工具：使用 CodeGuide 编写易于 AI 理解的编码文档。 graph TD A[项目上下文] --\u003e B[PRD] A --\u003e C[技术栈说明] A --\u003e D[文件结构] A --\u003e E[前端指南] A --\u003e F[后端架构] B --\u003e G[减少 AI 幻觉] C --\u003e G D --\u003e G E --\u003e G F --\u003e G 注意 提供高质量、深度的上下文信息，能显著提升 AI 生成代码的相关性和准确性。\n3. 配置项目级规则文件 #目标：利用 Cursor 的规则系统，为项目的不同部分（如前端、后端）定制细化的规则。\n新机制：Cursor 已将旧的 .cursorrules 文件机制更新为项目规则目录（路径：cursor/rules）。 规则定制：可以为前端、后端、认证模块等分别编写特定的规则文件。 兼容性：旧的 .cursorrules 文件目前仍然可用，但未来版本中将被逐步淘汰。 重要提醒 建议尽早迁移到新的 cursor/rules 目录结构，以确保与 Cursor 未来版本的兼容性。\n4. 选择对 AI 更友好的技术栈 #目标：选用与当前主流 AI 大模型兼容性更好的技术栈，以获得更高质量的代码生成结果。\n当前支持的 AI 模型示例：Claude Sonnet 3.5、GPT-4o、o3、o1 推荐技术栈： 应用类型 首选技术栈 备选技术栈 Web 应用 NextJS, ViteJS, Python (如 Flask/Django) - 移动应用 React Native SwiftUI 说明 根据经验，React (及其生态如 NextJS, React Native) 和 Python 相关框架通常能从 AI 模型获得更好的支持和生成效果。\n5. 使用入门套件 (Starter Kits) 快速启动项目 #目标：借助预先构建的项目模板，加速新项目的初始化过程。\n推荐工具示例：CodeGuide NextJS Starter Kit 优势：避免从零配置项目结构和基础设置，更快地进入实际编码阶段。 好处 使用 Starter Kits 通常可以在几分钟内完成项目启动，显著节省初期设置时间。\n6. 运用 Cursor Agent 提升自动化能力 #目标：利用 Cursor Agent 增强代码库的分析能力和任务执行的自动化水平。\n使用步骤： 在 Cursor Chat 中向 AI 提问，分析代码或排查错误。 在 Composer (编辑器界面) 中启用 Agent 模式来执行更复杂的任务。 Agent 模式特点：内置终端交互能力，可以自动执行命令（如代码生成、测试、依赖安装等），无需用户手动确认每一步。 关键：提供详尽的上下文和清晰的任务指令是优化 Agent 表现的核心。 graph LR A[Cursor Chat] --\u003e B[分析代码/错误] B --\u003e C[Composer] C --\u003e D[启用 Agent 模式] D --\u003e E[自动执行命令] F[详细上下文/指令] --\u003e D 7. 结合使用多个大语言模型 (LLM) #目标：根据任务需求，灵活切换和结合不同 LLM 的优势，以提升编码效率和问题解决能力。\n模型特点对比 (示例)： Claude Sonnet 3.5：通常速度更快，代码生成可靠性较高，但在复杂推理或罕见错误排查上可能稍弱。 GPT o1 (或其他 GPT-4 级别模型)：在理解复杂逻辑、调试疑难错误方面可能表现更优，但速度可能稍慢。 推荐组合策略：日常编码和常规任务使用速度较快的模型 (如 Claude Sonnet 3.5)，遇到复杂问题或需要深度分析时切换到推理能力更强的模型 (如 GPT o1)。 建议 掌握不同模型的特性，根据场景切换使用，取长补短。\n8. 保持频繁的 GitHub 提交 #目标：通过高频率、小范围的代码提交，有效管理版本，降低 AI 可能引入错误的风险。\n最佳实践：在每个功能点或修复成功完成后，进行一次目的单一、描述清晰的 commit。 频率：保持工作分支与 GitHub 远程仓库的定期同步。 警告 AI 辅助编码虽能提效，但也可能引入潜在错误。健壮的版本控制是重要的安全保障。\n总结 #遵循以上八项实践，开发者可以更充分地发挥 Cursor 等 AI 编程工具的潜力，有效提升软件开发效率和代码质量。其核心在于：为 AI 提供清晰的规则和丰富的上下文，明智地选择技术栈，灵活运用不同 AI 模型和工具特性，并始终保持良好的版本控制习惯。\n","date":"2025年4月3日","permalink":"/posts/cursor-%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97%E9%AB%98%E6%95%88%E5%88%A9%E7%94%A8-ai-%E8%BE%85%E5%8A%A9%E7%BC%96%E7%A8%8B/","section":"","summary":"\u003cp\u003e本文档基于英文版“CURSOR BEST PRACTICES”指南翻译并优化，旨在为开发者提供在软件开发中高效运用 AI 编程工具（特别是 Cursor）的八项关键实践。这些建议经过重新组织，以简体中文呈现，并采用清晰的视觉结构和图示，方便阅读和理解。\u003c/p\u003e","title":"Cursor 最佳实践指南：高效利用 AI 辅助编程"},{"content":"","date":null,"permalink":"/tags/development/","section":"Tags","summary":"","title":"Development"},{"content":"","date":null,"permalink":"/tags/animation/","section":"Tags","summary":"","title":"Animation"},{"content":"","date":null,"permalink":"/tags/video/","section":"Tags","summary":"","title":"Video"},{"content":"TechHalla 在 X 平台上分享了一篇教程，详细介绍了如何使用 AI 工具（如 ChatGPT 4o、Kling AI 和 Magnific AI）创建动画视频。教程以一个 2.5D 卡通风格的场景为示例，展示了一个光头男子和一只背着背包的豚鼠（capybara）走进一个带有“酒店”标志的西部小镇，场景融合了后末世与蒸汽朋克元素。\n本作品集将以简体中文重新整理该内容，优化视觉层次，并通过图表和图标增强可读性。\n核心内容 #1. 动画场景与灵感 #TechHalla 的动画灵感来源于经典西部片，但融入了现代 AI 技术，创造出独特的视觉风格。以下是主要场景描述：\n主角：一个光头男子，身穿黑色T恤和红色运动鞋，带有纹身，展现硬朗风格。 伙伴：一只背着皮质背包的豚鼠，增添趣味与温暖。 场景：一个西部小镇，带有“酒店”标志，背景中可见蒸汽朋克风格的机器人结构，营造后末世氛围。 场景预览 # 图：光头男子与豚鼠走进带有“酒店”标志的小镇，背景有蒸汽朋克机器人结构。\n2. 制作流程 #TechHalla 提供了一个清晰的动画制作流程，分为以下几个步骤：\n2.1 创建角色 # 工具：ChatGPT 4o 方法： 上传一张照片到 ChatGPT。 要求将其转换为 2.5D 卡通风格，T-pose 姿势，细线条，绿色背景，保持原有服装。 基于此风格，生成另外三个匹配的自定义角色。 角色展示 # 角色 描述 图片 主角 光头男子，黑色T恤，红色运动鞋，纹身 角色2 女性，红色T恤，背包，红色运动鞋 角色3 酒保，戴帽子，围裙，拿着杯子 伙伴 豚鼠，背着皮质背包 2.2 生成场景 # 工具：ChatGPT 4o 方法： 保持与角色相同的 2.5D 卡通风格。 描述场景（如西部小镇、酒吧内部），并指定 16:9 宽高比。 使用 Magnific AI 进行图像放大，增强细节。 2.3 场景组合与调整 #工具：ChatGPT 4o\n方法：\n使用 ChatGPT 将角色与场景组合，调整构图。 要求 ChatGPT 从不同角度生成新帧，保持空间一致性。 示例场景\n场景1：豚鼠与男子走上酒店台阶 场景2：酒吧内部，男子与酒保对话 场景3：沙漠中的剪影，戴帽男子背对镜头 沙漠剪影 2.4 动画制作 #工具：Kling AI v1.6\n方法：\n使用 Kling AI 将静态帧转化为动画。 添加音效，增强沉浸感。 动画工具对比\n工具 功能 优势 Kling AI 动画生成，音效添加 高质量动画，操作简单 Magnific AI 图像放大 提升细节，适合高质量输出 3. 使用的 AI 工具 #TechHalla 使用了以下 AI 工具，展示了 2025 年生成式 AI 在动画领域的趋势：\nChatGPT 4o：角色设计、场景生成、构图调整。 Kling AI：动画生成与音效添加。 Magnific AI：图像放大，增强细节。 4. 成果展示 #TechHalla 展示了多个动画场景片段，体现了 AI 工具的高效与创意：\n酒店入口：男子与豚鼠走进酒店，动态展现步伐与环境互动。 酒吧对话：男子与酒保的互动，背景瓶子与时钟增添氛围。 沙漠剪影：孤独的剪影画面，营造戏剧性张力。 总结 #通过 TechHalla 的教程，我们可以看到生成式 AI 在动画领域的巨大潜力。以下是关键收获：\n高效创作：从角色设计到动画生成，AI 工具大幅缩短了创作周期。 风格多样：2.5D 卡通风格结合西部与蒸汽朋克元素，展现了 AI 的创意灵活性。 易于上手：即使是初学者，也能通过 ChatGPT 和 Kling AI 快速制作动画。 作者：TechHalla\n发布日期：2025年3月30日\n主题：通过 AI 工具（如 ChatGPT 4o 和 Kling AI）创建动画视频的教程\n目标：帮助用户学习如何制作属于自己的动画视频\n","date":"2025年3月31日","permalink":"/posts/%E4%BD%BF%E7%94%A8-ai-%E6%89%93%E9%80%A0%E5%8A%A8%E7%94%BB%E8%A7%86%E9%A2%91techhalla-%E7%9A%84%E5%88%9B%E6%84%8F%E4%B9%8B%E6%97%85/","section":"","summary":"\u003cp\u003eTechHalla 在 X 平台上分享了一篇教程，详细介绍了如何使用 AI 工具（如 ChatGPT 4o、Kling AI 和 Magnific AI）创建动画视频。教程以一个 2.5D 卡通风格的场景为示例，展示了一个光头男子和一只背着背包的豚鼠（capybara）走进一个带有“酒店”标志的西部小镇，场景融合了后末世与蒸汽朋克元素。\u003c/p\u003e","title":"使用 AI 打造动画视频：TechHalla 的创意之旅"},{"content":"","date":null,"permalink":"/tags/mcp/","section":"Tags","summary":"","title":"Mcp"},{"content":"Anthropic的MCP（Model Control Protocol）作为一种新型接口，旨在让大语言模型与外部工具交互。我使用它一段时间后，有了一些思考。虽然它代表了AI领域的创新尝试，但也存在一些值得商榷的设计决策。本文将从实用角度分析MCP的优缺点，帮助你了解这一技术的现状。\n什么是MCP？ #简单来说，MCP是Anthropic开发的一种协议，让Claude等大语言模型能够与外部工具和服务进行标准化交互。它就像是AI与外部世界之间的\u0026quot;翻译官\u0026quot;，使模型能够调用各种API和服务来完成复杂任务。\n设计复杂性：是否必要？ #过度工程化的问题 #MCP的Python SDK设计让人感觉过于复杂。查看代码后，我发现它充斥着层层嵌套的包装器、访问器和装饰器，这些可能只需几行JSON和一个简单字典就能实现。\n# MCP Python SDK示例 - 复杂性展示 from anthropic.mcp import MCPClient, MCPServer, MCPTool from anthropic.mcp.decorators import tool_definition @tool_definition def simple_calculator(a: int, b: int, operation: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;一个简单的计算器工具\u0026#34;\u0026#34;\u0026#34; if operation == \u0026#34;add\u0026#34;: return a + b elif operation == \u0026#34;subtract\u0026#34;: return a - b # ... existing code ... 这种复杂性让人想起Java或TypeScript的开发风格，而不是Python的简洁哲学。对于许多开发者来说，这种设计增加了不必要的学习成本和使用门槛。\n为什么要重新发明轮子？ #一个核心问题是：为什么需要创建一个新服务器来包装已存在的API？我们已经有了REST和Swagger/OpenAPI这样成熟的API规范，为什么不直接利用这些现有技术？\n从实用角度看，从Swagger规范生成简化的JSON模式，使其与常用的工具定义对齐，会容易得多。这种方法可以减少代码重复，并利用已经存在的标准和工具。\nAPI集成的痛点 #现有API难以复用 #如果我想让大语言模型使用现有服务，为什么不让模型直接消费API呢？MCP要求我们为每个API创建新的包装层，这不仅增加了工作量，还引入了新的潜在故障点。\n更令人担忧的是，我没有看到太多关于安全性、访问控制的内容，这让人难以放心将MCP服务器暴露给大语言模型。虽然这是一项正在进行的工作，但感觉在为每个API创建新服务器上浪费了很多精力。\n更简单的替代方案 #直接利用现有的API规范（如OpenAPI）可能是一种更高效的方法：\n从现有API的Swagger/OpenAPI规范生成简化的JSON模式 使这些模式与常用的工具定义对齐 让模型直接调用这些API，而不是通过额外的中间层 这种方法不仅简化了开发流程，还能更好地利用现有的API生态系统。 架构不匹配：持久连接的问题 #与现代云架构的冲突 #MCP被设计为面向连接和有状态的接口，这与现代无状态云服务的架构理念不符。许多API现在运行在AWS Lambda或Cloudflare Workers等无状态环境中，而MCP的设计似乎没有充分考虑这一点。\n虽然Anthropic选择使用服务器发送事件（SSE）是个不错的决定，但即使去除网络组件，本地MCP服务器作为单独进程运行的方式仍然感觉臃肿且资源浪费。\n安全隐患 #将MCP服务器作为子进程运行引发了潜在的安全问题，特别是在权限控制和资源隔离方面。这种设计需要更严格的安全措施来防止可能的漏洞利用。\n上下文拥挤：选项过多 #模型上下文浪费 #在实际测试中，我发现MCP倾向于将所有可能的选项都塞入模型上下文。这不仅浪费了宝贵的tokens，还可能导致模型行为不稳定。\n例如，一个MCP服务器可能暴露以下所有功能：\n数据检索功能 数据处理功能 文件操作功能 API调用功能 数据库查询功能 图像处理功能 文本分析功能 \u0026hellip;更多功能 即使当前任务只需要其中一两个功能，所有这些选项都会被塞入模型上下文，这显然是不高效的。 智能路由的缺失 #MCP似乎缺少某种形式的\u0026quot;路由\u0026quot;或逐步、选择性暴露选项的机制。一个理想的解决方案应该能够根据当前任务的需求，只向模型暴露相关的工具和功能，这将大大提高效率和准确性。\n结论：期待更好的解决方案 #目前，MCP相比\u0026quot;标准\u0026quot;工具调用并没有显著优势。对于API集成，agents.json和围绕端点发现的概念可能是更自然的选择。\n当然，AI和大语言模型领域正处于快速发展阶段，最佳实践和标准还在不断演化。作为开发者，我们需要保持开放的心态，但也要对新技术保持健康的批判精神。\n虽然MCP代表了一种尝试标准化LLM与外部工具交互的努力，但其当前实现中的复杂性和设计决策使其难以在实际应用中广泛采用。随着这一领域的不断发展，我们可能会看到更简洁、更实用的解决方案出现。\n关于作者 #Rui Carmo #技术专家，对AI和大语言模型有深入研究。本文是他对Anthropic的MCP接口的个人评析和思考。\n","date":"2025年3月28日","permalink":"/posts/mcp%E8%AF%84%E6%9E%90anthropic%E7%9A%84%E6%96%B0%E6%8E%A5%E5%8F%A3%E6%8E%A2%E8%AE%A8/","section":"","summary":"\u003cp\u003eAnthropic的MCP（Model Control Protocol）作为一种新型接口，旨在让大语言模型与外部工具交互。我使用它一段时间后，有了一些思考。虽然它代表了AI领域的创新尝试，但也存在一些值得商榷的设计决策。本文将从实用角度分析MCP的优缺点，帮助你了解这一技术的现状。\u003c/p\u003e","title":"MCP评析：Anthropic的新接口探讨"},{"content":"过去一年，ChatGPT、GitHub Copilot和Claude等AI工具彻底改变了我们写代码的方式。这些工具声称能自动生成代码、解决复杂问题，甚至有人担心它们会取代程序员。但真实情况如何？程序员到底怎么用这些AI工具？\n本文汇总了数十位一线软件工程师的真实经验，揭示AI在编程中的实际应用、优势和局限性。\n图片来源：Wired\nAI使用模式 #根据调查，工程师们主要在这几个方面使用AI：\n1️⃣ 代码生成与补全 #这是最常见的使用场景。工程师们用AI（尤其是GitHub Copilot）来：\n生成样板代码 完成重复性任务 获取代码建议 2️⃣ 调试与问题解决 #当代码出现问题时，工程师们会把错误信息和代码片段提供给AI，寻求解决方案或解释。这对解决复杂错误特别有效。\n3️⃣ 学习与文档 #AI成为了学习新技术的得力助手，帮助工程师：\n快速掌握新编程语言或框架 生成代码文档和注释 解释复杂概念 4️⃣ 思路与架构设计 #与AI讨论软件架构、设计模式和算法选择，获取不同视角。这有助于解决复杂的设计问题。\n\u0026ldquo;AI不是在取代我的工作，而是让我能够专注于更高层次的问题。它处理了大量的样板代码和重复性任务，让我有更多时间思考架构和设计。\u0026rdquo; — 一位资深软件工程师\n代码生成实战 #代码生成是AI最直接的应用。调查显示，工程师们主要用AI生成这些代码：\n✅ 样板代码和重复性模式 ✅ 单元测试和测试用例 ✅ 数据转换和处理函数 ✅ API调用和集成代码 ✅ 配置文件和环境设置 使用AI生成代码的小技巧 # 提供详细上下文 - 越详细的需求说明，生成的代码越准确 仔细审查 - 不要盲目接受AI生成的每一行代码 迭代改进 - 多次调整提示，获取更精确的结果 比较多个方案 - 让AI生成不同解决方案进行对比 一位受访者坦言：\u0026ldquo;AI生成的代码通常90%正确，但剩下的10%可能藏着大问题。\u0026rdquo;\n调试神器 #AI在调试方面展现了惊人能力。工程师们经常将错误信息、日志和相关代码提供给AI，寻求解释和解决方案。\n真实案例：解决内存泄漏 # \u0026ldquo;我们团队被一个复杂的内存泄漏问题困扰了几周。我把调用栈、内存分析和相关代码提供给ChatGPT，它不仅找出了问题所在，还解释了原因并提供了修复方案。这至少节省了我们几天的调试时间。\u0026rdquo;\nAI在这些调试场景特别有用：\n🔍 错误解释 #把晦涩的错误消息翻译成人话，帮你快速理解问题本质。\n🔍 代码审查 #发现潜在问题、性能瓶颈和安全漏洞，提供改进建议。\n🔍 性能优化 #找出性能瓶颈，提供优化方案，提升代码效率。\n不过，AI在处理复杂系统时也有局限。一位系统架构师指出：\u0026ldquo;AI处理单个文件或函数的问题很在行，但面对跨多个服务的分布式系统问题时，它往往缺乏必要的上下文理解。\u0026rdquo;\n学习加速器 #AI已成为许多开发者学习新技术的秘密武器。无论是新语言、框架还是设计模式，AI都能提供个性化学习体验。\nAI如何帮你学习新技术 #📚 概念解释 #用通俗易懂的方式解释复杂概念，根据你的知识背景调整解释深度。\n📚 代码示例 #提供定制化代码示例，比官方文档更有针对性。\n📚 项目实践 #指导你完成小型项目，边做边学，效果显著。\n📚 最佳实践 #分享行业经验和常见陷阱，避免你走弯路。\n\u0026ldquo;以前学习新框架可能需要几周。现在，我几天内就能掌握基础并开始构建项目。AI不只告诉我\u0026rsquo;怎么做\u0026rsquo;，还解释\u0026rsquo;为什么这样做\u0026rsquo;。\u0026rdquo; — 一位全栈开发者\nAI的局限性 #尽管AI在编程领域潜力巨大，但受访工程师也指出了几个明显短板：\n⚠️ 幻觉与错误信息 #AI有时会生成看似合理但实际不正确的代码或解释，尤其是处理最新技术或冷门库时。\n⚠️ 上下文理解有限 #AI难以理解大型代码库的整体结构和业务逻辑，它通常只能处理你提供的代码片段，无法理解更广泛的系统架构。\n⚠️ 安全与隐私风险 #将代码提交给第三方AI服务可能带来安全和隐私风险，特别是处理敏感数据或专有代码时。许多公司已开始限制员工使用公共AI服务。\n⚠️ 代码质量问题 #AI生成的代码往往缺乏最佳实践，可能包含冗余、效率低下或难以维护的部分。它倾向于生成\u0026quot;能用\u0026quot;的代码，而不是\u0026quot;好\u0026quot;的代码。\n一位资深开发者总结道：\u0026ldquo;AI是强大工具，但不是万能的。它最好被视为一个聪明但经验有限的初级开发者，其建议需要有经验的工程师审查和指导。\u0026rdquo;\n未来展望 #随着AI技术不断发展，软件工程师与AI的协作方式也将持续演变。以下是几个主要趋势：\nAI在软件开发中的未来趋势 #🚀 更深入的代码理解 #未来的AI将能更好地理解大型代码库和系统架构，提供更全面的建议和解决方案。它们将能分析整个项目，而不仅是单个文件或函数。\n🚀 自动化软件维护 #AI将越来越多地用于自动化维护任务，如修复安全漏洞、优化性能、更新依赖项和重构代码。这让开发团队能专注于创新和新功能开发。\n🚀 个性化编程助手 #AI工具将变得更加个性化，能学习你的编码风格、偏好和工作流程。它们将成为真正的\u0026quot;编程伙伴\u0026quot;，而不仅是工具。\n🚀 本地和私有AI解决方案 #为解决安全和隐私问题，将出现更多本地运行的AI模型和私有云解决方案，让企业能在不泄露敏感代码的情况下利用AI优势。\n尽管AI在软件开发中的作用将继续扩大，但大多数受访工程师认为，AI不会完全取代人类程序员，而是改变他们的工作方式和重点。\n\u0026ldquo;未来的软件工程师将不再是那些能写出最多代码的人，而是那些能够最有效地与AI协作，指导AI生成高质量解决方案的人。这需要更高层次的系统思维和问题解决能力。\u0026rdquo; — 一位技术主管\n结论 #AI工具正在迅速改变软件开发的格局，但它们更像是强大的助手，而非替代品。最有效的方法是将AI视为开发团队的一部分，利用其优势处理重复性任务、生成样板代码和提供初步解决方案，同时依靠人类工程师的判断力、创造力和系统思维来指导和完善这些解决方案。\n成功使用AI进行软件开发的关键在于：\n理解其能力和局限性 建立适当的工作流程 培养有效的提示工程技能 随着这些工具不断发展，软件工程师需要持续适应和学习如何最有效地与AI协作。\n最终，AI不是在取代软件工程师，而是在改变他们的工作方式，使他们能够专注于更有创造性和战略性的任务，同时提高整体生产力和代码质量。\nAI与软件开发的关系 #🤝 工具，而非替代品 #AI是强大的工具，但需要人类的指导和判断才能发挥最大价值。\n🤝 互补关系 #AI擅长处理重复性任务和生成代码，人类擅长创新思维和系统设计。\n🤝 持续学习 #有效使用AI需要不断学习和适应，培养新的技能和工作流程。\n关于作者 #Aarian Marshall #Aarian Marshall是Wired杂志的资深科技记者，专注于报道人工智能、软件开发和技术趋势。本文基于对数十位软件工程师的深入采访，探讨了AI在实际编程工作中的应用。\n","date":"2025年3月28日","permalink":"/posts/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%A6%82%E4%BD%95%E5%AE%9E%E9%99%85%E4%BD%BF%E7%94%A8ai/","section":"","summary":"\u003cp\u003e过去一年，ChatGPT、GitHub Copilot和Claude等AI工具彻底改变了我们写代码的方式。这些工具声称能自动生成代码、解决复杂问题，甚至有人担心它们会取代程序员。但真实情况如何？程序员到底怎么用这些AI工具？\u003c/p\u003e","title":"软件工程师如何实际使用AI"},{"content":"我在2025年NICAR数据新闻会议上进行了两场演讲。本文是基于我对2024年LLM回顾的演讲，并扩展了几个月的内容，涵盖了2025年迄今为止发生的所有新进展。我的第二场演讲是关于前沿网络抓取技术的工作坊，我已在另一篇文章中详细介绍。\n以下是我对LLM新动向回顾的幻灯片和详细笔记，重点关注与数据新闻相关的趋势。\nLLM发展历程：从ChatGPT到现在 #2022年11月30日 - ChatGPT改变游戏规则 #ChatGPT的发布是AI领域的一个转折点。从技术角度看，它并不比GPT-3有重大技术飞跃，但聊天界面的包装使其成为有史以来增长最快的消费者应用之一。这一简单的界面改变使普通用户能够轻松访问强大的AI能力。\n2023年 - 相对平静的一年 #与2024年相比，2023年在LLM领域相对平静。当然也有一些引人注目的事件，比如必应聊天机器人因试图破坏Kevin Roose的婚姻而登上了《纽约时报》头版。\n2023年3月 - GPT-4独领风骚 #2023年最大的进步是GPT-4的发布，它最初由必应预览，然后在3月向所有人开放。在接下来的一年里，它几乎没有遇到真正的竞争对手。有一段时间，GPT-4似乎是一个无法超越的成就，没有其他公司能够赶上OpenAI。然而，这种局面在2024年彻底改变了。\n2024年的重大突破：竞争格局彻底改变 #2024年是LLM领域发展极为丰富的一年，发生了许多重大突破和变革。\nGPT-4级别模型成为\u0026quot;商品\u0026quot; #2024年，GPT-4的技术壁垒被彻底打破了。首先是Google的Gemini和Anthropic的Claude系列模型赶上了GPT-4的性能，随后几乎所有主要AI公司都推出了同等水平的模型。截至目前，已有18家实验室达到了这一里程碑，使GPT-4级别的能力几乎成为一种\u0026quot;商品\u0026quot;。\nOpenAI不再是无可争议的领导者 #OpenAI失去了在AI领域的绝对领先地位。多家公司的模型性能已经可以与OpenAI的模型相媲美，甚至在某些方面超越了它们。竞争的加剧推动了整个行业的创新速度。\n多模态能力成为标准配置 #过去约15个月最显著的趋势之一是多模态LLM的兴起。这些模型不仅能处理文本，还能出色地理解和处理图像，音频和视频处理能力也变得越来越实用。这极大扩展了LLM的应用场景。\n价格大幅下降，普及度提高 #如果你认为通过API访问这些强大模型仍然昂贵，那就需要重新评估了。大多数模型的价格都在大幅下降，使得更多开发者和企业能够负担得起这些技术。\n2025年最新动向：技术边界继续扩展 #虽然2025年才刚刚过去两个多月，但已经发生了许多重大进展，显示出AI技术继续快速发展的趋势。\n中国模型崭露头角 #一个重要趋势是中国模型的崛起，包括来自DeepSeek（DeepSeek v2和DeepSeek R1）和阿里巴巴的Qwen系列。这些模型在性能上已经可以与西方顶级模型相媲美，甚至在某些任务上表现更优。\n2025年最令人印象深刻的模型 #以下是2025年迄今为止给我留下最深刻的印象的模型发布：\nGemini 2.0系列：Pro Experimental、Flash和Flash-Lite版本 Claude 3.7 Sonnet：在理解和推理能力上有显著提升 OpenAI o3-mini：小型但功能强大的模型 GPT-4.5：OpenAI的最新旗舰模型 Mistral Small 3：在小型模型中表现卓越 如何评估模型性能？ #尽管有各种基准测试，但\u0026quot;实际使用感受\u0026quot;仍然是评估模型的最佳方式。对于数据新闻工作者来说，关键的一课是：如果要用这些模型做严肃的工作，我们需要自己的评估方法。例如，评估视觉OCR是否能够很好地处理警方报告，或者从文章中提取人物和地点的分类器是否准确。\n价格变革：从昂贵到平民化 #大多数情况下，模型价格正在大幅下降，但也有一些例外。\nGPT-4.5：高端市场的定价策略 #GPT-4.5是一个例外，它是一个非常昂贵的模型——比OpenAI当前最便宜的模型GPT-4o mini贵500倍！\nGPT-4.5价格 # 输入：$75.00 / 百万tokens 缓存输入：$37.50 / 百万tokens 输出：$150.00 / 百万tokens GPT-4o价格 # 输入：$2.50 / 百万tokens 缓存输入：$1.25 / 百万tokens 输出：$10.00 / 百万tokens GPT-4o mini价格 # 输入：$0.150 / 百万tokens 缓存输入：$0.075 / 百万tokens 输出：$0.600 / 百万tokens 与此同时，谷歌的Gemini模型提供了一些非常经济的选项。例如，使用Gemini 1.5 Flash 8B模型为68,000张照片生成描述只需花费$1.68，这在以前是不可想象的。\n本地模型的崛起：笔记本电脑也能运行强大AI #大约六个月前，我对可以在自己笔记本电脑上运行的模型失去了兴趣，因为它们的性能与云端托管模型相比差距明显。\n然而，这种情况已经发生了根本性变化——首先是Qwen 2.5 Coder，然后是Llama 3.3 70B，最近则是Mistral Small 3。这些模型都可以在同一台笔记本电脑上运行——一台64GB Apple Silicon MacBook Pro。\n令人惊讶的是，我现在可以在这台普通硬件上运行的模型确实非常实用，其中一些模型的表现让我感到与2023年初次体验GPT-4时一样惊艳。这意味着强大的AI能力正在走向真正的普及化。\n代码生成能力：从辅助到自主创作 #LLM在编写代码方面的表现已经超出了大多数人的预期，这一点已经毋庸置疑。\nClaude Artifacts：完整应用生成 #Claude的Artifacts功能允许模型生成完整的Web应用程序，包括HTML、CSS和JavaScript，几乎不需要人工干预。\nChatGPT代码解释器：安全执行环境 #这一功能允许用户在安全的沙盒环境中执行由ChatGPT生成的代码，特别适合数据分析和可视化任务，大大简化了从数据到洞察的过程。\nChatGPT Canvas：灵活的创作界面 #提供更灵活的界面来生成和编辑代码，支持多种编程语言和框架，使非专业开发者也能创建复杂应用。\n\u0026ldquo;Vibe编程\u0026rdquo;：新的编程范式 #\u0026ldquo;Vibe编程\u0026quot;是Andrej Karpathy创造的一个新术语，指的是使用LLM编写代码的方式：你只需描述你想要什么，然后输入任何错误或bug，看看AI是否能修复它们。这是一种探索AI编程能力的有趣方式，虽然有一些明显的局限性，但已经改变了许多开发者的工作方式。\n模型生成代码已成为商品 #模型能够输出完整的HTML+JavaScript自定义界面的能力如此强大且广泛可用，以至于它已经成为一种商品。WebDev Arena的存在就是一个证明——这是一个聊天机器人竞技场的衍生产品，你可以对两个模型运行相同的提示，比较哪个创建的应用程序更好。\n推理能力的提升：AI开始\u0026quot;思考\u0026rdquo; #2025年迄今为止的另一个重大趋势是\u0026quot;推理时计算\u0026quot;，也称为推理能力。\nOpenAI的o1和o3、DeepSeek R1、Qwen QwQ、Claude 3.7 Thinking和Gemini 2.0 Thinking都是这种模式的例子。\n\u0026ldquo;逐步思考\u0026quot;的进化 #这是模型在回答前\u0026quot;思考\u0026quot;问题的功能。它是几年前\u0026quot;逐步思考\u0026quot;技巧的进化版本，只是现在它已经内置到模型中。这种能力在处理代码和数学问题时特别有效，显著提高了回答的准确性。\n有趣的黑客技巧 #一个非常有趣的新发现：事实证明你可以\u0026quot;黑入\u0026quot;这些模型，拦截它们尝试用\u0026lt;/think\u0026gt;结束思考的行为，并将其替换为\u0026quot;等等，但是\u0026rdquo;——这会让它们\u0026quot;思考\u0026quot;得更深入！这种技巧展示了模型内部工作机制的一些有趣特性。\nOCR与PDF处理：解锁被困信息 #对新闻工作者来说，这可能是最实用的进展之一。世界上很多重要信息都被困在难以处理的PDF文件中。\n视觉LLM正在接近能够彻底解决这个问题的水平。\nPDF直接处理能力 #Gemini和Claude等模型现在可以直接接受PDF文件作为输入。对于其他模型（包括OpenAI的模型），你需要先将PDF分解为图像——每页一个PNG文件通常效果很好。\n最佳OCR模型对比 #迄今为止，我看到的针对PDF的最佳结果来自Gemini。Mistral OCR刚刚发布——虽然我还没有完全评估它的性能，但初步测试表明它在某些场景下可能与最新的Gemini不相上下。\n模型处理敏感信息的进步 #模型在处理敏感信息方面也在不断改进。例如，早期的Claude 3 Opus在处理竞选财务报告时会拒绝将其转换为JSON格式，认为这可能会导致个人信息被滥用。而最新的Claude 3.7 Sonnet则能够正确处理这类请求，提供结构化的JSON输出。这是模型随时间改进的一个很好例子，显示出AI系统在平衡功能性和安全性方面的进步。\n结论：AI革命才刚刚开始 #LLM领域正在以惊人的速度发展。从2022年底ChatGPT的发布到现在，我们已经看到了多模态能力的崛起、本地模型的显著改进、代码生成能力的提升以及推理能力的增强。价格也在大幅下降，使这些强大的工具更加普及。\n对于数据新闻工作者来说，这些进步提供了前所未有的机会，特别是在处理PDF文档、生成代码和分析数据方面。然而，重要的是开发自己的评估方法，确保这些工具在特定用例中的有效性。\n如果你在新闻编辑室工作并且正在研究这些技术，我很乐意通过Zoom与你的团队交流。请通过电子邮件联系我。\n关于作者 #Simon Willison #Simon Willison是一位著名程序员和技术专家，专注于AI和数据领域。本文基于他在2025年3月7日NICAR会议上的演讲，通俗地介绍了AI在过去一年的巨大飞跃，是了解LLM最新发展的绝佳综述。\n","date":"2025年3月21日","permalink":"/posts/llm%E4%B8%96%E7%95%8C%E7%9A%84%E6%96%B0%E5%8A%A8%E5%90%912025%E5%B9%B4%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95/","section":"","summary":"\u003cp\u003e我在2025年NICAR数据新闻会议上进行了两场演讲。本文是基于我对2024年LLM回顾的演讲，并扩展了几个月的内容，涵盖了2025年迄今为止发生的所有新进展。我的第二场演讲是关于前沿网络抓取技术的工作坊，我已在另一篇文章中详细介绍。\u003c/p\u003e\n\u003cp\u003e以下是我对LLM新动向回顾的幻灯片和详细笔记，重点关注与数据新闻相关的趋势。\u003c/p\u003e","title":"LLM世界的新动向：2025年大语言模型最新进展"},{"content":"","date":null,"permalink":"/tags/%E6%8A%80%E6%9C%AF%E8%B6%8B%E5%8A%BF/","section":"Tags","summary":"","title":"技术趋势"},{"content":"","date":null,"permalink":"/tags/programming/","section":"Tags","summary":"","title":"Programming"},{"content":"","date":null,"permalink":"/tags/tradecraft/","section":"Tags","summary":"","title":"Tradecraft"},{"content":"","date":null,"permalink":"/tags/visual-programming/","section":"Tags","summary":"","title":"Visual-Programming"},{"content":"可视化编程曾被誉为编程的未来，承诺让编程变得更加直观、易学且高效。然而，几十年过去了，可视化编程工具仍然主要停留在表单层面，未能真正取代文本编程。本文将探讨这一现象背后的原因，分析可视化编程的历史、局限性以及未来可能的发展方向。\n尽管可视化编程在某些领域取得了成功，如游戏开发（Unity、Unreal）、数据分析（Tableau）和自动化工作流（Zapier、IFTTT），但它在通用编程领域的应用仍然有限。为什么可视化编程工具无法超越表单界面的限制？为什么程序员仍然偏爱文本编辑器和IDE？这些问题值得我们深入思考。\n核心问题：可视化编程工具的设计通常专注于如何让初学者更容易入门，而不是如何让专业开发者更高效地工作。这种设计理念导致了可视化编程工具在复杂项目中的局限性。\n可视化编程的历史演进 #可视化编程的历史可以追溯到20世纪60年代，经历了多次发展浪潮，但始终未能在通用编程领域取得突破性进展。\n1960年代 - 开创性探索 #Ivan Sutherland的Sketchpad系统开创了人机交互的新时代，允许用户通过光笔直接在屏幕上绘制和操作图形对象。这一创新为后来的可视化编程奠定了基础，展示了计算机与人类交互的新可能性。\n1970-1980年代 - 图形化编程语言萌芽 #这一时期出现了多种图形化编程语言，如Prograph、Pygmalion和ThingLab。这些系统尝试通过图形表示来简化编程过程，但大多数仍然局限于学术研究，未能进入主流应用。\n1990年代 - 商业化尝试与突破 #Visual Basic和LabVIEW等工具开始将可视化编程引入商业领域。Visual Basic通过拖放界面元素和属性编辑器简化了Windows应用程序的开发，而LabVIEW则专注于科学和工程领域的数据流编程，为特定领域用户提供了便利。\n2000年代至今 - 专业化与多样化发展 #可视化编程工具开始在特定领域取得成功：\n教育领域：Scratch和Blockly让编程教育变得更加直观有趣 游戏开发：Unity和Unreal Engine的可视化工具链降低了游戏开发门槛 自动化领域：Node-RED和Zapier简化了工作流自动化过程 然而，在通用软件开发领域，文本编程仍然占据主导地位。\n表单困境：可视化编程的瓶颈 #大多数可视化编程工具最终都演变成了复杂的表单系统。用户通过拖放组件，然后填写属性面板中的各种字段来配置这些组件。这种方法虽然直观，但也带来了一系列问题。\n空间效率低下 #表单界面占用大量屏幕空间，导致用户需要不断滚动和切换窗口。相比之下，文本代码更加紧凑，能在有限空间内表达更多信息。一个简单的功能可能需要在可视化界面中占据整个屏幕，而在文本代码中只需几行。\n可见性与可发现性问题 #表单界面往往将大量选项隐藏在嵌套菜单和标签页中，使用户难以发现和理解所有可用功能。文本代码则可以通过自动补全和文档提示更有效地展示可用选项，让开发者能够更快地找到所需功能。\n输入效率瓶颈 #对于熟练的开发者来说，通过键盘输入文本代码通常比使用鼠标在表单界面中导航和填写字段更快。表单界面经常打断开发者的思维流程，降低工作效率。当你正在专注思考问题解决方案时，不得不停下来寻找正确的表单字段会打断创造性思维。\n版本控制的挑战 #表单生成的代码通常以二进制或难以阅读的格式存储，这使得版本控制和协作变得困难。相比之下，文本代码可以轻松地使用Git等工具进行版本控制和差异比较，便于团队协作和代码审查。\n表单界面的本质是将编程抽象为填写表格的过程，这种抽象虽然降低了入门门槛，但也限制了表达能力和灵活性。真正强大的可视化编程需要超越表单，提供更丰富、更直观的交互方式。\n可视化编程的深层次局限 #除了表单问题外，可视化编程还面临着一系列更深层次的局限性，这些局限性解释了为何它在某些领域难以取代文本编程。\n抽象与复杂性的矛盾 #可视化编程工具通常试图通过抽象来简化编程过程，但这种抽象往往无法处理复杂的编程概念和模式。当项目复杂度增加时，可视化表示可能变得混乱且难以管理。\n例如，在处理复杂的数据结构、递归算法或高阶函数时，图形化表示可能变得异常复杂，反而增加了理解难度。文本代码通过抽象语法和结构化组织，能够更有效地表达这些复杂概念。\n表达能力与灵活性的限制 #可视化编程工具通常限制了用户的表达能力，只允许使用预定义的组件和操作。这种限制在简单任务中可能不明显，但在需要创新解决方案的复杂问题中可能成为障碍。\n文本编程语言提供了更高的灵活性和表达能力，允许开发者创建自定义抽象、设计模式和算法。这种灵活性对于解决新颖问题和优化性能至关重要。\n性能与规模的挑战 #可视化编程工具通常生成的代码效率较低，或者需要额外的运行时支持。这在小型项目中可能不是问题，但在大型、性能敏感的应用程序中可能成为瓶颈。\n此外，可视化编程环境本身在处理大型项目时可能变得缓慢，因为需要渲染和管理大量图形元素。文本编辑器在处理大型代码库方面通常更加高效。\n成功案例：可视化编程的闪光点 #尽管存在局限性，可视化编程在某些特定领域取得了显著成功。分析这些成功案例可以帮助我们理解可视化编程的潜力和适用场景。\n游戏开发领域的突破 #Unity和Unreal Engine的可视化脚本系统（如Blueprints）允许游戏设计师和艺术家无需编写代码即可创建复杂的游戏逻辑。这些系统成功的关键在于它们专注于特定领域，并与文本编程无缝集成，让不同背景的团队成员能够协同工作。\n数据分析与可视化的革新 #Tableau、Power BI和KNIME等工具通过可视化界面简化了数据分析和可视化过程。这些工具成功的原因在于它们将复杂的数据操作抽象为直观的可视化组件，同时保留了足够的灵活性，让数据分析师能够快速探索和理解数据。\n自动化与集成的普及 #Zapier、IFTTT和Microsoft Power Automate等工具通过可视化界面使非技术用户能够创建复杂的自动化工作流。这些工具成功的关键在于它们将复杂的API集成简化为易于理解的触发器和动作，让普通用户也能实现过去需要专业开发者才能完成的任务。\n成功因素分析 #这些成功案例有几个共同特点：\n领域聚焦：专注于特定领域，而非通用编程 适当抽象：提供适当的抽象级别，既简化了常见任务，又保留了足够的灵活性 混合模式：与文本编程无缝集成，允许在需要时切换到代码视图 用户导向：针对目标用户群体（通常是领域专家而非专业程序员）优化用户体验 未来发展：突破表单的桎梏 #尽管可视化编程面临诸多挑战，但随着技术的进步和设计理念的革新，可视化编程仍然有望突破当前的局限，实现新的发展。以下是几个可能的发展方向：\n人工智能辅助编程 #人工智能可以帮助可视化编程工具更智能地理解用户意图，自动生成和优化代码，提供上下文相关的建议。AI可以弥合可视化编程和文本编程之间的差距，创造更自然、更高效的编程体验。\n想象一下，当你拖放组件时，AI能够理解你的意图，自动完成常见模式，甚至提供性能优化建议，这将大大提高可视化编程的效率和表达能力。\n空间编程的新维度 #虚拟现实和增强现实技术可以为可视化编程提供全新的交互方式。开发者可以在三维空间中直观地构建和操作代码结构，克服传统二维界面的空间限制，创造更沉浸式的编程体验。\n在VR/AR环境中，代码可以成为可触摸、可操作的立体结构，让开发者能够从不同角度查看和理解复杂系统，这可能会彻底改变我们与代码交互的方式。\n双向编辑的无缝体验 #未来的可视化编程工具可能会实现真正的双向编辑，允许开发者无缝地在可视化表示和文本代码之间切换，同时保持两者的同步。这种方法结合了两种编程范式的优点，为不同背景的开发者提供灵活的工作方式。\n开发者可以根据任务需求自由选择最适合的表示方式，既可以利用可视化界面的直观性，又可以利用文本代码的精确性和灵活性。\n领域特定语言的繁荣 #可视化编程可能会更加专注于创建领域特定的可视化语言，而不是试图成为通用编程的替代品。这些专业化的工具可以为特定领域的专家提供高度优化的编程体验，同时与通用编程语言保持互操作性。\n例如，机器学习、物联网、生物信息学等领域可能会出现专门的可视化编程工具，这些工具能够直观地表达领域概念，大大提高领域专家的工作效率。\n超越表单的设计理念 #未来的可视化编程需要超越传统的表单界面，探索更直观、更表达力强的视觉语言。这可能包括：\n基于约束的编程模型：允许开发者通过指定关系和约束来定义程序行为，而不是详细的步骤 直接操作界面：使开发者能够直接与程序的运行时状态交互，实时看到变化的效果 自适应可视化：根据上下文和任务自动调整视觉表示的抽象级别，在简洁和详细之间找到平衡 协作编程环境：支持多人同时在同一项目上工作，并可视化展示每个人的贡献和代码变化 结论：重新思考可视化编程 #可视化编程的困境在于它试图通过表单界面简化编程，但这种简化往往以牺牲表达能力和灵活性为代价。虽然可视化编程在特定领域取得了成功，但在通用编程领域，它仍然面临着重大挑战。\n未来的可视化编程需要超越表单，探索新的交互模式和视觉语言，同时与文本编程建立更紧密的联系。它不应该被视为文本编程的替代品，而应该是对编程范式的补充和扩展，为不同背景和需求的开发者提供多样化的工具。\n随着技术的进步和设计理念的革新，可视化编程有望突破当前的局限，实现新的发展。人工智能、虚拟现实和新型交互设计等技术可能会为可视化编程带来革命性的变化，使其真正成为编程的未来。\n可视化编程的真正价值不在于取代文本编程，而在于扩展编程的可能性，使更多人能够参与到软件创造的过程中，并为特定领域提供更高效、更直观的解决方案。\n关于作者 #Ivan Reese #Ivan Reese是一位软件开发者和设计师，对可视化编程和人机交互有着深入的研究。他的文章《Visual Programming is Stuck on the Form》深入分析了可视化编程的现状和未来发展方向。\n","date":"2025年3月14日","permalink":"/posts/%E5%8F%AF%E8%A7%86%E5%8C%96%E7%BC%96%E7%A8%8B%E7%9A%84%E5%9B%B0%E5%A2%83%E4%B8%BA%E4%BD%95%E6%AD%A2%E6%AD%A5%E4%BA%8E%E8%A1%A8%E5%8D%95/","section":"","summary":"\u003cp\u003e可视化编程曾被誉为编程的未来，承诺让编程变得更加直观、易学且高效。然而，几十年过去了，可视化编程工具仍然主要停留在表单层面，未能真正取代文本编程。本文将探讨这一现象背后的原因，分析可视化编程的历史、局限性以及未来可能的发展方向。\u003c/p\u003e\n\u003cp\u003e尽管可视化编程在某些领域取得了成功，如游戏开发（Unity、Unreal）、数据分析（Tableau）和自动化工作流（Zapier、IFTTT），但它在通用编程领域的应用仍然有限。为什么可视化编程工具无法超越表单界面的限制？为什么程序员仍然偏爱文本编辑器和IDE？这些问题值得我们深入思考。\u003c/p\u003e","title":"可视化编程的困境：为何止步于表单？"},{"content":"在软件工程领域，写代码只是成功的一小部分。真正的行业技艺（Tradecraft）是那些学校里没教，却能决定你职业高度的隐形技能。\n想象一下：两位工程师，技术能力相当，五年后，一位成为团队的技术领袖，另一位仍在原地踏步。差别在哪里？往往就是这些不成文的规则和策略。\n本文汇集了多年一线经验，帮你揭开软件行业的\u0026quot;潜规则\u0026quot;，让你少走弯路，加速职业发展。\n核心原则：职场生存的基石 #诚信为本：你最值钱的资产 #真实案例：小王在一次系统故障中，本可以掩盖自己的错误，但他选择坦诚承认并提出解决方案。结果不仅没有受到责备，反而赢得了团队的信任，后来被提拔为核心开发组成员。\n记住：你的声誉会比当前工作持续更久。一次失信，可能需要十次守信才能弥补。\n持续学习：技术人的生存法则 #每天至少投入30分钟学习新技术。可以是：\n阅读技术博客 尝试新工具 参与开源项目 解决一个编程挑战 行动清单：\n建立个人学习计划 找到3个值得定期阅读的技术资源 每周尝试一个新的编程技巧 解决问题，而非代码：思维方式的转变 #优秀的工程师关注的是解决业务问题，而非仅仅编写代码。下次接到需求时，先问自己：\n这个需求背后的真正目的是什么？ 有没有不需要写代码就能解决的方法？ 最简单有效的解决方案是什么？ 思考题：回想你最近完成的项目，有没有可能用更简单的方法解决？\n团队合作 #软件开发是团队运动。学会与不同背景、性格和技能水平的人有效合作。分享知识，接受反馈，共同成长。个人英雄主义在长期看来往往适得其反。\n技术技能让你获得工作，但行业技艺让你在职业生涯中持续进步。最成功的工程师不仅精通技术，还精通人际关系和组织动态。\n沟通技巧 #有效沟通是软件工程师最重要的非技术技能。无论你的代码多么出色，如果无法清晰地表达想法、理解需求或说服他人，你的影响力将受到限制。\n受众导向沟通 #根据沟通对象调整你的语言和深度。与产品经理交流时，关注业务价值和用户体验；与其他工程师交流时，可以深入技术细节；与高管交流时，突出战略影响和关键指标。\n沟通对象 沟通重点 避免事项 技术团队 实现细节、技术挑战、代码质量 过度简化技术概念 产品团队 可行性、时间估计、用户体验影响 过多技术术语 管理层 业务价值、风险、资源需求 无关的技术细节\n主动沟通 #不要等到问题变大才沟通。提前分享进展、障碍和风险。定期更新利益相关者，即使只是确认一切按计划进行。没有消息往往被解读为坏消息。\n文档能力 #优秀的文档是强大的沟通工具。学会编写清晰的设计文档、README和注释。好的文档减少会议，加速入职，并作为你技术思维的展示窗口。\n提问艺术 #学会提出好问题。在寻求帮助前，先研究问题，尝试解决方案，准备具体例子。展示你的思考过程，而不只是寻求答案。这样既能获得更好的帮助，也能展示你的学习态度。\n职业成长 #职业发展不是自然而然发生的，而是需要有意识地规划和行动。了解行业生态、建立个人品牌、寻找导师和赞助者，这些都是加速职业成长的关键策略。\n职业阶梯与成长路径 #了解你所在公司和行业的职业发展路径。软件工程师通常有两条主要发展路线：技术专家路线和管理路线。根据自己的兴趣和优势选择适合的方向。\n初级工程师 #专注于技术基础，学习编码规范，理解现有系统，在指导下完成任务。\n中级工程师 #独立完成功能开发，参与设计讨论，开始指导初级工程师，深入特定技术领域。\n高级工程师 #领导技术决策，设计系统架构，指导团队成员，解决复杂问题，与业务紧密合作。\n技术专家路线 #架构师、技术专家、首席工程师 - 深入技术领域，影响公司技术方向。\n管理路线 #技术主管、工程经理、技术总监 - 专注于团队建设、项目管理和组织领导。\n建立个人品牌 #在行业内建立自己的专业声誉。可以通过技术博客、开源贡献、演讲或社区参与来展示你的专业知识。个人品牌不仅帮助你获得更好的工作机会，还能扩大你的影响力和人脉网络。\n寻找导师与赞助者 #导师提供建议和指导，而赞助者则积极为你创造机会。两者对职业发展都至关重要。主动寻找经验丰富的专业人士作为导师，同时通过出色的工作表现吸引赞助者的注意。\n职业成长技巧 # 定期进行自我评估，了解自己的优势、劣势和发展方向 寻求具有挑战性的项目，即使它们超出你当前的舒适区 建立广泛的专业网络，不仅限于技术圈子 学会讲述你的成就故事，突出你的贡献和影响 定期更新简历和在线档案，即使你不在找工作 技术能力 #虽然行业技艺强调软技能，但扎实的技术基础仍然是软件工程师的核心竞争力。以下是提升技术能力的关键策略。\nT型技能模型 #发展\u0026quot;T型\u0026quot;技能组合：横向是广泛的技术知识，纵向是一两个专精领域。这种组合使你既能理解系统全局，又能在特定领域做出深入贡献。\n系统思维 #培养系统思维能力，理解组件之间的交互和依赖关系。优秀的工程师不仅关注单个功能，还考虑整体架构、性能、安全性和可维护性。这种思维有助于设计更健壮的系统。\n工具掌握 #精通你的工具链。深入学习编辑器、调试工具、版本控制系统和自动化工具。投资时间配置开发环境，创建有用的快捷键和脚本。良好的工具掌握可以显著提高生产力。\n代码质量与可维护性 #编写高质量代码不仅是技术能力的体现，也是对团队的respect。可维护的代码具有以下特点：\n清晰的命名和结构，使意图明确 适当的注释，解释\u0026quot;为什么\u0026quot;而非\u0026quot;是什么\u0026quot; 合理的抽象和模块化，降低复杂度 全面的测试覆盖，确保功能正确性 一致的代码风格，遵循团队约定 记住：你写的代码很可能会被他人（包括未来的你）阅读和修改多次。优化代码的可读性和可维护性，就是优化团队的长期效率。\n职场动态 #理解组织文化和职场动态对于职业成功至关重要。技术决策往往受到组织因素的影响，学会在这种环境中有效导航是行业技艺的重要组成部分。\n理解组织文化 #每个公司都有独特的文化和价值观。花时间观察和理解你所在组织的决策方式、沟通风格和优先事项。适应组织文化并不意味着放弃个人原则，而是找到有效工作的方式。\n建立联盟 #在组织中建立积极的关系网络。与不同团队和部门的人建立联系，理解他们的目标和挑战。这些关系不仅有助于获取信息和资源，还能在需要支持时提供帮助。\n组织政治导航 #\u0026ldquo;组织政治\u0026quot;这个词常带有负面含义，但实际上它只是描述了人们在组织中如何互动、如何做决策以及如何分配资源的过程。理解并有效参与这一过程是职业成功的关键。\n有效策略 避免行为 理解各方利益和动机 忽视组织动态，纯粹从技术角度思考 建立广泛的关系网络 只与直接团队成员交流 寻找共赢解决方案 坚持己见，不考虑他人需求 战略性选择战斗 对每个决定都争论不休 给予和接受帮助，建立互惠关系 只在需要时才与他人互动\n管理期望 #学会有效管理他人对你工作的期望。承诺少于你能交付的，然后超额完成。清晰沟通项目状态、风险和时间线。避免意外惊喜，特别是负面的。\n影响力构建 #影响力来自于信任和专业知识的结合。通过一致地交付高质量工作、分享知识、帮助他人和提供有价值的见解来建立影响力。真正的影响力不需要职位权力。\n结论 #行业技艺是软件工程师职业成功的关键因素，它弥补了正式教育与实际工作之间的差距。通过掌握这些非正式但至关重要的技能，你可以在职业生涯中取得更大的成功和满足感。\n记住，行业技艺不是一次性学习的知识，而是需要持续实践和反思的技能集合。随着你职业的发展，不断调整和完善你的方法，适应新的环境和挑战。\n最后，分享你的知识和经验。帮助新一代工程师学习这些技能，不仅能够回馈社区，还能加深你自己的理解和影响力。\n\u0026ldquo;在软件工程中，代码只是表面；真正的技艺在于理解人、流程和组织如何共同工作以创造有价值的产品。\u0026rdquo;\n关于作者 #Geoffrey Huntley #Geoffrey Huntley是一位经验丰富的软件工程师和技术领导者，专注于帮助开发者提升职业技能和行业洞察。他的文章《Tradecraft》分享了多年以来在软件行业积累的实战经验和策略。\n","date":"2025年3月14日","permalink":"/posts/%E8%A1%8C%E4%B8%9A%E6%8A%80%E8%89%BA%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E7%94%9F%E5%AD%98%E7%A7%98%E7%B1%8D/","section":"","summary":"\u003cp\u003e在软件工程领域，写代码只是成功的一小部分。真正的行业技艺（Tradecraft）是那些学校里没教，却能决定你职业高度的隐形技能。\u003c/p\u003e\n\u003cp\u003e想象一下：两位工程师，技术能力相当，五年后，一位成为团队的技术领袖，另一位仍在原地踏步。差别在哪里？往往就是这些不成文的规则和策略。\u003c/p\u003e","title":"行业技艺：软件工程师的生存秘籍"},{"content":"","date":null,"permalink":"/tags/design/","section":"Tags","summary":"","title":"Design"},{"content":"","date":null,"permalink":"/tags/sensory-design/","section":"Tags","summary":"","title":"Sensory Design"},{"content":"","date":null,"permalink":"/tags/ux/","section":"Tags","summary":"","title":"Ux"},{"content":"我们通常认为网站是视觉的产物，但有没有想过，能否让网站触动用户的嗅觉、听觉甚至触觉？本文将探讨如何超越传统的视觉设计，巧妙运用设计策略，唤起用户的多种感官体验，创造更具沉浸感和情感连接的网站。灵感来源于 Smashing Magazine 的文章 \u0026ldquo;Can You Design A Website For The Five Senses?\u0026quot;，并结合更多实例进行分析。\n传统的网页设计往往只关注“看”，忽略了其他感官的力量。然而，优秀的用户体验设计，应该追求更深层次的互动。一个能调动多感官的网站，不仅能给用户留下更深刻的印象，还能建立更强的情感连接，最终提升用户满意度和转化效果。\n接下来，我们将逐一探索如何通过设计，间接模拟或唤起用户的视觉、嗅觉、听觉和触觉体验。（味觉由于高度依赖物理交互，在纯数字环境中实现难度极大，暂不深入讨论，但其理念仍有启发意义。）\n视觉设计：超越表象，直抵人心 #视觉是网站设计的基础，也是最直接的感官入口。出色的视觉设计不仅关乎美感，更能传递品牌价值、引导用户行为，甚至影响用户情绪。\n色彩的力量与文化差异： 色彩心理学 (颜色理论) 是设计师的有力武器。不同颜色能唤起不同情感：蓝色代表信任，绿色象征自然，红色激发热情。但请注意，颜色的含义并非放之四海而皆准，它深受文化背景影响。设计师必须考虑目标受众，避免色彩误用。例如，黄色在某些文化中象征快乐，在另一些文化中则可能带有负面含义。\n营造沉浸式视觉体验： Visit Philly 网站就是绝佳范例。它通过高质量的全屏图片，成功营造了身临其境的视觉感受。\n每个页面都像一扇窗，直接展示费城的魅力，避免了冗余的视频背景或令人疲惫的轮播图。一张精心挑选的图片，精准传达了目的地的氛围。\n对于去过的游客，熟悉的场景能唤起美好回忆；对于未去过的人，引人入胜的画面则激发向往，提前“体验”城市魅力，这是一种高效的营销。\n清晰的视觉层次： 除了美观，视觉设计还需承担信息传递的重任。清晰的视觉层次、易读的排版和适当的留白，能帮用户快速理解内容，提升浏览效率。通过对比、字体大小、粗细和布局，引导用户的视线流向关键信息和操作按钮。\n嗅觉设计：用视觉唤醒气味的记忆 #嗅觉与情感和记忆紧密相连。虽然网站无法直接传递气味，但我们可以利用视觉元素，巧妙地唤起用户对特定气味的联想。\n触发嗅觉联想的图像：\n一个卖旧书的网站，配上泛黄书卷的图片，如 Providence Athenaeum，就能让爱书人仿佛“闻到”图书馆特有的墨香和纸张气息，产生亲切感。 Tide 洗衣液网站展示洁净衣物的图片，即使不用该品牌，用户也能联想到衣物清新的气味，这通常与舒适、安心感相关。 视频增强嗅觉联想： Coffee Culture Cafe＆Eatery 网站首页视频展示了咖啡豆的原始状态。 对咖啡爱好者而言，这几乎能让他们“闻”到浓郁的咖啡香，唤起对一杯热咖啡的期待。视频比静态图片更能强化这种感官联想。\n听觉设计：无声胜有声的力量 #听觉常被网站设计忽略，但这不代表它不重要。即使没有背景音乐，我们也可以通过视觉元素唤起用户对特定声音的联想，或营造静谧的氛围。\n触发声音联想的图像：\nKindermusik 网站展示孩子玩木琴的图片。木琴是许多人的童年回忆，看到图片，人们很容易联想到清脆的琴声。 该网站“教学成效”页面顶部的婴儿微笑照片，也能让人联想到婴儿清脆的笑声，给正在选择教育机构的父母带来积极暗示。 利用“静音”设计： 另一种处理听觉的方式是刻意营造安静。Scandinave Spa 网站通过视觉设计，有效地传递了其核心价值——宁静与放松。 网站图片多为宁静的自然风光或放松场景，色彩柔和，构图简洁。这种**“静音”设计**与 Spa 提供的无噪音体验相呼应，强调了逃离喧嚣、回归平静的主题。避免使用嘈杂、闪烁或拥挤的元素，营造出与现实生活对比鲜明的宁静感。\n触觉设计：感受界面的“温度”与“质感” #触觉是感知物理世界的重要方式。虽然我们无法触摸屏幕来感受纹理，但可以通过视觉元素传递触感的联想，例如材质纹理、物体形状、重量感，以及人与人之间的接触。\n通过图像传递触感： Massage Envy 网站展示人们接受按摩的图片，试图传递放松和舒适感。 这比只展示空房间要好，但图片的真实感传递力可能有限。\n视频增强触觉体验： Bodhi Spa 网站使用视频更生动地展示护理服务。 视频不仅展示手法，更通过镜头和表情传递触感的细微变化和舒适感受，比静态图片更能引发触觉联想。 展示夫妇在盐疗池放松的场景，也传递了水流、盐晶和漂浮的触感。\n界面交互中的“触觉”反馈： 现代 Web 技术也允许在界面交互中提供“触觉”反馈。鼠标悬停动画、按钮点击效果、表单成功提示等，都能在一定程度上模拟物理世界的反馈，增强用户的操作感知。\n结论：超越视觉，拥抱多感官设计 #为五种感官设计网站并非易事，完全模拟所有感官在数字环境中仍有挑战。然而，通过巧妙运用视觉元素，我们可以有效地唤起用户对嗅觉、听觉和触觉的联想，甚至通过“静音”设计来强调听觉的缺失。\n未来的网页设计将更加注重多感官体验。随着触觉反馈设备、虚拟现实（VR）和增强现实（AR）等技术的发展，我们将能在更深层次上触动用户感官。想象一下：在线服装店让用户“感受”面料质感；食品电商网站让用户“闻到”食物香味（尽管仍在实验阶段）。\n总之，设计师应不断探索新方法和技术，超越视觉的局限，创造更具沉浸感、情感连接和令人难忘的多感官网站体验。一个能触动用户多重感官的网站，必将在竞争中脱颖而出，赢得用户的青睐。\n","date":"2020年8月7日","permalink":"/posts/%E7%BD%91%E7%AB%99%E8%AE%BE%E8%AE%A1%E6%96%B0%E7%BB%B4%E5%BA%A6%E4%B8%BA%E4%BA%94%E7%A7%8D%E6%84%9F%E5%AE%98%E6%89%93%E9%80%A0%E6%B2%89%E6%B5%B8%E5%BC%8F%E4%BD%93%E9%AA%8C/","section":"","summary":"\u003cp\u003e我们通常认为网站是视觉的产物，但有没有想过，能否让网站触动用户的\u003cstrong\u003e嗅觉、听觉甚至触觉\u003c/strong\u003e？本文将探讨如何超越传统的视觉设计，巧妙运用设计策略，唤起用户的多种感官体验，创造更具沉浸感和情感连接的网站。灵感来源于 Smashing Magazine 的文章 \u0026ldquo;\u003ca href=\"https://www.smashingmagazine.com/2020/08/design-website-five-senses\" target=\"_blank\" rel=\"noreferrer\"\u003eCan You Design A Website For The Five Senses?\u003c/a\u003e\u0026quot;，并结合更多实例进行分析。\u003c/p\u003e","title":"网站设计新维度：为五种感官打造沉浸式体验"},{"content":"","date":null,"permalink":"/tags/cli/","section":"Tags","summary":"","title":"Cli"},{"content":"","date":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git"},{"content":"本文翻译自Useful tricks you might not know about git log。\n如果你已经使用 Git 一段时间，那么 git log 命令你一定不陌生。它是一个基础但强大的工具，用于查看项目的提交历史和变更记录。\n然而，git log 的功能远不止于此。它提供了许多选项（参数），可以帮助我们更精确、更高效地浏览和筛选项目历史。本文将介绍一些非常实用但可能不太常见的 git log 用法。\n1. 查看简洁的单行日志：--oneline #当你只想快速概览提交历史，而不需要详细信息时，--oneline 参数非常有用。\ngit log --oneline 这个命令会将每次提交压缩到一行显示，只包含短哈希值和提交消息，看起来非常清爽。\n2. 按时间段筛选提交 #你可以使用 \u0026ndash;after 和 \u0026ndash;before 参数来筛选特定时间范围内的提交。\n指定日期筛选： 使用相对日期筛选： Git 还支持更自然的相对时间表达： 3. 查看提交的具体变更：-p 或 \u0026ndash;patch #如果你想知道每次提交具体修改了哪些内容，可以使用 -p 参数。\n该命令会在显示提交信息的同时，附带每次提交的详细差异（diff），让你清楚地看到代码的增删改。\n上图展示了包含 diff 信息的日志输出。\n4. 按作者筛选提交：\u0026ndash;author #想查看某个特定成员的提交记录？ \u0026ndash;author 参数可以帮你实现。\n这个命令会筛选出指定作者的所有提交。Git 会使用正则表达式进行模式匹配，所以通常不需要完全精确匹配，并且默认是区分大小写的（但行为可能因 Git 版本和配置略有不同）。\n组合使用参数： git log 的参数可以组合使用，实现更复杂的筛选。例如：\n5. 按提交消息筛选提交：\u0026ndash;grep #如果你记得提交消息中的某个关键词（比如关联的任务 ID 或 Bug 编号），可以使用 \u0026ndash;grep 参数来搜索。\nGit 会使用正则表达式进行匹配。默认情况下，搜索是 区分大小写 的。\n忽略大小写搜索： 添加 -i 参数。 搜索多个模式（OR）： 使用 | 连接不同的模式（注意可能需要引号或转义）。 6. 按文件筛选提交：\u0026lt;路径/文件名\u0026gt; #想查看某个或某些特定文件的修改历史？直接在 git log 命令后跟上文件路径即可。\n组合使用： 同样可以与其他参数组合。\n7. 按文件内容变更筛选提交：-S #这是一个非常强大的功能！如果你想查找 引入或删除了特定代码片段 （比如某个函数名或特定字符串）的提交，可以使用 -S 参数（注意是大写的 S）。\n这个命令会搜索提交的 变更内容 （diff），而不是提交消息。默认区分大小写。\n忽略大小写并查看差异： 8. 仅显示合并提交：\u0026ndash;merges #如果你只关心分支的合并历史，比如想看看特性分支是何时合并到主干的，可以使用 \u0026ndash;merges 参数。\n这个命令会过滤掉普通的提交，只显示合并提交记录。\n9. 显示两个分支间的差异提交：.. #想知道一个分支相对于另一个分支多了哪些提交？可以使用 .. 语法。\n这个命令可以帮助你了解 develop 分支领先 master 分支多少工作量。 注意： 在比较前，最好先 git fetch 或 git pull 更新本地仓库，确保比较的是最新的状态。\n10. 自定义日志输出格式：\u0026ndash;pretty=format #觉得默认的日志格式信息太多或太少？ git log 允许你通过 \u0026ndash;pretty=format:\u0026quot;\u0026lt;格式字符串\u0026gt;\u0026quot; 来自定义输出。\ngit log --pretty=format:\u0026#34;%Cred%an - %ar%n %Cblue %h -%Cgreen %s %n\u0026#34; 上图展示了自定义格式的日志输出。 常用的格式占位符： - %h : 短哈希值 - %H : 完整哈希值 - %an : 作者名字 - %ae : 作者邮箱 - %ad : 作者提交日期 (格式可定制) - %ar : 作者提交日期 (相对时间，如 \u0026#34;2 hours ago\u0026#34;) - %cn : 提交者名字 - %cr : 提交者日期 (相对时间) - %s : 提交消息主题 (单行) - %Cred , %Cgreen , %Cblue , %Creset : 设置颜色 你可以组合这些占位符，并添加颜色，创建完全符合你需求的日志视图。更多格式选项可以查阅 Git 官方文档 。 希望这些 git log 的小技巧能帮助你更高效地使用 Git！熟练运用它们，可以大大提升你浏览和理解项目历史的效率。 ","date":"2020年7月31日","permalink":"/posts/git-log-%E5%91%BD%E4%BB%A4%E4%BD%A0%E5%8F%AF%E8%83%BD%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B0%8F%E6%8A%80%E5%B7%A7/","section":"","summary":"\u003cp\u003e本文翻译自\u003ca href=\"https://gitbetter.substack.com/p/useful-tricks-you-might-not-know\" target=\"_blank\" rel=\"noreferrer\"\u003eUseful tricks you might not know about git log\u003c/a\u003e。\u003c/p\u003e","title":"Git Log 命令：你可能不知道的实用小技巧"},{"content":"","date":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"想体验 MacOS 系统但苦于没有苹果设备？通过 Docker 容器技术，我们可以在 Linux 系统上轻松运行 MacOS！这个方案不仅适用于 Intel CPU，AMD CPU 同样可以使用，让我们一起来看看具体怎么操作。\n准备工作 #在开始之前，请确保你的系统满足以下条件：\n系统环境要求 # 支持的 Linux 发行版： Arch / Manjaro Ubuntu / Debian CentOS / RHEL / Fedora BIOS 已开启虚拟化功能（CPU Virtualization） 安装必要软件 # 安装 QEMU 和相关依赖 # Arch/Manjaro 用户运行： sudo pacman -S qemu libvirt dnsmasq virt-manager bridge-utils flex bison ebtables edk2-ovmf # Ubuntu/Debian 用户运行： sudo apt install qemu qemu-kvm libvirt-clients libvirt-daemon-system bridge-utils virt-manager # CentOS/RHEL/Fedora 用户运行： sudo yum install libvirt qemu-kvm -y 启用必要的系统服务 sudo systemctl enable libvirtd.service sudo systemctl enable virtlogd.service sudo modprobe kvm 确保已安装 Docker 安装步骤 #1. 启动 MacOS 容器 #打开终端，运行以下命令：\ndocker run \\ -e RAM=8 \\ -e CORES=4 \\ --privileged -e \u0026#34;DISPLAY=${DISPLAY:-:0.0}\u0026#34; \\ -v /tmp/.X11-unix:/tmp/.X11-unix sickcodes/docker-osx:latest 参数说明：\nRAM=8 ：分配 8GB 内存（可根据实际情况调整） CORES=4 ：分配 4 核 CPU（可根据实际情况调整） 2. 系统安装 # 在启动界面选择 \u0026ldquo;Disk Utility\u0026rdquo; 找到容量最大的磁盘，点击 \u0026ldquo;Erase\u0026rdquo;（抹除） 关闭 Disk Utility，点击 \u0026ldquo;Reinstall macOS\u0026rdquo; 开始安装系统 日常使用 #启动已安装的系统 #首次安装完成后，如果想再次使用，请按以下步骤操作：\n查找已安装的容器 ID docker ps --all 使用容器 ID 启动系统 docker start \u0026lt;容器ID\u0026gt; 例如： docker start abc123xyz567\n高级配置 #扩展磁盘空间 #默认磁盘大小为 200GB。如需更大空间，请在首次启动前执行：\ndocker build -t docker-osx:latest --build-arg SIZE=500G 此命令将创建一个 500GB 的磁盘空间（可根据需求调整）。 ","date":"2020年7月24日","permalink":"/posts/linux%E7%B3%BB%E7%BB%9F%E4%B8%80%E8%A1%8C%E5%91%BD%E4%BB%A4%E5%AE%89%E8%A3%85macos/","section":"","summary":"\u003cp\u003e想体验 MacOS 系统但苦于没有苹果设备？通过 Docker 容器技术，我们可以在 Linux 系统上轻松运行 MacOS！这个方案不仅适用于 Intel CPU，AMD CPU 同样可以使用，让我们一起来看看具体怎么操作。\u003c/p\u003e","title":"Linux系统一行命令安装MacOS"},{"content":"","date":null,"permalink":"/tags/macos/","section":"Tags","summary":"","title":"Macos"},{"content":"","date":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown"},{"content":"对于习惯使用 Markdown 写作的朋友来说，如果能将写好的文档快速转换成 PPT 演示文稿，especially 对样式要求不高的场合，那绝对是一种高效的体验。今天就向大家推荐一款 VS Code 插件神器：Marp for VS Code，它可以轻松帮你实现这个目标！\nMarp for VS Code 核心功能 # 实时预览： 在 VS Code 中直接预览幻灯片效果，所见即所得。 多格式导出： 支持将 Markdown 导出为 HTML、PDF、PPTX (PowerPoint) 文件。 图片导出： 支持将幻灯片导出为 PNG 或 JPEG 图片（注意：免费版通常仅支持导出第一页）。 主题支持： 内置多种主题，并支持用户自定义 CSS 主题。 Markdown 扩展： 支持标准的 Markdown 语法，并增加了一些用于幻灯片的特殊指令 (Directives)。 代码折叠： 编辑器中可以方便地折叠单张幻灯片的代码。 HTML 嵌入： 允许在 Markdown 中直接嵌入 HTML 代码，实现更复杂的布局或效果。 安装步骤 #安装过程非常简单：\n打开 Visual Studio Code。 点击左侧边栏的 扩展 图标 (Extensions)。 在搜索框中输入 Marp。 找到 Marp for VS Code 插件。 点击 安装 (Install) 按钮。 使用方法 #基础操作 # 启用 Marp：\n方法一 (推荐): 在你的 Markdown 文件最顶部添加 YAML Front Matter 声明 marp: true。 --- marp: true --- # 你的第一页标题 方法二: 打开命令面板 (Ctrl+Shift+P 或 Cmd+Shift+P)，输入 Marp: Toggle Marp preview 并执行，为当前文件临时启用/禁用 Marp 预览。 分隔幻灯片： 使用 Markdown 的水平分割线 --- 来分隔不同的幻灯片页面。\n--- marp: true --- # 第一页 这是第一页的内容。 --- # 第二页 这是第二页的内容。 开启实时预览：\n点击 VS Code 编辑器右上角的 预览 图标 (通常是一个带有 Markdown 标志和眼睛的图标)。 或者，打开命令面板 (Ctrl+Shift+P)，输入 Markdown: Open Preview to the Side (Markdown: 打开侧边预览) 并执行。 导出幻灯片：\n点击 VS Code 编辑器右上角的 Marp 图标 (通常是一个带有 Marp Logo 的图标)。 在弹出的菜单中选择 Export Slide Deck\u0026hellip; (导出幻灯片\u0026hellip;)。 选择你想要的导出格式，例如 PowerPoint document (.pptx)。 根据提示选择保存位置，点击 Export (导出) 按钮。搞定！✌️ 自定义主题 #如果你觉得内置主题不够用，可以创建自己的主题。\n创建 CSS 主题文件：\n在你的项目工作区 (workspace) 内创建一个 CSS 文件，例如命名为 my-theme.css。 在 CSS 文件顶部，使用 /* @theme 主题名称 */ 来声明你的主题名称，例如 /* @theme fjcanyue */。 使用 @import 'default'; 或 @import 'uncover'; 来继承一个内置主题的样式，然后在此基础上进行修改。 编写你的自定义 CSS 规则。 /* @theme fjcanyue */ /* 声明主题名称 */ @import \u0026#39;default\u0026#39;; /* 继承默认主题样式 */ /* 自定义 section (幻灯片页面) 样式 */ section { width: 1280px; /* 页面宽度 */ height: 720px; /* 页面高度 (改为常见的16:9) */ font-size: 32px; /* 默认字体大小 */ padding: 40px; /* 内边距 */ background: #f0f0f0; /* 页面背景色 */ color: #333; /* 默认文字颜色 */ } /* 自定义 h1 标题样式 */ h1 { font-size: 60px; color: #007acc; /* 蓝色标题 */ text-align: center; /* 标题居中 */ margin-bottom: 40px; } /* 自定义 h2 标题样式 */ h2 { font-size: 48px; color: #4a4a4a; } /* 示例：为带有 \u0026#39;invert\u0026#39; class 的页面设置反色效果 */ section.invert { background: #333; color: #f0f0f0; } section.invert h1 { color: #fff; } 在 VS Code 中注册自定义主题：\n打开 VS Code 的 设置 (Settings)。可以通过 文件 -\u0026gt; 首选项 -\u0026gt; 设置 (File \u0026gt; Preferences \u0026gt; Settings) 或快捷键 Ctrl+, 打开。 切换到 工作区 (Workspace) 设置选项卡。 在搜索框中输入 markdown.marp.themes。 找到 \u0026ldquo;Markdown › Marp: Themes\u0026rdquo; 设置项，点击 在 settings.json 中编辑 (Edit in settings.json)。 在 settings.json 文件中，添加你的 CSS 文件相对于工作区根目录的路径。如果 settings.json 不存在，VS Code 会在 .vscode 目录下创建它。 { \u0026#34;markdown.marp.themes\u0026#34;: [ \u0026#34;./my-theme.css\u0026#34; // 添加你的自定义主题文件路径 ] } 保存 settings.json 文件。 在 Markdown 中使用自定义主题： 在 Markdown 文件的 YAML Front Matter 中，将 theme 指令设置为你定义的 CSS 主题名称 (即 @theme 后面的名字)。\n--- marp: true theme: fjcanyue # 使用你自定义的主题名称 --- # 使用自定义主题的第一页 内容... --- \u0026lt;!-- _class: invert --\u0026gt; \u0026lt;!-- 可以为单页应用特定样式 --\u0026gt; # 使用自定义主题的第二页 (反色) 内容... 现在，你的 Marp 预览和导出的文件就会应用你自定义的主题样式了！\n","date":"2020年7月7日","permalink":"/posts/markdown-%E7%A7%92%E5%8F%98-ppt%E7%A5%9E%E5%99%A8-marp-for-vs-code-%E6%95%99%E7%A8%8B/","section":"","summary":"\u003cp\u003e对于习惯使用 Markdown 写作的朋友来说，如果能将写好的文档快速转换成 PPT 演示文稿，especially 对样式要求不高的场合，那绝对是一种高效的体验。今天就向大家推荐一款 VS Code 插件神器：\u003cstrong\u003eMarp for VS Code\u003c/strong\u003e，它可以轻松帮你实现这个目标！\u003c/p\u003e","title":"Markdown 秒变 PPT：神器 Marp for VS Code 教程"},{"content":"","date":null,"permalink":"/tags/marp/","section":"Tags","summary":"","title":"Marp"},{"content":"","date":null,"permalink":"/tags/ppt/","section":"Tags","summary":"","title":"Ppt"},{"content":"","date":null,"permalink":"/tags/presentation/","section":"Tags","summary":"","title":"Presentation"},{"content":"","date":null,"permalink":"/tags/vscode/","section":"Tags","summary":"","title":"Vscode"},{"content":"","date":null,"permalink":"/tags/database/","section":"Tags","summary":"","title":"Database"},{"content":"","date":null,"permalink":"/tags/innodb/","section":"Tags","summary":"","title":"Innodb"},{"content":"","date":null,"permalink":"/tags/mysql/","section":"Tags","summary":"","title":"Mysql"},{"content":" 本文翻译自 Percona 博客文章 MySQL 101: Parameters to Tune for MySQL Performance，原作者：@jordan。\nMySQL 性能调优是一个复杂但至关重要的任务。虽然没有一劳永逸的“银弹”配置，但通过理解和调整一些核心参数，我们可以显著提升数据库的运行效率。本文旨在梳理那些对性能影响最大、最值得关注的关键 MySQL (特别是 InnoDB 存储引擎) 参数，为你提供一个清晰的调优起点。\n请注意： 文中提到的某些参数的默认值可能因你使用的 MySQL 版本 (如 5.7, 8.0 等) 而异，但调优的基本思路和原理是通用的。\n通常，我们可以将基础的 MySQL 性能调优分为三类：\n硬件相关调优： 根据服务器的 CPU、内存、磁盘等硬件资源进行配置。 最佳实践调优： 遵循社区公认的最佳实践和通用建议进行设置。 负载相关调优： 基于数据库的实际读写负载特性进行精细化调整。 一、硬件相关调优 #这类参数的设置与运行 MySQL 的服务器（物理机或虚拟机）硬件规格密切相关。你需要根据你的实际硬件配置来调整它们。\ninnodb_buffer_pool_size (InnoDB 缓冲池大小) # 作用： 这是 最重要的性能参数之一。InnoDB 使用缓冲池来缓存表数据和索引，大幅减少对磁盘的读写次数 (Disk I/O)，是提升性能的关键。 建议值： 通常建议设置为服务器总可用物理内存的 50% 到 70%。例如，如果服务器有 64GB 内存，可以考虑设置为 32GB 到 45GB 左右。 注意事项： 设置的值不需要超过数据库所有数据和索引的总大小。如果你的数据库总共只有 10GB，那么设置 32GB 的缓冲池意义不大。 理想情况下，缓冲池应能容纳你最常访问的“热”数据。 使用监控工具 (如 Percona Monitoring and Management - PMM) 观察缓冲池命中率 (Buffer Pool Hit Rate)、脏页比例 (Dirty Pages) 等指标，可以帮助你更精确地判断当前设置是否合适。高命中率通常表示缓冲池效率高。 innodb_log_file_size (InnoDB 日志文件大小) # 作用： InnoDB 使用事务日志 (Redo Log) 来保证事务的持久性 (Durability) 和崩溃恢复能力。所有的数据更改都会先写入日志文件。 建议值： 单个日志文件的大小通常设置在 128MB 到 2GB 之间。InnoDB 通常有多个日志文件 (由 innodb_log_files_in_group 控制，默认为 2)。总日志大小 = innodb_log_file_size * innodb_log_files_in_group。 注意事项： 总日志文件大小应足够容纳大约一小时的事务数据量。这有助于减少日志切换 (Log Switch) 和检查点 (Checkpoint) 的频率。 足够大的日志文件允许 MySQL 在后台更从容地将脏页刷写到磁盘，可以将随机写操作聚合成更高效的顺序写，提升写入性能。 通过 PMM 或 SHOW ENGINE INNODB STATUS 监控日志序列号 (LSN) 的增长速度和检查点活动，可以判断日志文件大小是否合适。如果日志空间使用率频繁超过 75% 或检查点过于频繁，可能需要增大日志文件。 修改此参数需要重启 MySQL 服务。 innodb_flush_log_at_trx_commit (事务提交时日志刷新策略) # 作用： 控制每次事务提交时，InnoDB 如何将内存中的日志缓冲区 (Log Buffer) 数据写入并刷新 (fsync) 到磁盘上的日志文件。这直接影响数据安全性和写入性能。 可选值： 1 (默认值，最安全): 每次事务提交时，都将日志缓冲区写入磁盘并执行 fsync 操作，确保数据完全落盘。提供最高的 ACID 持久性保证，但性能相对最低。 0 (性能最高，风险最大): 每秒才将日志缓冲区写入并刷新到磁盘一次。如果服务器宕机，可能会丢失最后一秒内所有已提交的事务。 2 (性能与安全的折中): 每次事务提交时，将日志缓冲区写入操作系统的文件缓存，但每秒才执行一次 fsync 操作将其刷新到磁盘。如果仅 MySQL 进程崩溃，数据不会丢失；但如果操作系统或服务器宕机，仍可能丢失最后一秒的事务。 建议： 根据业务对数据安全性的要求选择。 金融、交易等对数据一致性要求极高的场景，必须使用 1。 如果能容忍极端情况下少量数据丢失，以换取更高的写入吞吐量，可以考虑 2。0 的风险通常较大，较少使用。 innodb_flush_method (数据文件刷新方式) # 作用： 控制 InnoDB 数据文件和日志文件如何通过操作系统写入存储设备。 建议值： 在 Linux/Unix 系统上，通常建议设置为 O_DIRECT。 原因： O_DIRECT 会绕过操作系统的文件系统缓存 (Page Cache)，直接将数据写入磁盘。这可以避免 MySQL InnoDB 缓冲池和操作系统缓存造成的“双重缓冲”问题，减少内存浪费和潜在的性能冲突，尤其是在使用带有硬件缓存的 RAID 卡或 SSD 时效果更佳。 注意： 确保你的操作系统和文件系统支持 O_DIRECT。在某些虚拟化环境或特定文件系统下可能存在兼容性问题或性能反而下降，需要进行测试验证。 二、最佳性能/最佳实践调优 #这些参数基于社区的最佳实践和通用经验，通常能带来性能提升或改善可管理性。\ninnodb_file_per_table (独立表空间) # 作用： 决定 InnoDB 表的数据和索引是存储在各自独立的 .ibd 文件中，还是全部存储在共享的系统表空间文件 (如 ibdata1) 中。 建议值： 设置为 ON (MySQL 5.6 及之后版本的默认值)。 优点： 空间管理更方便： 删除或 TRUNCATE 表后，对应的 .ibd 文件占用的磁盘空间可以被操作系统回收。使用共享表空间时，空间通常难以收缩。 备份恢复更灵活： 可以更容易地使用 Percona XtraBackup 等工具进行单表备份和恢复。 性能隔离： 不同表的 I/O 模式可能不同，独立文件有助于操作系统和存储更好地处理。 innodb_stats_on_metadata (元数据访问时更新统计信息) # 作用： 控制在执行 SHOW TABLE STATUS 或查询 INFORMATION_SCHEMA 中的表信息时，InnoDB 是否自动重新计算并更新表的统计信息 (用于查询优化器)。 建议值： 设置为 OFF (MySQL 5.6.6 及之后版本的默认值)。 原因： 在访问元数据时触发统计信息更新可能会非常耗时，尤其是在表很多或很大的情况下，导致 SHOW TABLE STATUS 等操作变慢甚至卡顿。关闭后，这些操作会快得多。 注意： 关闭后，统计信息不会自动更新。你需要定期或在数据发生显著变化后手动运行 ANALYZE TABLE table_name; 来更新统计信息，以保证查询优化器能做出正确的决策。 innodb_buffer_pool_instances (缓冲池实例数) # 作用： 将 InnoDB 缓冲池划分为多个独立的区域 (实例)，每个实例有自己的锁机制。这有助于在高并发访问缓冲池时减少内部锁竞争，提升并发性能。 建议值： 如果 innodb_buffer_pool_size 大于 1GB，建议设置为 8 或 16 (通常不超过 CPU 核数)。 如果 innodb_buffer_pool_size 小于 1GB，设置为 1 即可。 原理： 减少了多线程争用同一个缓冲池全局锁的情况，尤其在多核 CPU 服务器上效果明显。 query_cache_type 和 query_cache_size (查询缓存) # 作用： 查询缓存 (Query Cache) 用于缓存 SELECT 查询的文本和结果集。如果后续收到完全相同的查询语句，MySQL 可以直接返回缓存结果，跳过解析、优化和执行阶段。 建议值： 强烈建议禁用查询缓存，将 query_cache_type 设置为 0 或 OFF，并将 query_cache_size 设置为 0。 原因： 查询缓存的维护成本很高。任何对表的修改 (INSERT, UPDATE, DELETE) 都会导致该表所有相关的缓存项失效。在高并发写入场景下，缓存的命中率极低，反而因为频繁的失效和加锁操作成为严重的性能瓶颈。 查询缓存的锁粒度较大，容易引发争用。 MySQL 8.0 版本已完全移除查询缓存功能。 因此，即使在旧版本，也建议禁用它，为未来升级做好准备。 三、负载相关调优 #要进行更精细的调优，你需要了解数据库的实际运行负载情况。监控是关键！\n部署监控系统： 安装如 Percona Monitoring and Management (PMM)、Zabbix+Grafana、Prometheus+Grafana 等监控工具，收集关键性能指标。 分析核心指标： 缓冲池 (innodb_buffer_pool_size)： 关注缓冲池命中率、脏页数量、空闲页数量、读写请求数。结合服务器可用内存，判断是否需要调整大小。命中率低且内存充足，可考虑增大；内存紧张或命中率已很高，则无需增加。 日志文件 (innodb_log_file_size)： 观察 InnoDB 日志写入量 (Bytes written to log file per second)、未检查点 LSN 差距 (Uncheckpointed Bytes)、日志空间使用率。如前所述，确保持续写入一小时的数据量小于总日志大小。 I/O 性能： 监控磁盘读写 IOPS、吞吐量 (MB/s)、平均等待时间。了解存储系统的瓶颈。 并发连接与线程： 观察活跃连接数、运行线程数、线程缓存命中率 (Threads_created vs Connections)。 锁等待： 监控行锁等待次数和时间 (Innodb_row_lock_waits, Innodb_row_lock_time_avg)。 根据监控数据，你可以更有针对性地调整参数。例如，如果发现磁盘 I/O 成为瓶颈，除了优化 SQL 和索引，还可以调整 innodb_io_capacity 等参数。\n四、其他值得关注的设置 #innodb_autoinc_lock_mode (自增锁模式) # 作用： 控制插入带有自增 (AUTO_INCREMENT) 列的表时，获取自增值的锁定机制。 建议值： 设置为 2 (交错模式 - Interleaved lock mode)。 优点： 在执行多行 INSERT (如 INSERT INTO ... SELECT ... 或 LOAD DATA) 时，不再需要持有表级 AUTO-INC 锁直到语句结束，而是为每一行分配自增值时短暂锁定，显著提高并发插入性能。 前提条件： 需要将 binlog_format 设置为 ROW 或 MIXED。如果使用 STATEMENT 格式的 binlog，为了保证主从复制的一致性，即使设置为 2，在执行某些语句时仍可能退化为传统模式 (0)。MySQL 5.7.7 及之后版本默认 binlog_format=ROW，通常无需担心。 innodb_io_capacity / innodb_io_capacity_max (后台 I/O 能力) # 作用： 这两个参数告诉 InnoDB 后台任务 (如脏页刷新、合并插入缓冲等) 大约可以使用多少磁盘 I/O 能力 (以 IOPS 为单位)。这直接影响后台刷新的速度和对前台用户请求的影响。 建议值： 首先，你需要了解你的存储系统实际能提供多少 IOPS。可以使用 fio、sysbench fileio 等工具进行基准测试。 innodb_io_capacity：通常设置为存储系统稳定提供的 IOPS 值。这是 InnoDB 尝试维持的后台 I/O 水平。默认值 200 对于现代 SSD 来说通常太低。 innodb_io_capacity_max：定义了 InnoDB 在需要紧急刷新 (例如，脏页比例过高或需要快速完成检查点) 时，可以使用的最大 IOPS。通常设置为存储系统的峰值 IOPS，或者比 innodb_io_capacity 高一倍或数倍。默认值 2000 可能也需要调整。 注意： 这两个值主要影响写入密集型负载。对于读密集型负载影响较小。 设置过低可能导致脏页堆积，引发用户线程被迫参与刷新，造成性能抖动。 设置过高可能导致后台 I/O 抢占过多资源，影响前台用户查询的响应时间。 需要根据监控到的磁盘 I/O 使用率和脏页刷新情况进行调整。 总结与后续步骤 #本文介绍的参数是 MySQL (InnoDB) 性能调优中最常见和最有效的一部分。虽然不能涵盖所有细节，但遵循这些建议进行调整，通常能为你的数据库带来立竿见影的性能改善。\n核心要点：\n没有万能配置： 理解每个参数的作用，结合你的硬件、MySQL 版本和业务负载进行调整。 监控驱动调优： 部署可靠的监控系统 (如 PMM) 是进行科学调优的前提。用数据说话，而不是凭感觉。 循序渐进，逐个测试： 每次只修改少量相关参数，进行测试和观察，确认效果后再进行下一步调整。避免一次性修改大量参数导致问题难以定位。 持续学习与优化： 数据库调优是一个持续的过程。随着业务发展中和数据增长，需要定期审视和调整配置。 下一步行动建议：\n检查当前配置： 对比本文提到的参数，检查你的 MySQL 实例当前设置。 实施调整： 根据你的环境和监控数据，选择性地应用本文的建议。 强化监控： 如果还没有监控系统，尽快部署一个。 深入学习： 阅读 MySQL 官方文档、Percona 博客等资源，了解更多高级调优技巧和特定场景下的最佳实践。 ","date":"2020年7月1日","permalink":"/posts/mysql-%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E5%9F%BA%E7%A1%80%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E5%BB%BA%E8%AE%AE/","section":"","summary":"\u003cp\u003e\n\n\n\n\n\n\n\n  \u003cfigure\u003e\n    \u003cimg src=\"https://www.percona.com/blog/wp-content/uploads/2020/06/Parameters-to-Tune-for-MySQL-Performance.png\" alt=\"\" class=\"mx-auto my-0 rounded-md\" loading=\"lazy\" /\u003e\n    \n  \u003c/figure\u003e\n\u003c/p\u003e","title":"MySQL 性能调优基础：关键参数解析与优化建议"},{"content":"","date":null,"permalink":"/tags/%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/","section":"Tags","summary":"","title":"参数配置"},{"content":"","date":null,"permalink":"/tags/%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/","section":"Tags","summary":"","title":"性能调优"},{"content":"","date":null,"permalink":"/tags/chinese/","section":"Tags","summary":"","title":"Chinese"},{"content":"","date":null,"permalink":"/tags/docker-compose/","section":"Tags","summary":"","title":"Docker-Compose"},{"content":"","date":null,"permalink":"/tags/gitlab/","section":"Tags","summary":"","title":"Gitlab"},{"content":"背景说明 #许多用户之前可能使用了 twang2218/gitlab-ce-zh 这个 Docker 镜像来运行 GitLab 社区版的汉化版本。然而，该镜像的维护者已停止更新，导致用户无法升级到较新的 GitLab 版本。\n直接切换到 GitLab 官方的 Docker 镜像 (gitlab/gitlab-ce:latest) 进行升级时，可能会因为中文环境的 locale 配置问题导致升级失败或数据迁移出错。\n为了解决这个问题，我们提供了一个基于官方镜像并处理了 locale 兼容性的新镜像 fjcanyue/gitlab-ce-zh，旨在帮助大家顺利地从旧的汉化版镜像迁移和升级到更新的 GitLab 版本。\n如何使用新镜像进行升级/迁移 #核心步骤非常简单：只需将你现有的 Docker 或 Docker Compose 配置中的镜像名称替换为 fjcanyue/gitlab-ce-zh:latest (或指定版本号) 即可。\n场景一：使用 Docker Compose #如果你的 GitLab 是通过 Docker Compose 部署的，请修改你的 docker-compose.yml 文件：\n找到 services -\u0026gt; gitlab -\u0026gt; image 这一行。 将其值从 twang2218/gitlab-ce-zh:xxx 或其他旧镜像，更改为 fjcanyue/gitlab-ce-zh:latest。 修改后的 docker-compose.yml 文件示例：\nversion: \u0026#39;2\u0026#39; # 或者 \u0026#39;3\u0026#39; 等你的版本 services: gitlab: # --- 修改这里 --- # image: \u0026#39;twang2218/gitlab-ce-zh:some-old-version\u0026#39; # 旧镜像 image: \u0026#39;fjcanyue/gitlab-ce-zh:latest\u0026#39; # 替换为新镜像 # --- 修改结束 --- restart: unless-stopped hostname: \u0026#39;gitlab.example.com\u0026#39; # 替换为你的 GitLab 访问域名 environment: TZ: \u0026#39;Asia/Shanghai\u0026#39; GITLAB_OMNIBUS_CONFIG: | # --- 保留你原有的配置 --- external_url \u0026#39;http://gitlab.example.com\u0026#39; # 替换为你的 GitLab 访问 URL gitlab_rails[\u0026#39;time_zone\u0026#39;] = \u0026#39;Asia/Shanghai\u0026#39; # 需要配置到 gitlab.rb 中的配置可以在这里配置，每个配置一行，注意缩进。 # 比如下面的电子邮件的配置： # gitlab_rails[\u0026#39;smtp_enable\u0026#39;] = true # gitlab_rails[\u0026#39;smtp_address\u0026#39;] = \u0026#34;smtp.exmail.qq.com\u0026#34; # gitlab_rails[\u0026#39;smtp_port\u0026#39;] = 465 # gitlab_rails[\u0026#39;smtp_user_name\u0026#39;] = \u0026#34;xxxx@xx.com\u0026#34; # gitlab_rails[\u0026#39;smtp_password\u0026#39;] = \u0026#34;password\u0026#34; # gitlab_rails[\u0026#39;smtp_authentication\u0026#39;] = \u0026#34;login\u0026#34; # gitlab_rails[\u0026#39;smtp_enable_starttls_auto\u0026#39;] = true # gitlab_rails[\u0026#39;smtp_tls\u0026#39;] = true # gitlab_rails[\u0026#39;gitlab_email_from\u0026#39;] = \u0026#39;xxxx@xx.com\u0026#39; # --- 保留你原有的配置结束 --- ports: # --- 保留你原有的端口映射 --- - \u0026#39;80:80\u0026#39; - \u0026#39;443:443\u0026#39; - \u0026#39;22:22\u0026#39; # 如果需要 SSH 访问，请保留 volumes: # --- 确保 Volumes 映射保持不变，指向你现有的数据目录 --- - config:/etc/gitlab - data:/var/opt/gitlab - logs:/var/log/gitlab # --- 确保 Volumes 定义保持不变 --- volumes: config: # external: true # 如果你使用的是外部 volume，请保留相应配置 data: # external: true logs: # external: true ","date":"2020年5月13日","permalink":"/posts/gitlab-ce-%E6%B1%89%E5%8C%96%E7%89%88-docker-%E9%95%9C%E5%83%8F%E5%8D%87%E7%BA%A7%E4%B8%8E%E8%BF%81%E7%A7%BB%E6%8C%87%E5%8D%97/","section":"","summary":"\u003ch2 id=\"背景说明\" class=\"relative group\"\u003e背景说明 \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#%e8%83%8c%e6%99%af%e8%af%b4%e6%98%8e\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003e许多用户之前可能使用了 \u003ccode\u003etwang2218/gitlab-ce-zh\u003c/code\u003e 这个 Docker 镜像来运行 GitLab 社区版的汉化版本。然而，该镜像的维护者已停止更新，导致用户无法升级到较新的 GitLab 版本。\u003c/p\u003e","title":"GitLab CE 汉化版 Docker 镜像升级与迁移指南 (从 twang2218/gitlab-ce-zh 迁移)"},{"content":"","date":null,"permalink":"/tags/migration/","section":"Tags","summary":"","title":"Migration"},{"content":"","date":null,"permalink":"/tags/upgrade/","section":"Tags","summary":"","title":"Upgrade"},{"content":"","date":null,"permalink":"/tags/atlassian/","section":"Tags","summary":"","title":"Atlassian"},{"content":"","date":null,"permalink":"/tags/issue/","section":"Tags","summary":"","title":"Issue"},{"content":"","date":null,"permalink":"/tags/jira/","section":"Tags","summary":"","title":"Jira"},{"content":"JIRA 作为业界领先的项目与事务跟踪工具，其背后有着相对复杂的数据库结构来支撑其强大的功能。了解其核心表结构对于进行数据分析、报表定制、系统集成或高级故障排查非常有帮助。\n本文参考了 Atlassian 官方提供的 JIRA 数据库结构文档，并结合常见的核心概念（项目、事务、用户），对主要的表及其关系进行梳理和解读。\n重要提示：\n直接查询 JIRA 数据库应谨慎进行，尤其避免直接进行写操作，这可能会绕过 JIRA 的业务逻辑，导致数据不一致甚至损坏。 对于大多数数据交互需求，强烈建议优先使用 JIRA 官方提供的 REST API。 数据库结构可能随 JIRA 版本更新而略有变化。 以下是 JIRA 中关于项目、事务和用户这三个核心概念的主要数据库表关系图解（图片来源于网络或官方文档截图，仅为示意）：\n一、项目 (Project) 相关表 #项目是 JIRA 中组织工作的基本单位。与项目相关的核心信息存储在以下关键表中：\nJIRA Project Schema Diagram project: 存储项目的核心信息，如项目 ID (ID)、项目 Key (pkey)、项目名称 (pname)、负责人 (lead) 等。这是项目相关查询的入口点。 projectcategory: 项目分类表，用于对项目进行分组管理。通过 project.pcategory 关联。 component: 项目组件表，用于将项目下的事务按功能模块细分。通过 component.project 关联到 project 表。 projectversion: 项目版本表，用于管理项目的发布版本。通过 projectversion.project 关联到 project 表。 权限相关表（图中可能未完全展示）：如 projectrole (项目角色定义)、projectroleactor (项目角色与用户/组的关联)、permissionscheme (权限方案)、nodeassociation (项目与权限/通知等方案的关联) 等，共同构成了 JIRA 灵活的权限体系。 二、事务 (Issue) 相关表 #事务 (Issue) 是 JIRA 的核心，代表了工作项（如 Bug、任务、故事等）。事务相关的数据分散在多个表中：\nJIRA Issue Schema Diagram jiraissue: 最核心的表，存储每个事务的基本信息，如事务 ID (ID)、事务 Key (issuenum 结合 project.pkey 构成如 PROJ-123 的 Key)、所属项目 (project)、报告人 (reporter)、经办人 (assignee)、事务类型 (issuetype)、状态 (issuestatus)、优先级 (priority)、解决结果 (resolution)、创建/更新时间等。 issuetype: 定义事务类型（如 Bug, Task, Story）。通过 jiraissue.issuetype 关联。 issuestatus: 定义事务状态（如 Open, In Progress, Resolved, Closed）。通过 jiraissue.issuestatus 关联。 priority: 定义事务优先级（如 Highest, High, Medium, Low）。通过 jiraissue.priority 关联。 resolution: 定义事务解决结果（如 Fixed, Won\u0026rsquo;t Fix, Duplicate）。通过 jiraissue.resolution 关联。 customfield: 定义自定义字段。 customfieldvalue: 存储自定义字段的值。通过 customfieldvalue.issue 关联到 jiraissue 表，通过 customfieldvalue.customfield 关联到 customfield 表。这是实现 JIRA 高度可定制性的关键。 worklog: 存储工作日志（时间跟踪）信息。通过 worklog.issueid 关联到 jiraissue 表。 jiraaction: 存储事务的操作历史记录，如评论 (actiontype = 'comment')、状态变更等。通过 jiraaction.issueid 关联到 jiraissue 表。 关联关系表：如 issuelink (事务之间的链接关系)、nodeassociation (事务与组件/版本的关联) 等。 三、用户 (User) 相关表 #JIRA 的用户和组管理通常依赖于底层的 Atlassian Crowd 库（即使是内置用户目录）。\nJIRA User Schema Diagram cwd_user: 存储用户的核心信息，如用户 ID (id)、用户名 (user_name)、显示名称 (display_name)、邮箱 (email_address)、活动状态 (active) 等。 cwd_group: 存储用户组信息。 cwd_membership: 存储用户和用户组的成员关系。关联 cwd_user 和 cwd_group。 app_user: JIRA 内部用于关联 cwd_user 和 JIRA 应用层用户概念的表，包含 user_key (在 JIRA 中更常用的用户标识) 和 lower_user_name。查询时经常需要将 jiraissue 表中的 assignee 或 reporter (存储 user_key) 与 app_user 关联，再关联到 cwd_user 获取详细信息。 cwd_directory: 存储用户目录信息（如内部目录、LDAP 目录等）。 总结 #JIRA 的数据库结构设计精巧但也相对复杂，反映了其功能的全面性和灵活性。理解上述核心表的用途和它们之间的关联关系，是进行 JIRA 数据相关工作的基础。\n再次强调，除非绝对必要且完全理解其影响，否则应避免直接修改数据库。优先使用 JIRA 提供的 API 进行数据交互是更安全、更推荐的做法。希望本文的梳理能为你理解 JIRA 的数据存储提供一个清晰的概览。\n","date":"2017年9月21日","permalink":"/posts/jira-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%A0%B8%E5%BF%83%E8%A1%A8%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-%E9%A1%B9%E7%9B%AE%E4%BA%8B%E5%8A%A1%E7%94%A8%E6%88%B7/","section":"","summary":"\u003cp\u003eJIRA 作为业界领先的项目与事务跟踪工具，其背后有着相对复杂的数据库结构来支撑其强大的功能。了解其核心表结构对于进行数据分析、报表定制、系统集成或高级故障排查非常有帮助。\u003c/p\u003e","title":"JIRA 数据库核心表结构解析 (项目、事务、用户)"},{"content":"","date":null,"permalink":"/tags/project/","section":"Tags","summary":"","title":"Project"},{"content":"","date":null,"permalink":"/tags/schema/","section":"Tags","summary":"","title":"Schema"},{"content":"","date":null,"permalink":"/tags/user/","section":"Tags","summary":"","title":"User"},{"content":"","date":null,"permalink":"/tags/apache-httpclient/","section":"Tags","summary":"","title":"Apache-Httpclient"},{"content":"","date":null,"permalink":"/tags/apm/","section":"Tags","summary":"","title":"Apm"},{"content":"","date":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":"","date":null,"permalink":"/tags/mns/","section":"Tags","summary":"","title":"Mns"},{"content":"","date":null,"permalink":"/tags/pinpoint/","section":"Tags","summary":"","title":"Pinpoint"},{"content":"","date":null,"permalink":"/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting"},{"content":"问题背景 #在使用分布式应用性能管理 (APM) 工具 Pinpoint 对 Java 应用进行监控时，我们发现在调用某些基于 Apache HTTP Client (特别是其异步 NIO 客户端) 的服务时，偶尔会出现诡异的 SocketTimeoutException 错误。具体场景是我们的应用通过 Apache HTTP Client 调用阿里云消息服务 (MNS)。\n错误现象 #启用 Pinpoint Agent 后，相关调用链在 Pinpoint UI 上可能显示正常或超时，同时应用程序日志中出现类似以下的堆栈信息：\nCaused by: java.net.SocketTimeoutException at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:351) at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92) at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39) at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:177) at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:265) at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:494) at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:215) at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:282) at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:106) at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:590) ... more 这个超时异常看起来像是网络问题或服务端处理慢，但奇怪的是，禁用 Pinpoint Agent 后问题就消失了，这表明问题很可能与 Pinpoint 的监控插桩有关。\n问题分析：HttpEntity 的可重复性 (Repeatability) #经过排查，我们发现问题根源在于 Pinpoint 对 Apache HTTP Client 请求体 (HttpEntity) 的处理方式以及 HttpEntity 自身的可重复性特性。\nPinpoint 为了能在调用链追踪中记录 HTTP 请求的详细信息（例如 POST 请求的参数），会尝试读取请求体的内容。然而，Apache HTTP Client 中的 HttpEntity 分为几种类型，其中一种是 流式实体 (Streamed Entity) 。\n根据 Apache HttpCore 的官方文档说明：\n流式 (streamed): 内容从流中接收，或动态生成。特别是，从连接接收的实体属于此类。 流式实体通常是不可重复读取的 (not repeatable) 。 自包含 (self-contained): 内容在内存中，或通过独立于连接或其他实体的方式获取。 自包含实体通常是可重复读取的 (repeatable) 。 包装 (wrapping): 内容从另一个实体获取。 当 HttpEntity 是一个 不可重复读取 的流式实体时（例如，直接来自输入流或者某些动态生成的内容），它内部的数据流只能被消费一次。如果 Pinpoint 为了记录请求参数而尝试读取这个流，那么当 Apache HTTP Client 真正要发送请求时，流可能已经到达末尾或处于无效状态，导致无法正确发送请求内容，最终可能表现为连接异常或超时。\n在我们的场景中，调用阿里云 MNS SDK 时，其内部使用的 Apache HTTP Client 可能创建了不可重复读取的 HttpEntity 。Pinpoint 的默认配置会尝试读取这种实体，从而干扰了正常的请求发送流程。\n解决方案 #要解决这个问题，我们需要调整 Pinpoint Agent 的配置文件 pinpoint.config (或 pinpoint-env.config )， 禁止 Pinpoint 尝试读取可能不可重复的请求体 。\n找到以下配置项：\n将其值修改为 false ：\n修改说明：\nprofiler.apache.httpclient4.entity 这个配置项控制 Pinpoint 是否尝试记录 Apache HTTP Client 4.x 发出的 POST 或 PUT 请求的实体内容。 虽然注释中提到 \u0026ldquo;Limited to entities where HttpEntity.isRepeatable() == true\u0026rdquo;，但在某些复杂或特定实现下，Pinpoint 的判断或读取操作仍可能干扰不可重复的流。 将其设置为 false 后，Pinpoint 将不再尝试读取请求体内容，从而避免了与不可重复实体的冲突。缺点是在 Pinpoint UI 上可能无法看到具体的 POST/PUT 请求参数，但可以保证业务调用的正常进行。 修改配置后， 重启 你的 Java 应用使配置生效。之后，调用 MNS 或其他类似服务的 SocketTimeoutException 问题应该就能得到解决。 总结 #Pinpoint 是一个强大的 APM 工具，但在监控某些特定库（如 Apache HTTP Client 的异步调用）时，其默认配置可能与库的内部机制（如 HttpEntity 的可重复性）产生冲突。遇到类似问题时，除了检查网络和服务端，也应考虑 APM 工具本身的配置和插桩行为。通过调整 profiler.apache.httpclient4.entity 配置，可以有效解决因读取不可重复请求体导致的超时或其他异常。\n","date":"2017年5月10日","permalink":"/posts/%E8%A7%A3%E5%86%B3-pinpoint-%E7%9B%91%E6%8E%A7-apache-http-client-%E5%BC%82%E6%AD%A5%E8%AF%B7%E6%B1%82%E6%97%B6%E7%9A%84-sockettimeoutexception-%E9%97%AE%E9%A2%98/","section":"","summary":"\u003ch2 id=\"问题背景\" class=\"relative group\"\u003e问题背景 \u003cspan class=\"absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\"\u003e\u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\" style=\"text-decoration-line: none !important;\" href=\"#%e9%97%ae%e9%a2%98%e8%83%8c%e6%99%af\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\u003c/span\u003e\u003c/h2\u003e\u003cp\u003e在使用分布式应用性能管理 (APM) 工具 \u003ca href=\"https://github.com/pinpoint-apm/pinpoint\" target=\"_blank\" rel=\"noreferrer\"\u003ePinpoint\u003c/a\u003e 对 Java 应用进行监控时，我们发现在调用某些基于 Apache HTTP Client (特别是其异步 NIO 客户端) 的服务时，偶尔会出现诡异的 \u003ccode\u003eSocketTimeoutException\u003c/code\u003e 错误。具体场景是我们的应用通过 Apache HTTP Client 调用阿里云消息服务 (MNS)。\u003c/p\u003e","title":"解决 Pinpoint 监控 Apache HTTP Client 异步请求时的 SocketTimeoutException 问题"},{"content":"","date":null,"permalink":"/tags/cluster/","section":"Tags","summary":"","title":"Cluster"},{"content":"","date":null,"permalink":"/tags/distributed-systems/","section":"Tags","summary":"","title":"Distributed-Systems"},{"content":"","date":null,"permalink":"/tags/zookeeper/","section":"Tags","summary":"","title":"Zookeeper"},{"content":"本文档介绍了如何使用 fjcanyue/docker-zookeeper 这个 Docker 镜像来部署 Apache Zookeeper。该镜像支持以下三种常见的部署模式，方便你在不同场景下快速搭建 Zookeeper 服务：\n单机模式 (Standalone Mode): 在单台机器上运行一个独立的 Zookeeper 实例，主要用于开发或测试环境。 伪集群模式 (Pseudo-Distributed Mode): 在单台机器上运行多个 Zookeeper 实例，模拟一个真实的集群环境，同样适用于开发或测试。 完全分布式模式 (Fully-Distributed Mode): 将多个 Zookeeper 实例分别部署在不同的物理或虚拟机上，组成一个生产环境级别的高可用集群。 以下将分别介绍如何使用 Docker Compose 配置和启动这三种模式。\n模式一：单机模式 (Standalone Mode) #此模式下，我们只在一台机器上部署一个 Zookeeper 实例。\ndocker-compose.yml 配置:\nversion: \u0026#39;2\u0026#39; # 或更高版本，如 \u0026#39;3.8\u0026#39; services: zookeeper: # 服务名称可以自定义 container_name: zookeeper_standalone # 容器名称 hostname: zk_standalone # 容器主机名 image: fjcanyue/zookeeper # 使用指定的 Zookeeper 镜像 restart: always # 容器退出时总是自动重启 ports: - \u0026#34;2181:2181\u0026#34; # 将主机的 2181 端口映射到容器的 2181 端口 (Zookeeper 客户端端口) # environment: # 单机模式下通常不需要特殊环境变量，镜像会默认配置 # ZOOKEEPER_PORT: 2181 # 默认客户端端口 volumes: # 可选：将数据和日志持久化到宿主机，防止容器删除后数据丢失 - ./zk-standalone/data:/data - ./zk-standalone/datalog:/datalog 启动命令: 在包含 docker-compose-standalone.yml 文件的目录下执行：\n模式二：伪集群模式 (Pseudo-Distributed Mode) #此模式下，我们在 同一台机器 上部署三个 Zookeeper 实例，它们通过内部网络相互通信，组成一个集群。\ndocker-compose.yml 配置:\nversion: \u0026#39;2\u0026#39; # 或更高版本 services: zookeeper1: container_name: zookeeper1 hostname: zk1 # 容器主机名，用于节点间通信 image: fjcanyue/zookeeper restart: always ports: - \u0026#34;2181:2181\u0026#34; # 第一个实例的客户端端口映射到主机 2181 environment: ZOOKEEPER_PORT: 2181 # 容器内客户端端口 ZOOKEEPER_MYID: 1 # 当前实例的唯一 ID (1-255) # 定义集群所有节点的地址信息 (server.ID=hostname:peerPort:leaderPort) ZOOKEEPER_NODES: server.1=zk1:2888:3888,server.2=zk2:2888:3888,server.3=zk3:2888:3888 volumes: - ./zk-pseudo/zk1/data:/data - ./zk-pseudo/zk1/datalog:/datalog networks: zk_network: # 将所有实例连接到同一个自定义网络 zookeeper2: container_name: zookeeper2 hostname: zk2 image: fjcanyue/zookeeper restart: always ports: - \u0026#34;2182:2181\u0026#34; # 第二个实例的客户端端口映射到主机 2182 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_MYID: 2 # 当前实例的唯一 ID ZOOKEEPER_NODES: server.1=zk1:2888:3888,server.2=zk2:2888:3888,server.3=zk3:2888:3888 volumes: - ./zk-pseudo/zk2/data:/data - ./zk-pseudo/zk2/datalog:/datalog networks: zk_network: zookeeper3: container_name: zookeeper3 hostname: zk3 image: fjcanyue/zookeeper restart: always ports: - \u0026#34;2183:2181\u0026#34; # 第三个实例的客户端端口映射到主机 2183 environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_MYID: 3 # 当前实例的唯一 ID ZOOKEEPER_NODES: server.1=zk1:2888:3888,server.2=zk2:2888:3888,server.3=zk3:2888:3888 volumes: - ./zk-pseudo/zk3/data:/data - ./zk-pseudo/zk3/datalog:/datalog networks: zk_network: # 定义自定义网络，允许容器通过 hostname (zk1, zk2, zk3) 相互访问 networks: zk_network: driver: bridge 启动命令: 在包含 docker-compose-pseudo.yml 文件的目录下执行：\n模式三：完全分布式模式 (Fully-Distributed Mode) #此模式下，我们将三个 Zookeeper 实例分别部署在 三台不同的机器 上。假设这三台机器的主机名（或 IP 地址）分别是 machine1 , machine2 , machine3 ，并且它们之间网络互通（特别是 2888 和 3888 端口）。\n你需要在 每台机器上 分别放置一个 docker-compose.yml 文件，并进行相应的配置。\n机器 1 ( machine1 ) 上的 docker-compose.yml :\nversion: \u0026#39;2\u0026#39; services: zookeeper1: container_name: zookeeper1 # 容器名称 # hostname: zk1 # 可选，如果网络配置允许通过主机名访问 image: fjcanyue/zookeeper restart: always network_mode: \u0026#34;host\u0026#34; # 使用主机网络模式，容器直接使用主机的网络栈和端口 # ports: # host 模式下不需要再定义端口映射 # - \u0026#34;2181:2181\u0026#34; # - \u0026#34;2888:2888\u0026#34; # - \u0026#34;3888:3888\u0026#34; environment: ZOOKEEPER_PORT: 2181 # 客户端端口 ZOOKEEPER_MYID: 1 # 当前机器上实例的 ID # 定义集群所有节点的地址信息，使用实际的主机名或 IP 地址 ZOOKEEPER_NODES: server.1=machine1:2888:3888,server.2=machine2:2888:3888,server.3=machine3:2888:3888 volumes: # 将数据持久化到宿主机 - /path/on/machine1/zk/data:/data - /path/on/machine1/zk/datalog:/datalog 机器 2 ( machine2 ) 上的 docker-compose.yml :\nversion: \u0026#39;2\u0026#39; services: zookeeper2: # 服务名和容器名建议与 ID 对应 container_name: zookeeper2 image: fjcanyue/zookeeper restart: always network_mode: \u0026#34;host\u0026#34; environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_MYID: 2 # 修改为当前机器实例的 ID ZOOKEEPER_NODES: server.1=machine1:2888:3888,server.2=machine2:2888:3888,server.3=machine3:2888:3888 # 集群节点信息保持一致 volumes: - /path/on/machine2/zk/data:/data # 注意修改为本机路径 - /path/on/machine2/zk/datalog:/datalog # 注意修改为本机路径 机器 3 ( machine3 ) 上的 docker-compose.yml :\nversion: \u0026#39;2\u0026#39; services: zookeeper3: container_name: zookeeper3 image: fjcanyue/zookeeper restart: always network_mode: \u0026#34;host\u0026#34; environment: ZOOKEEPER_PORT: 2181 ZOOKEEPER_MYID: 3 # 修改为当前机器实例的 ID ZOOKEEPER_NODES: server.1=machine1:2888:3888,server.2=machine2:2888:3888,server.3=machine3:2888:3888 # 集群节点信息保持一致 volumes: - /path/on/machine3/zk/data:/data # 注意修改为本机路径 - /path/on/machine3/zk/datalog:/datalog # 注意修改为本机路径 启动命令: 在 每台 机器上，进入包含对应 docker-compose.yml 文件的目录，执行：\ndocker-compose up -d 注意事项 (完全分布式模式):\n网络模式: 推荐使用 network_mode: \u0026quot;host\u0026quot; ，这样容器直接使用宿主机的 IP 和端口，简化了节点间通信的配置。如果使用默认的 bridge 网络，你需要确保容器端口（2181, 2888, 3888）正确映射到主机，并且 ZOOKEEPER_NODES 中使用的是 主机 IP 或可解析的主机名 。 主机名/IP: ZOOKEEPER_NODES 环境变量中的 machine1 , machine2 , machine3 必须替换为 实际的、相互可访问 的主机名或 IP 地址。 防火墙: 确保每台机器的防火墙允许其他 Zookeeper 节点访问所需的端口 (默认为 2181, 2888, 3888)。 数据卷: 确保每台机器上的 volumes 路径指向本机实际存在的目录，用于持久化数据。 选择适合你需求的模式，并根据示例配置进行调整即可快速部署 Zookeeper。 ","date":"2017年3月24日","permalink":"/posts/%E4%BD%BF%E7%94%A8-docker-%E9%83%A8%E7%BD%B2-zookeeper-%E9%9B%86%E7%BE%A4%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3/","section":"","summary":"\u003cp\u003e本文档介绍了如何使用 \u003ca href=\"https://github.com/fjcanyue/docker-zookeeper\" target=\"_blank\" rel=\"noreferrer\"\u003efjcanyue/docker-zookeeper\u003c/a\u003e 这个 Docker 镜像来部署 Apache Zookeeper。该镜像支持以下三种常见的部署模式，方便你在不同场景下快速搭建 Zookeeper 服务：\u003c/p\u003e","title":"使用 Docker 部署 Zookeeper 集群：三种模式详解 (fjcanyue/zookeeper 镜像)"},{"content":"","date":null,"permalink":"/tags/vagrant/","section":"Tags","summary":"","title":"Vagrant"},{"content":"Vagrant 是一个强大的虚拟机管理工具，但在使用过程中，尤其是在 Windows 环境下，可能会遇到一些常见问题。本文汇总了一些典型问题及其解决方案，希望能帮助你快速排查和解决。\n问题一：同步文件夹失败 - 找不到 \u0026ldquo;rsync\u0026rdquo; (rsync not found on PATH) #问题描述： 在执行 vagrant up 或 vagrant rsync-auto 时，出现类似 \u0026quot;rsync\u0026quot; could not be found on your PATH 的错误。\n原因分析： Vagrant 默认倾向于使用 rsync 工具来实现主机与虚拟机之间的文件同步，因为它通常比 VirtualBox 自带的共享文件夹 (vboxsf) 性能更好，尤其是在处理大量小文件时。然而，Windows 系统默认不包含 rsync 命令。\n解决方案：\n你可以选择以下任一方法解决：\n安装 Windows 版 rsync (推荐)：\n下载 cwRsync (Free Edition) 或通过 Git for Windows (其 Git Bash 包含了 rsync) 等方式获取 Windows 下可用的 rsync.exe。 将 rsync.exe 所在的目录添加到系统的 PATH 环境变量中，确保 Vagrant 能够找到它。重启命令行或 VS Code 终端使环境变量生效。 使用 Cygwin 或 WSL：\n安装 Cygwin 或启用 Windows Subsystem for Linux (WSL)。 在 Cygwin 终端或 WSL 终端中安装 rsync (apt install rsync 或 yum install rsync，取决于你的 WSL 发行版)。 在 Cygwin 或 WSL 环境中运行 Vagrant 命令 (vagrant up, vagrant reload 等)。Vagrant 会检测到该环境下的 rsync。 强制使用 VirtualBox 共享文件夹 (性能可能较低)：\n如果不想安装 rsync，可以修改 Vagrant Box 的内部 Vagrantfile (不推荐，因为 Box 更新会覆盖) 或者在你的项目 Vagrantfile 中明确指定同步类型为 virtualbox。 编辑你项目根目录下的 Vagrantfile，找到或添加 config.vm.synced_folder 配置，并显式指定 type: \u0026quot;virtualbox\u0026quot;： Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # ... 其他配置 ... # 将默认的 rsync 同步方式改为 VirtualBox 共享文件夹 config.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, type: \u0026#34;virtualbox\u0026#34; # ... 其他配置 ... end 修改后运行 vagrant reload 使配置生效。 问题二：无法挂载共享文件夹 - 缺少 \u0026ldquo;vboxsf\u0026rdquo; 文件系统 (VirtualBox Guest Additions 问题) #问题描述： 启动虚拟机时看到类似 Vagrant was unable to mount VirtualBox shared folders. This is usually because the filesystem \u0026quot;vboxsf\u0026quot; is not available. 的错误信息。\n原因分析： 这通常意味着虚拟机内部没有正确安装或加载 VirtualBox Guest Additions (增强功能)。vboxsf 是 Guest Additions 提供的用于挂载 VirtualBox 共享文件夹的文件系统驱动。缺少它，Vagrant 就无法将主机目录挂载到虚拟机中（即使你强制使用 type: \u0026quot;virtualbox\u0026quot;）。\n解决方案： 安装 vagrant-vbguest 插件。这个插件会在虚拟机启动时自动检查 Guest Additions 的版本，并在需要时尝试安装或更新它。\n在你的主机命令行中执行：\nvagrant plugin install vagrant-vbguest 安装完成后，重新加载或启动虚拟机：\nvagrant reload 或\nvagrant up 插件会自动处理 Guest Additions 的安装/更新过程。\n配置调整：修改虚拟机内存 #需求： 需要调整 Vagrant 创建的 VirtualBox 虚拟机的内存大小。\n解决方案： 在项目的 Vagrantfile 文件中，添加或修改 config.vm.provider 配置块，指定 VirtualBox 提供者的内存设置。\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| # ... 其他配置 ... config.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| # 将内存设置为 1024MB (单位是 MB) vb.memory = \u0026#34;1024\u0026#34; # 也可以设置 CPU 核心数等其他 VirtualBox 特定选项 # vb.cpus = 2 end # ... 其他配置 ... end 修改后，执行 vagrant reload 来应用新的内存配置。\n配置调整：一个项目管理多个虚拟机 (Multi-Machine) #需求： 在一个 Vagrantfile 中定义和管理多个相互关联的虚拟机（例如，一个 Web 服务器和一个数据库服务器）。\n解决方案： 使用 config.vm.define 块来分别为每个虚拟机定义配置。\nVagrant.configure(\u0026#34;2\u0026#34;) do |config| # 通用配置，适用于所有定义的虚拟机 (除非被覆盖) config.vm.box = \u0026#34;centos/7\u0026#34; # 定义第一个虚拟机 \u0026#34;web01\u0026#34; config.vm.define \u0026#34;web01\u0026#34; do |web01_config| # web01 的特定配置 web01_config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.11\u0026#34; web01_config.vm.hostname = \u0026#34;web01\u0026#34; # web01_config.vm.provision ... end # 定义第二个虚拟机 \u0026#34;web02\u0026#34; config.vm.define \u0026#34;web02\u0026#34; do |web02_config| # web02 的特定配置 web02_config.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.12\u0026#34; web02_config.vm.hostname = \u0026#34;web02\u0026#34; # web02_config.vm.provision ... end # 也可以定义数据库服务器等 # config.vm.define \u0026#34;db01\u0026#34; do |db01_config| # ... # end end 使用这种方式定义后，你可以通过指定虚拟机名称来单独管理它们：\n# 启动所有虚拟机 vagrant up # 只启动 web01 vagrant up web01 # 重启 web02 vagrant reload web02 # SSH 连接到 web01 vagrant ssh web01 希望这些整理能帮助你更顺畅地使用 Vagrant！\n","date":"2017年3月23日","permalink":"/posts/vagrant-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E6%B1%87%E6%80%BB/","section":"","summary":"\u003cp\u003eVagrant 是一个强大的虚拟机管理工具，但在使用过程中，尤其是在 Windows 环境下，可能会遇到一些常见问题。本文汇总了一些典型问题及其解决方案，希望能帮助你快速排查和解决。\u003c/p\u003e","title":"Vagrant 常见问题排查与解决方案汇总"},{"content":"","date":null,"permalink":"/tags/virtualbox/","section":"Tags","summary":"","title":"Virtualbox"},{"content":"","date":null,"permalink":"/tags/vm/","section":"Tags","summary":"","title":"Vm"},{"content":"","date":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows"},{"content":"","date":null,"permalink":"/tags/benchmark/","section":"Tags","summary":"","title":"Benchmark"},{"content":"","date":null,"permalink":"/tags/freemarker/","section":"Tags","summary":"","title":"Freemarker"},{"content":"在 Java Web 开发或代码生成场景中，模板引擎扮演着重要的角色，它负责将数据与预定义的模板结合，生成最终的 HTML 页面或文本文件。不同的模板引擎在语法、功能特性以及性能方面可能存在显著差异。性能是选择模板引擎时的一个重要考量因素，尤其是在高并发或需要大量渲染的场景下。\n最近在 GitHub 上关注到一个名为 template-benchmark 的项目，它对多种流行的 Java 模板引擎进行了性能基准测试。该项目提供了一个直观的方式来比较不同引擎在相同任务下的渲染速度。\n我将该项目 Fork 了一份，并将其中涉及的模板引擎库更新到了当时较新的版本，然后重新运行了基准测试。本文将基于更新后的测试结果，对这些 Java 模板引擎的性能进行对比和简要分析。\n基准测试环境与方法 (简述) #该基准测试的核心任务通常是渲染一个包含一定复杂度（如循环、条件判断、变量替换）的模板，模拟常见的 Web 页面生成场景。测试结果以 每秒操作数 (operations per second) 来衡量，数值越高表示性能越好。\n(注：具体的测试环境、模板内容和数据结构请参考原始 GitHub 仓库。)\n性能测试结果 (更新版本后) #以下是更新模板引擎版本后，运行基准测试得到的部分结果（按性能从高到低排序，单位：ops/sec）：\nRocker: 29893.517 (表现最佳) Pebble: 24148.035 Trimou: 19225.674 Velocity: 16919.416 Mustache.java: 16044.050 Handlebars.java: 15043.915 Freemarker: 13009.378 Thymeleaf: 4272.166 (相对较低) Benchmark Results Chart 图：各模板引擎性能对比柱状图 (越高越好)\n结果分析与讨论 #从测试结果来看：\nRocker 表现突出： Rocker 在这个基准测试中性能遥遥领先。Rocker 的一个特点是它会将模板预编译成 Java 类文件，运行时直接执行 Java 代码，从而避免了解析和解释模板的开销，这通常能带来显著的性能优势。 Pebble 和 Trimou 性能优异： 这两个引擎也展现了非常高的性能，远超许多传统引擎。 传统引擎表现稳健： Velocity, Mustache.java, Handlebars.java, Freemarker 这些广泛使用的老牌模板引擎性能处于中上水平，彼此差距不大，对于大多数应用场景来说性能足够。 Thymeleaf 性能相对较低： 值得特别关注的是 Thymeleaf。即使在更新到 Thymeleaf 3 版本后（相比 Thymeleaf 2 有所改善），其性能在这个基准测试中仍然显著低于其他引擎。究其原因，很可能是因为 Thymeleaf 的核心机制与其他引擎不同。Thymeleaf 通常需要将模板解析成一个完整的 DOM (文档对象模型) 树，然后在内存中处理这个树来插入数据和执行逻辑。这种基于 DOM 的处理方式虽然带来了可以直接在浏览器中预览模板（自然模板）等优点，但也引入了较大的解析和内存开销，导致其在纯粹的渲染速度上不如那些基于文本流或预编译的引擎。 如何选择？性能并非唯一标准 #虽然性能是一个重要的指标，但在选择模板引擎时，还需要考虑其他因素：\n功能特性： 是否支持布局继承、宏、自定义函数/标签等高级功能？ 语法与易用性： 模板语法是否简洁易懂？学习曲线如何？ 社区与生态： 是否有活跃的社区支持？与常用框架（如 Spring Boot）的集成是否方便？ 错误处理： 模板出错时的提示信息是否友好？ 特定场景需求： 例如，Thymeleaf 的“自然模板”特性对于需要前端设计师直接预览模板的场景非常有价值。 总结 #该基准测试为我们提供了一个关于不同 Java 模板引擎原始渲染性能的参考。Rocker、Pebble 等较新的引擎在性能上表现出色，而 Thymeleaf 由于其基于 DOM 的处理方式，在速度上相对落后。\n最终选择哪个模板引擎，需要根据项目的具体需求、团队的技术栈偏好以及对性能、功能、易用性等多个维度的权衡来决定。对于性能敏感的应用，可以优先考虑 Rocker 或 Pebble；对于需要自然模板或与 Spring 生态紧密集成的场景，Thymeleaf 仍然是一个流行的选择（并可通过缓存等手段优化）；而 Freemarker、Velocity 等传统引擎则以其稳定性和广泛的应用基础占据一席之地。\n","date":"2016年12月12日","permalink":"/posts/java-%E6%A8%A1%E6%9D%BF%E5%BC%95%E6%93%8E%E6%80%A7%E8%83%BD%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E5%AF%B9%E6%AF%94%E4%B8%8E%E5%88%86%E6%9E%90/","section":"","summary":"\u003cp\u003e在 Java Web 开发或代码生成场景中，模板引擎扮演着重要的角色，它负责将数据与预定义的模板结合，生成最终的 HTML 页面或文本文件。不同的模板引擎在语法、功能特性以及性能方面可能存在显著差异。性能是选择模板引擎时的一个重要考量因素，尤其是在高并发或需要大量渲染的场景下。\u003c/p\u003e","title":"Java 模板引擎性能基准测试对比与分析"},{"content":"","date":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance"},{"content":"","date":null,"permalink":"/tags/rocker/","section":"Tags","summary":"","title":"Rocker"},{"content":"","date":null,"permalink":"/tags/template-engine/","section":"Tags","summary":"","title":"Template-Engine"},{"content":"","date":null,"permalink":"/tags/thymeleaf/","section":"Tags","summary":"","title":"Thymeleaf"},{"content":"","date":null,"permalink":"/tags/velocity/","section":"Tags","summary":"","title":"Velocity"},{"content":"","date":null,"permalink":"/tags/command-line/","section":"Tags","summary":"","title":"Command-Line"},{"content":"","date":null,"permalink":"/tags/conflict/","section":"Tags","summary":"","title":"Conflict"},{"content":"","date":null,"permalink":"/tags/hadoop/","section":"Tags","summary":"","title":"Hadoop"},{"content":"","date":null,"permalink":"/tags/path/","section":"Tags","summary":"","title":"Path"},{"content":"","date":null,"permalink":"/tags/yarn/","section":"Tags","summary":"","title":"Yarn"},{"content":"","date":null,"permalink":"/tags/yarnpkg/","section":"Tags","summary":"","title":"Yarnpkg"},{"content":"Yarn 是一个流行的 JavaScript 包管理器，由 Facebook (现 Meta) 开发，旨在替代 npm 提供更快、更可靠的依赖管理。 Hadoop YARN (Yet Another Resource Negotiator) 则是 Apache Hadoop 生态系统中的资源管理器和作业调度器。\n问题在于，这两款完全不同的工具，其命令行可执行文件都可能被命名为 yarn (或 yarn.cmd 在 Windows 上)。\n如果你在同一台机器（尤其是 Windows）上同时安装了 Yarn (JS 包管理器) 和 Hadoop，并且两者的安装路径都被添加到了系统的 PATH 环境变量中，那么当你尝试在命令行中执行 yarn 命令时，系统可能会错误地调用 Hadoop YARN 的命令，导致意外的错误。\n冲突症状 #当你期望运行 Yarn (JS 包管理器) 的命令时，例如检查版本：\nyarn --version 你可能会遇到以下 并非来自 Yarn (JS 包管理器) 的错误信息：\nUnrecognized option: --version Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. 错误原因分析： 这个错误信息实际上是 Hadoop YARN 抛出的。因为系统根据 PATH 环境变量的顺序，先找到了 Hadoop 的 yarn.cmd 。Hadoop YARN 是一个 Java 程序，它不识别 \u0026ndash;version 这个命令行参数，并且在尝试启动 Java 虚拟机 (JVM) 时可能因为错误的参数或其他配置问题而失败，最终导致程序退出。\n根本原因 #问题的根源在于：\n命令名称冲突： 两个不同的程序使用了相同的 yarn 命令名。 PATH 环境变量顺序： Windows (或其他操作系统) 在执行命令时，会按照 PATH 环境变量中列出的目录顺序查找可执行文件。如果 Hadoop 的 bin 目录在 PATH 中排在了 Yarn (JS 包管理器) 的 bin 目录之前，那么系统就会优先执行 Hadoop 的 yarn.cmd 。 解决方案 #有几种方法可以解决这个冲突：\n方案一：使用 yarnpkg 别名 (官方推荐) #Yarn (JS 包管理器) 的开发者意识到了这个冲突，并提供了一个备用命令名 yarnpkg 。在大多数情况下，你可以直接使用 yarnpkg 来代替 yarn 执行所有命令。\n例如，检查版本：\nyarnpkg --version 安装依赖：\nyarnpkg install 添加依赖：\nyarnpkg add \u0026lt;package_name\u0026gt; 这是最简单、最推荐的解决方案，因为它不需要修改系统环境。\n方案二：调整 PATH 环境变量顺序 (Windows) #如果你更希望直接使用 yarn 命令来调用 JS 包管理器，可以通过调整系统 PATH 环境变量，让 Yarn (JS 包管理器) 的路径 优先于 Hadoop 的路径。\n步骤 (以 Windows 10/11 为例)：\n在 Windows 搜索栏中搜索“环境变量”，选择“编辑系统环境变量”。 在“系统属性”对话框中，点击“高级”选项卡下的“环境变量(N)\u0026hellip;”按钮。 在“环境变量”对话框的“系统变量(S)”（或“用户变量”，取决于你的安装方式）区域，找到名为 Path (或 PATH ) 的变量，选中它，然后点击“编辑(E)\u0026hellip;”。 在“编辑环境变量”对话框中，你会看到一个路径列表。找到包含 Yarn (JS 包管理器) 的 bin 目录的路径（例如 C:\\Program Files (x86)\\Yarn\\bin 或 C:\\Users\u0026lt;YourUsername\u0026gt;\\AppData\\Local\\Yarn\\bin ）。 找到包含 Hadoop 的 bin 目录的路径（例如 C:\\hadoop\\bin ）。 选中 Yarn 的路径，使用右侧的“上移(U)”按钮，将其移动到 Hadoop 路径的 上方 。 点击“确定”保存所有打开的对话框。 重要： 关闭所有已打开的命令行窗口（如 CMD, PowerShell, VS Code 终端等），然后 重新打开 一个新的命令行窗口，使 PATH 的更改生效。 现在，当你在新的命令行窗口中输入 yarn 时，系统应该会优先找到并执行 Yarn (JS 包管理器) 的命令。 方案三：修改 Hadoop 的启动脚本 (不推荐) #理论上也可以修改 Hadoop 的 yarn.cmd 脚本名称，但这可能会影响 Hadoop 的正常使用或升级，因此 不推荐 这样做。\n相关讨论 #这个问题在 Yarn 的 GitHub 仓库中有过讨论： Issue #673 。虽然曾有人提议将 Yarn (JS) 命令改名为 nyarn 等，但最终官方选择提供 yarnpkg 别名作为主要的解决方案。\n总结 #Yarn (JS 包管理器) 和 Hadoop YARN 的 yarn 命令冲突是由于命名相同以及 PATH 环境变量顺序引起的。推荐使用官方提供的 yarnpkg 别名来调用 JS 包管理器，或者通过调整 PATH 环境变量的顺序来解决此问题。\n","date":"2016年11月30日","permalink":"/posts/yarnpkg%E5%92%8Chadoop-yarn%E5%91%BD%E5%90%8D%E5%86%B2%E7%AA%81/","section":"","summary":"\u003cp\u003e\u003ca href=\"https://yarnpkg.com/\" target=\"_blank\" rel=\"noreferrer\"\u003eYarn\u003c/a\u003e 是一个流行的 JavaScript 包管理器，由 Facebook (现 Meta) 开发，旨在替代 npm 提供更快、更可靠的依赖管理。\n\u003ca href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\" target=\"_blank\" rel=\"noreferrer\"\u003eHadoop YARN\u003c/a\u003e (Yet Another Resource Negotiator) 则是 Apache Hadoop 生态系统中的资源管理器和作业调度器。\u003c/p\u003e\n\u003cp\u003e问题在于，这两款完全不同的工具，其命令行可执行文件都可能被命名为 \u003ccode\u003eyarn\u003c/code\u003e (或 \u003ccode\u003eyarn.cmd\u003c/code\u003e 在 Windows 上)。\u003c/p\u003e\n\u003cp\u003e如果你在同一台机器（尤其是 Windows）上同时安装了 Yarn (JS 包管理器) 和 Hadoop，并且两者的安装路径都被添加到了系统的 \u003ccode\u003ePATH\u003c/code\u003e 环境变量中，那么当你尝试在命令行中执行 \u003ccode\u003eyarn\u003c/code\u003e 命令时，系统可能会错误地调用 Hadoop YARN 的命令，导致意外的错误。\u003c/p\u003e","title":"解决 Windows 环境下 Yarn (JS 包管理器) 与 Hadoop YARN 的 `yarn` 命令冲突问题"},{"content":"最近入手了亚马逊 Fire HDX 7 平板，想刷入第三方 ROM (如 CyanogenMod/LineageOS) 以获得更自由的体验。解锁 Bootloader 是第一步。本文参考了 XDA 论坛的教程，并结合我自己的实践经验，整理了一份更详细、清晰的解锁流程，希望能帮助到大家。\n重要提示： 解锁 Bootloader 会清除设备上的所有数据，并可能导致设备失去保修。请在操作前务必备份好重要数据，并仔细阅读所有步骤。操作有风险，请谨慎进行。\n第一步：准备工作与 Root # 获取 Root 权限： 解锁过程需要 Root 权限。你可以使用以下任一工具尝试获取 Root：\nKingoRoot KingRoot 下载并安装对应的 App 或 PC 软件，按照提示进行一键 Root 操作。 安装 ADB 和 Fastboot 驱动： 电脑需要正确识别你的设备才能执行后续命令。\n下载驱动： 访问 此 Google Drive 链接 (原教程提供，请自行确认链接有效性)，进入 Non_English 文件夹下载适用于你系统的驱动文件。 (Windows) 禁用驱动程序强制签名： Windows 8/10/11 通常需要禁用驱动强制签名才能安装未签名的驱动。 a. 按住 SHIFT 键，然后点击“开始”菜单中的“重启”。 b. 重启后，依次选择“疑难解答” -\u0026gt; “高级选项” -\u0026gt; “启动设置”，然后点击“重启”。 c. 电脑再次重启后，会显示启动设置菜单，按 F7 键选择“禁用驱动程序强制签名”。 d. 进入系统后，驱动签名验证就被临时禁用了。 安装驱动： 解压下载的驱动文件，找到 dpinst.exe (或类似安装程序)，以管理员身份运行。如果提供的是 .inf 文件，可以通过设备管理器手动安装。原教程提供的命令 dpinst.exe /EL 可能是特定安装包的参数，请根据实际下载的文件调整安装方式。 验证安装： 连接平板到电脑，打开开发者选项并启用 USB 调试。在电脑上打开命令提示符 (CMD) 或 PowerShell，输入 adb devices。如果看到设备序列号，表示 ADB 驱动安装成功。 第二步：获取设备硬件信息 #解锁需要用到设备的唯一识别码。\n连接设备： 确保平板通过 USB 连接到电脑，并且 USB 调试已开启。 打开 ADB Shell： 在电脑的命令提示符或 PowerShell 中输入： adb shell 查询硬件信息： 在 adb shell 环境下，依次输入以下两条命令： cat /sys/block/mmcblk0/device/manfid cat /sys/block/mmcblk0/device/serial 记录结果： 你会得到类似下面的输出： # cat /sys/block/mmcblk0/device/manfid 0x0000mm \u0026lt;-- mm 是你的 manfid 值 # cat /sys/block/mmcblk0/device/serial 0xssssssss \u0026lt;-- ssssssss 是你的 serial 值 将这两个值组合成 0xmmssssssss 的格式（例如，如果 manfid 是 0x000015，serial 是 0x1234abcd，则组合结果为 0x151234abcd）。请务必记下这个组合后的值，后面会用到。 退出 ADB Shell： 输入 exit 并回车。 第三步：生成解锁文件 (.unlock) #需要使用 Python 脚本来生成解锁 Bootloader 所需的文件。\n安装 Python： 如果你的电脑没有安装 Python，请从 Python 官网 下载并安装 (建议选择 Python 2.7 或兼容版本，根据脚本要求)。确保将 Python 添加到系统环境变量 (PATH)。 安装 gmpy2 库： cuberHDX.py 脚本依赖 gmpy2 库。打开命令提示符或 PowerShell，使用 pip 安装： pip install gmpy2 注意： 在 Windows 上安装 gmpy2 可能需要预编译的二进制文件或额外的编译环境。你可以尝试从 这里 下载对应 Python 版本的 .whl 文件，然后使用 pip install gmpy2-xxxx.whl 安装。 下载 cuberHDX 脚本： 从 XDA 帖子 下载 cuberHDX.py 脚本文件，并将其保存到电脑上一个方便访问的位置。 生成解锁文件： 打开命令提示符或 PowerShell，切换到 cuberHDX.py 脚本所在的目录，然后运行以下命令（将 0xmmssssssss 替换为 你在第二步中记下的那个值）： python cuberHDX.py 0xmmssssssss 如果一切顺利，脚本会在当前目录下生成一个名为 0xmmssssssss.unlock 的文件。这就是你的解锁文件。 第四步：刷入旧版 aboot 和 TWRP Recovery #为了能够成功解锁，需要先刷入一个存在漏洞的旧版 aboot (引导加载程序的一部分) 和一个定制的 TWRP Recovery。\n下载所需文件： 再次访问 此 Google Drive 链接 (原教程提供)，下载 aboot_vuln.mbn 和 twrp_cubed.img 这两个文件。 传输文件到设备： 将下载好的 aboot_vuln.mbn 和 twrp_cubed.img 文件复制到平板的内部存储空间根目录 (通常是 /sdcard/)。 刷入 TWRP Recovery： 打开 adb shell： adb shell 获取 Root 权限 (如果 adb shell 默认不是 root 用户)： su (此时平板上可能会弹出授权请求，请允许) 使用 dd 命令刷入 TWRP (请极其小心，确保命令无误！)： dd if=/sdcard/twrp_cubed.img of=/dev/block/platform/msm_sdcc.1/by-name/recovery 刷入完成后，输入 exit 两次退出 adb shell。 刷入旧版 aboot： (原教程似乎遗漏了刷入 aboot_vuln.mbn 的步骤，但根据 XDA 流程通常需要这一步。请参考 XDA 原帖确认是否需要以及如何刷入，通常也是通过 dd 命令刷入到 aboot 分区，例如 dd if=/sdcard/aboot_vuln.mbn of=/dev/block/platform/msm_sdcc.1/by-name/aboot。刷写 aboot 风险极高，请务必确认无误再操作！ 如果不确定，请严格按照 XDA 最新教程操作。) 第五步：执行解锁命令 #现在可以正式解锁 Bootloader 了。\n进入 Fastboot 模式： 先将平板完全关机。 按住 音量加键 不放，同时连接 USB 数据线到电脑。设备屏幕应该会停留在 Kindle Fire Logo 界面，这就是 Fastboot 模式。 或者，在 adb shell 中执行 reboot bootloader 命令。 验证 Fastboot 连接： 在电脑的命令提示符或 PowerShell 中输入： fastboot -i 0x1949 devices 如果看到设备序列号，表示 Fastboot 连接成功。(-i 0x1949 是指定 Amazon 设备的 Vendor ID)。 刷入解锁文件： 确保你的命令提示符或 PowerShell 当前目录是之前生成 .unlock 文件的目录。然后执行以下命令 (将 0xmmssssssss 替换为 你的那个值)： fastboot -i 0x1949 flash unlock 0xmmssssssss.unlock 检查结果： 如果命令执行成功，你应该会看到类似 unlock code is correct 的提示。这表示你的 Bootloader 已经成功解锁！ 重启设备： fastboot -i 0x1949 reboot 第六步：刷入自定义 ROM (例如 CM13) #Bootloader 解锁后，你就可以刷入第三方 Recovery (如果之前刷入的 TWRP 不是最新版，建议更新) 和自定义 ROM 了。\n下载 ROM： 前往 CyanogenMod 下载页面 (或其他 ROM 提供方) 下载适用于 Fire HDX 7 (代号 thor) 的 CM13 (或其他你选择的 ROM) 的 zip 文件，以及对应的 GApps (Google 应用包，如果需要的话)。 传输文件： 将下载好的 ROM zip 文件和 GApps zip 文件复制到平板的内部存储空间。 进入 Recovery 模式： 关机状态下，按住 音量减键 和 电源键，直到看到 TWRP Recovery 界面。 或者在 adb shell 中执行 reboot recovery。 刷入 ROM： 在 TWRP 中，建议先进行 Wipe (清除) 操作，选择 \u0026ldquo;Advanced Wipe\u0026rdquo;，勾选 Dalvik/ART Cache, System, Data, Cache，然后滑动确认清除 (不要清除 Internal Storage)。 返回主菜单，选择 Install (安装)。 找到你之前复制到设备的 ROM zip 文件，选择它，然后滑动确认刷入。 刷入 GApps (如果需要)，步骤同上。 完成后，返回主菜单，选择 Reboot (重启) -\u0026gt; System (系统)。 首次启动新系统可能会比较慢，请耐心等待。恭喜你，现在你的 Fire HDX 7 已经解锁并运行自定义 ROM 了！\n","date":"2016年11月2日","permalink":"/posts/amazon-fire-hdx7-bootloader-%E8%A7%A3%E9%94%81%E6%95%99%E7%A8%8B/","section":"","summary":"\u003cp\u003e最近入手了亚马逊 Fire HDX 7 平板，想刷入第三方 ROM (如 CyanogenMod/LineageOS) 以获得更自由的体验。解锁 Bootloader 是第一步。本文参考了 \u003ca href=\"http://forum.xda-developers.com/kindle-fire-hdx/general/thor-unlocking-bootloader-firmware-t3463982\" target=\"_blank\" rel=\"noreferrer\"\u003eXDA 论坛的教程\u003c/a\u003e，并结合我自己的实践经验，整理了一份更详细、清晰的解锁流程，希望能帮助到大家。\u003c/p\u003e","title":"Amazon Fire HDX7 Bootloader 解锁教程"},{"content":"","date":null,"permalink":"/tags/android/","section":"Tags","summary":"","title":"Android"},{"content":"","date":null,"permalink":"/tags/bootloader/","section":"Tags","summary":"","title":"Bootloader"},{"content":"","date":null,"permalink":"/tags/cyanogenmod/","section":"Tags","summary":"","title":"Cyanogenmod"},{"content":"","date":null,"permalink":"/tags/kindle-fire-hdx/","section":"Tags","summary":"","title":"Kindle Fire Hdx"},{"content":"","date":null,"permalink":"/tags/root/","section":"Tags","summary":"","title":"Root"},{"content":"","date":null,"permalink":"/tags/twrp/","section":"Tags","summary":"","title":"Twrp"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]